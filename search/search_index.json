{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Preparation & Analysis","text":""},{"location":"#data-preparation-analysis-neural-networks","title":"Data Preparation &amp; Analysis \u2014 Neural Networks","text":"Edi\u00e7\u00e3o <p>2025.2</p> <p>This report presents a focused, hands-on study on how data generation and conditioning shape neural network learning. The project was divided into three main parts:</p> <ol> <li> <p>generate and analyze 2D Gaussian datasets;</p> </li> <li> <p>create and visualize multi-dimensional data;</p> </li> <li> <p>prepare a real-world dataset for a neural network with tanh activations in its hidden layers.</p> </li> </ol>"},{"location":"#part-1-exploring-class-separability-in-2d","title":"Part 1: Exploring Class Separability in 2D","text":"<p>Understanding how data is distributed is the first design step before designing a network architecture. In this part, I generated and visualized a two-dimensional dataset with four Gaussian classes to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"#data-generation","title":"Data Generation","text":"<p>First, I created a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). I used a Gaussian distribution to generate the points for each class based on the following parameters:</p> <ul> <li>Class 0: Mean = \\([2, 3]\\), Standard Deviation = \\([0.8, 2.5]\\)</li> <li>Class 1: Mean = \\([5, 6]\\), Standard Deviation = \\([1.2, 1.9]\\)</li> <li>Class 2: Mean = \\([8, 1]\\), Standard Deviation = \\([0.9, 0.9]\\)</li> <li>Class 3: Mean = \\([15, 4]\\), Standard Deviation = \\([0.5, 2.0]\\)</li> </ul> <p>To generate the dataset, I utilized the function <code>numpy.random.multivariate_normal</code>, which allows for the creation of samples from a multivariate normal distribution. It's documentation can be found here. The code bellow is the one I used to generate the data:</p> <pre><code>import numpy as np\n\nnp.random.seed(2)\n\n\ndef create_data(N, mean, std_dev):\n    cov = np.diag(std_dev)\n    x, y = np.random.multivariate_normal(mean, cov, N).T\n    return x, y\n\n\nx1, y1 = create_data(100, [2, 3], [0.8, 2.5])\nx2, y2 = create_data(100, [5, 6], [1.2, 1.9])\nx3, y3 = create_data(100, [8, 1], [0.9, 0.9])\nx4, y4 = create_data(100, [15, 4], [0.5, 2.0])\n</code></pre>"},{"location":"#data-visualization","title":"Data Visualization","text":"<p>After that, I visualized the dataset using scatter plots. The first plot shows the raw data points colored by their respective classes, while the second plot includes lines that represent potential decision boundaries separating the classes. The complete code for generating and plotting the data is shown below:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\n# https://numpy.org/doc/2.2/reference/random/generated/numpy.random.multivariate_normal.html\n\nnp.random.seed(2)\n\n\ndef create_data(N, mean, std_dev):\n    cov = np.diag(std_dev)\n    x, y = np.random.multivariate_normal(mean, cov, N).T\n    return x, y\n\n\nx1, y1 = create_data(100, [2, 3], [0.8, 2.5])\nx2, y2 = create_data(100, [5, 6], [1.2, 1.9])\nx3, y3 = create_data(100, [8, 1], [0.9, 0.9])\nx4, y4 = create_data(100, [15, 4], [0.5, 2.0])\n\nimport matplotlib.pyplot as plt\n\nplt.close('all')\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\naxes[0].scatter(x1, y1, alpha=0.5, s=50, c=\"blue\")\naxes[0].scatter(x2, y2, alpha=0.5, s=50, c=\"red\")\naxes[0].scatter(x3, y3, alpha=0.5, s=50, c=\"green\")\naxes[0].scatter(x4, y4, alpha=0.5, s=50, c=\"yellow\")\naxes[0].set_title(\"Scatter Plot de dados gerados com gaussianas\")\naxes[0].set_xlabel(\"X values\")\naxes[0].set_ylabel(\"Y values\")\n\naxes[1].scatter(x1, y1, alpha=0.5, s=50, c=\"blue\")\naxes[1].scatter(x2, y2, alpha=0.5, s=50, c=\"red\")\naxes[1].scatter(x3, y3, alpha=0.5, s=50, c=\"green\")\naxes[1].scatter(x4, y4, alpha=0.5, s=50, c=\"yellow\")\n\naxes[1].plot([6.38, 0.56], [-2, 10], c=\"black\", linewidth=2)\naxes[1].plot([11.92, 2.5], [10, -2], c=\"black\", linewidth=2)\naxes[1].plot([13.4, 7.4], [-1.7, 10], c=\"black\", linewidth=2)\n\naxes[1].set_title(\"Scatter Plot + Linhas de Separa\u00e7\u00e3o\")\naxes[1].set_xlabel(\"X values\")\naxes[1].set_ylabel(\"Y values\")\n\nplt.tight_layout()\nplt.show()\n</pre> Output Clear <pre></pre> <p></p> <p>The output is shown below:</p> <p></p>"},{"location":"#analysis-of-class-separability","title":"Analysis of Class Separability","text":"<p>The four classes form well-defined Gaussian clusters. The blue one sits on the left with moderate vertical spread. The red is above and slightly to the right of blue. Green lies farther right and lower, with a tighter cloud. Finally, Yellow is isolated on the far right. There\u2019s light overlap between blue and red and a narrow diagonal corridor of conflict between red and green. Yellow barely mixes with others.</p> <p>Considering that, it would not be possible to create a simple linear decision boundary that perfectly separates all four classes due to the overlaps and proximity of some classes. A neural network would likely need to learn more complex, non-linear boundaries to effectively classify the data points.</p>"},{"location":"#part-2-non-linearity-in-higher-dimensions","title":"Part 2: Non-Linearity in Higher Dimensions","text":"<p>Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable.</p>"},{"location":"#data-generation-in-higher-dimensions","title":"Data Generation in Higher Dimensions","text":"<p>To explore this, I generated a synthetic dataset in a higher-dimensional space where classes are not linearly separable. I generated 2 classes, each onde with 500 samples. The parameters used for generating the data were:</p> <ul> <li>Class A:</li> </ul> <p>Mean vector</p> <p>$$   \\mu_A = [0, 0, 0, 0, 0]   $$</p> <p>Covariance matrix</p> <p>$$     \\Sigma_A =     \\begin{pmatrix}     1.0 &amp; 0.8 &amp; 0.1 &amp; 0.0 &amp; 0.0 \\\\     0.8 &amp; 1.0 &amp; 0.3 &amp; 0.0 &amp; 0.0 \\\\     0.1 &amp; 0.3 &amp; 1.0 &amp; 0.5 &amp; 0.0 \\\\     0.0 &amp; 0.0 &amp; 0.5 &amp; 1.0 &amp; 0.2 \\\\     0.0 &amp; 0.0 &amp; 0.0 &amp; 0.2 &amp; 1.0     \\end{pmatrix}   $$</p> <ul> <li>Class B:</li> </ul> <p>Mean vector</p> <p>$$   \\mu_B = [1.5, 1.5, 1.5, 1.5, 1.5]   $$</p> <p>Covariance matrix</p> <p>$$   \\Sigma_B = \\begin{pmatrix}   1.5 &amp; -0.7 &amp; 0.2 &amp; 0.0 &amp; 0.0 \\\\   -0.7 &amp; 1.5 &amp; 0.4 &amp; 0.0 &amp; 0.0 \\\\   0.2 &amp; 0.4 &amp; 1.5 &amp; 0.6 &amp; 0.0 \\\\   0.0 &amp; 0.0 &amp; 0.6 &amp; 1.5 &amp; 0.3 \\\\   0.0 &amp; 0.0 &amp; 0.0 &amp; 0.3 &amp; 1.5   \\end{pmatrix}   $$</p> <p>Using the same <code>numpy.random.multivariate_normal</code> function, I generated the samples for both classes. The code used is shown below:</p> <pre><code>import numpy as np\n\nmean_A = np.zeros(5)\n\nSigma_A = np.array(\n    [\n        [1.0, 0.8, 0.1, 0.0, 0.0],\n        [0.8, 1.0, 0.3, 0.0, 0.0],\n        [0.1, 0.3, 1.0, 0.5, 0.0],\n        [0.0, 0.0, 0.5, 1.0, 0.2],\n        [0.0, 0.0, 0.0, 0.2, 1.0],\n    ]\n)\n\nmean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\n\nSigma_B = np.array(\n    [\n        [1.5, -0.7, 0.2, 0.0, 0.0],\n        [-0.7, 1.5, 0.4, 0.0, 0.0],\n        [0.2, 0.4, 1.5, 0.6, 0.0],\n        [0.0, 0.0, 0.6, 1.5, 0.3],\n        [0.0, 0.0, 0.0, 0.3, 1.5],\n    ]\n)\n\nptsA = np.random.multivariate_normal(mean_A, Sigma_A, 500)\nptsB = np.random.multivariate_normal(mean_B, Sigma_B, 500)\n</code></pre>"},{"location":"#dimensionality-reduction-visualization","title":"Dimensionality Reduction &amp; Visualization","text":"<p>Since it's not possible to plot a 5D graph, I had to reduce the dimensionality of the data. To visualize the high-dimensional data, I applied Principal Component Analysis (PCA) to reduce the data to 2 dimensions.</p> <pre><code># PCA Implementation\nsub_a = ptsA - mean_A\nsub_b = ptsB - mean_B\nstdA = np.sqrt(np.diag(Sigma_A))\nstdB = np.sqrt(np.diag(Sigma_B))\nptsA_ = sub_a / stdA\nptsB_ = sub_b / stdB\n\n\nC_A = (1 / 500) * (ptsA_.T @ ptsA_)\nC_B = (1 / 500) * (ptsB_.T @ ptsB_)\n\neigenvalues_A, eigenvectors_A = np.linalg.eig(C_A)\neigenvalues_B, eigenvectors_B = np.linalg.eig(C_B)\n\nsorted_indices_A = np.argsort(eigenvalues_A)[::-1]\nsorted_indices_B = np.argsort(eigenvalues_B)[::-1]\n\nk = 2\nW_A = eigenvectors_A[:, sorted_indices_A[:k]]\nW_B = eigenvectors_B[:, sorted_indices_B[:k]]\n\nYA = ptsA_ @ W_A\nYB = ptsB_ @ W_B\n</code></pre> <p>Finally, I plotted the 2D projections of both classes to visualize their distribution after dimensionality reduction. The full code can be run below:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\nmean_A = np.zeros(5)\n\nSigma_A = np.array(\n    [\n        [1.0, 0.8, 0.1, 0.0, 0.0],\n        [0.8, 1.0, 0.3, 0.0, 0.0],\n        [0.1, 0.3, 1.0, 0.5, 0.0],\n        [0.0, 0.0, 0.5, 1.0, 0.2],\n        [0.0, 0.0, 0.0, 0.2, 1.0],\n    ]\n)\n\nmean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\n\nSigma_B = np.array(\n    [\n        [1.5, -0.7, 0.2, 0.0, 0.0],\n        [-0.7, 1.5, 0.4, 0.0, 0.0],\n        [0.2, 0.4, 1.5, 0.6, 0.0],\n        [0.0, 0.0, 0.6, 1.5, 0.3],\n        [0.0, 0.0, 0.0, 0.3, 1.5],\n    ]\n)\n\nptsA = np.random.multivariate_normal(mean_A, Sigma_A, 500)\nptsB = np.random.multivariate_normal(mean_B, Sigma_B, 500)\n\nsub_a = ptsA - mean_A\nsub_b = ptsB - mean_B\nstdA = np.sqrt(np.diag(Sigma_A))\nstdB = np.sqrt(np.diag(Sigma_B))\nptsA_ = sub_a / stdA\nptsB_ = sub_b / stdB\n\n\nC_A = (1 / 500) * (ptsA_.T @ ptsA_)\nC_B = (1 / 500) * (ptsB_.T @ ptsB_)\n\neigenvalues_A, eigenvectors_A = np.linalg.eig(C_A)\neigenvalues_B, eigenvectors_B = np.linalg.eig(C_B)\n\nsorted_indices_A = np.argsort(eigenvalues_A)[::-1]\nsorted_indices_B = np.argsort(eigenvalues_B)[::-1]\n\nk = 2\nW_A = eigenvectors_A[:, sorted_indices_A[:k]]\nW_B = eigenvectors_B[:, sorted_indices_B[:k]]\n\nYA = ptsA_ @ W_A\nYB = ptsB_ @ W_B\n\nimport matplotlib.pyplot as plt\n\nplt.close('all')\nplt.scatter(YA[:, 0], YA[:, 1], alpha=0.5, s=50, c=\"black\")\nplt.scatter(YB[:, 0], YB[:, 1], alpha=0.5, s=50, c=\"red\")\nplt.title(\"Scatter Plot de dados gerados com gaussianas\")\nplt.xlabel(\"X values\")\nplt.ylabel(\"Y values\")\nplt.show()</pre> Output Clear <pre><code></code></pre> <p></p> <p>The resulting plot is shown below:</p> <p></p>"},{"location":"#analysis-of-the-plot","title":"Analysis of the plot","text":"<p>Based on the 2D projection, the two classes largely co-occupy the same region. The red class forms a tighter, roughly elliptical core around the origin, whereas the black class is more diffuse and spreads farther in all directions. This means their centers are similar but their dispersion/covariance differs, leading to substantial overlap and no obvious linear split. Because of that, a multi-layer neural network would be better than a simple linear model, since it can combine multiple simple cuts to form a curved, flexible boundary that better follows the different shapes and separates the classes more accurately.</p>"},{"location":"#part-3-preparing-real-world-data-for-a-neural-network","title":"Part 3: Preparing Real-World Data for a Neural Network","text":"<p>After all that preparation with synthetic data, it was important to move on to a real-world dataset: the Spaceship Titanic dataset from Kaggle. The objective was, using this data, make the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent activation function in its hidden layers.</p> Important Note <p>To do this part, I used as reference the Spaceship Titanic: A complete guide</p>"},{"location":"#description-of-the-dataset","title":"Description of the Dataset","text":"<p>The Spaceship Titanic challenge asks you to build a binary classifier that predicts whether each passenger was transported to an alternate dimension after a spacetime-anomaly collision.</p> <p>Features descriptions:</p> <ul> <li> <p>PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.</p> </li> <li> <p>HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.</p> </li> <li> <p>CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.</p> </li> <li> <p>Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.</p> </li> <li> <p>Destination - The planet the passenger will be debarking to.</p> </li> <li> <p>Age - The age of the passenger.</p> </li> <li> <p>VIP - Whether the passenger has paid for special VIP service during the voyage.   RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.</p> </li> <li> <p>Name - The first and last names of the passenger.</p> </li> <li> <p>Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.</p> </li> </ul> <p>Analizing Missing Values</p> <p> </p> Editor (session: default) Run <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pyodide.http import open_url\n\nurl = \"https://raw.githubusercontent.com/HenriqueFBadin/Data-Preparation-and-Analysis-for-Neural-Networks/main/docs/assets/data/train.csv\"\ndf = pd.read_csv(open_url(url))\n\nprint(\"Missing values per column:\\n\")\nprint(df.isnull().sum())</pre> Output Clear <pre></pre> <p></p>"},{"location":"#preprocess-the-data","title":"Preprocess the Data:","text":"<p>Therefore, I proceeded to clean and transform the data to make it suitable for a neural network. As the tanh activation function produces outputs in the range [-1, 1], I ensured that the input data was scaled appropriately for stable training. The complete code used for data preprocessing is shown below:</p> <p> </p> Editor (session: default) Run <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pyodide.http import open_url\n\nurl = \"https://raw.githubusercontent.com/HenriqueFBadin/Data-Preparation-and-Analysis-for-Neural-Networks/main/docs/assets/data/train.csv\"\ndf = pd.read_csv(open_url(url))\n\nprint(\"Missing values per column:\\n\")\nprint(df.isnull().sum())\n\nprint(\"Unique values per column:\\n\")\nprint(df.nunique())\n\nprint(\"\\nData types of each column:\\n\")\nprint(df.dtypes)\n\ny = df[\"Transported\"]\nX = df.drop(columns=[\"Transported\"])\n\nnum_cols = X.select_dtypes(include=[\"float64\"]).columns\ncat_cols = X.select_dtypes(include=[\"object\", \"bool\"]).columns\n\nX[num_cols] = X[num_cols].fillna(X[num_cols].median())\n\nfor c in cat_cols:\n    moda = X[c].mode(dropna=True)\n    X[c] = X[c].fillna(moda.iloc[0] if not moda.empty else \"Unknown\")\n\ndummies = pd.get_dummies(X[[\"HomePlanet\", \"Destination\"]])\ndummies = dummies.astype(int)\nX_enc = X.drop(columns=[\"HomePlanet\", \"Destination\"]).join(dummies)\n\nX_enc[[\"Deck\", \"CabinNum\", \"Side\"]] = (\n    X_enc[\"Cabin\"].astype(str).str.split(\"/\", expand=True)\n)\ndataframe = X_enc.drop(columns=[\"Cabin\"])\n\ndummies = pd.get_dummies(dataframe[[\"CryoSleep\", \"VIP\", \"Side\"]])\ndummies = dummies.astype(int)\ndataframe = dataframe.drop(columns=[\"CryoSleep\", \"VIP\", \"Side\"]).join(dummies)\n\nprint(\"\\nEstado intermedi\u00e1rio do dataframe:\\n\")\nprint(dataframe.head())\n\ndummies = pd.get_dummies(dataframe[\"Deck\"], prefix=\"Deck\", dummy_na=False, dtype=int)\ndataframe = pd.concat([dataframe.drop(columns=[\"Deck\"]), dummies], axis=1)\n\ndataframe[\"CabinNum\"] = pd.to_numeric(dataframe[\"CabinNum\"], errors=\"coerce\")\ndataframe[\"CabinNum\"] = dataframe[\"CabinNum\"].fillna(dataframe[\"CabinNum\"].median())\ncont_cols = [\n    \"Age\",\n    \"RoomService\",\n    \"FoodCourt\",\n    \"ShoppingMall\",\n    \"Spa\",\n    \"VRDeck\",\n    \"CabinNum\",\n]</pre> Output Clear <pre></pre> <p></p>"},{"location":"#visualizing-the-preprocessed-data","title":"Visualizing the Preprocessed Data","text":"<p>Finally, I visualized the distribution of the numerical features to ensure they were appropriately scaled for a neural network using the tanh activation function. The histograms below show that the features are now centered around zero and fall within a range suitable for tanh.</p> <pre><code>fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\nsns.histplot(dataframe[\"FoodCourt\"], bins=30, kde=True, ax=axes[0])\naxes[0].set_title(\"FoodCourt\")\n\nsns.histplot(dataframe[\"Age\"], bins=30, kde=True, ax=axes[1])\naxes[1].set_title(\"Age\")\n\nfig.suptitle(\n    \"Distribui\u00e7\u00f5es: FoodCourt e Idade Antes da Normaliza\u00e7\u00e3o\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nplt.tight_layout()\nplt.show()\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\ndataframe[cont_cols] = scaler.fit_transform(dataframe[cont_cols])\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\nsns.histplot(dataframe[\"FoodCourt\"], bins=30, kde=True, ax=axes[0])\naxes[0].set_title(\"FoodCourt\")\n\nsns.histplot(dataframe[\"Age\"], bins=30, kde=True, ax=axes[1])\naxes[1].set_title(\"Age\")\n\n\nfig.suptitle(\n    \"Distribui\u00e7\u00f5es: FoodCourt e Idade Ap\u00f3s a Normaliza\u00e7\u00e3o\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nplt.tight_layout()\nplt.show()\n</code></pre> <p>The output is shown below:</p> <p>Before Normalization:</p> <p></p> <p>After Normalization:</p> <p></p>"},{"location":"#conclusion","title":"Conclusion","text":"<p>That way, I successfully preprocessed the Spaceship Titanic dataset, handling missing values, encoding categorical variables, and normalizing numerical features to ensure compatibility with a neural network using the tanh activation function. This preprocessing is crucial for achieving stable and effective training of the model.</p>"},{"location":"#references","title":"References","text":"<ul> <li>Kaggle - Spaceship Titanic</li> <li>Spaceship Titanic: A complete guide</li> <li>Numpy Documentation - multivariate_normal</li> <li>ANN-DL Course - Preprocessing</li> </ul>"},{"location":"VAE_Exercise/","title":"VAE - MNIST","text":"In\u00a0[1]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision.utils import save_image\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom torchvision.utils import make_grid\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntorch.manual_seed(42)\n\nprint(f\"Using device: {device}\")\n</pre> import torch from torch.utils.data import DataLoader, random_split from torchvision import datasets, transforms from torch import nn, optim from torch.nn import functional as F from torchvision.utils import save_image import os import matplotlib.pyplot as plt import numpy as np from sklearn.manifold import TSNE from sklearn.decomposition import PCA from torchvision.utils import make_grid  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  torch.manual_seed(42)  print(f\"Using device: {device}\") <pre>Using device: cpu\n</pre> <p>Primeiramente salvamos os losses para posteriormente plotarmos e visualizarmos o andamento da rede</p> In\u00a0[2]: Copied! <pre>batch_size = 128\nepochs = 10\nlog_interval = 10\ndata_root = \"./data\"\n\n# Listas para logar losses (VAE e ConvVAE)\nvae_train_losses_total = []\nvae_val_losses_total = []\nvae_train_losses_bce = []\nvae_val_losses_bce = []\nvae_train_losses_kld = []\nvae_val_losses_kld = []\n\nconv_train_losses_total = []\nconv_val_losses_total = []\nconv_train_losses_bce = []\nconv_val_losses_bce = []\nconv_train_losses_kld = []\nconv_val_losses_kld = []\n</pre> batch_size = 128 epochs = 10 log_interval = 10 data_root = \"./data\"  # Listas para logar losses (VAE e ConvVAE) vae_train_losses_total = [] vae_val_losses_total = [] vae_train_losses_bce = [] vae_val_losses_bce = [] vae_train_losses_kld = [] vae_val_losses_kld = []  conv_train_losses_total = [] conv_val_losses_total = [] conv_train_losses_bce = [] conv_val_losses_bce = [] conv_train_losses_kld = [] conv_val_losses_kld = [] In\u00a0[3]: Copied! <pre>kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n\ntrain_full = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n\ntotal = len(train_full)\nval_len = (len(test_data))\n\ntrain_len = total - val_len\ntrain_data, val_data = random_split(train_full, [train_len, val_len])\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)\nval_loader   = DataLoader(val_data,   batch_size=batch_size, shuffle=False, **kwargs)\ntest_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False, **kwargs)\n\nprint(f\"Full train size: {total}\")\nprint(f\"Train size: {train_len}, Val size: {val_len}, Test size: {len(test_data)}\")\nprint(f\"Batch size: {batch_size}, num_workers: {kwargs.get('num_workers', 0)}, pin_memory: {kwargs.get('pin_memory', False)}\")\n</pre> kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}  train_full = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor()) test_data = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())  total = len(train_full) val_len = (len(test_data))  train_len = total - val_len train_data, val_data = random_split(train_full, [train_len, val_len])  train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs) val_loader   = DataLoader(val_data,   batch_size=batch_size, shuffle=False, **kwargs) test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False, **kwargs)  print(f\"Full train size: {total}\") print(f\"Train size: {train_len}, Val size: {val_len}, Test size: {len(test_data)}\") print(f\"Batch size: {batch_size}, num_workers: {kwargs.get('num_workers', 0)}, pin_memory: {kwargs.get('pin_memory', False)}\") <pre>100.0%\n100.0%\n100.0%\n100.0%</pre> <pre>Full train size: 60000\nTrain size: 50000, Val size: 10000, Test size: 10000\nBatch size: 128, num_workers: 0, pin_memory: False\n</pre> <pre></pre> In\u00a0[4]: Copied! <pre>batch = next(iter(train_loader))\nimages, labels = batch\nprint(\"Example batch shapes -&gt; images:\", images.shape, \"labels:\", labels.shape)\n</pre> batch = next(iter(train_loader)) images, labels = batch print(\"Example batch shapes -&gt; images:\", images.shape, \"labels:\", labels.shape) <pre>Example batch shapes -&gt; images: torch.Size([128, 1, 28, 28]) labels: torch.Size([128])\n</pre> <p>Para a constru\u00e7\u00e3o do VAE, utilizei o github do pytorch para aprender a criar VAEs com a biblioteca deles. O exemplo que eu utilizei de guia para saber se eu estava fazendo corretamente est\u00e1 no link.</p> In\u00a0[5]: Copied! <pre>class VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n\n        # 784 \u00e9 28\u00b2, onde 28 \u00e9 o tamanho da imagem\n        # Cada fc \u00e9 relativa a uma camada oculta que faziamos no MLP do zero anteriormente\n        # Entra o tamanho da imagem -&gt; camada oculta de 400 par\u00e2metros -&gt; Camada oculta de 20 par\u00e2metros -&gt; volta para 400 par\u00e2metros -&gt; reconstroi a imagem original\n        self.fc1 = nn.Linear(784, 400)\n        self.fc21 = nn.Linear(400, 20)\n        self.fc22 = nn.Linear(400, 20)\n        self.fc3 = nn.Linear(20, 400)\n        self.fc4 = nn.Linear(400, 784)\n\n    def encode(self, x):\n        h1 = F.relu(self.fc1(x))\n        return self.fc21(h1), self.fc22(h1)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5*logvar)\n        eps = torch.randn_like(std)\n        return mu + eps*std\n\n    def decode(self, z):\n        h3 = F.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h3))\n\n    def forward(self, x):\n        mu, logvar = self.encode(x.view(-1, 784))\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n</pre> class VAE(nn.Module):     def __init__(self):         super(VAE, self).__init__()          # 784 \u00e9 28\u00b2, onde 28 \u00e9 o tamanho da imagem         # Cada fc \u00e9 relativa a uma camada oculta que faziamos no MLP do zero anteriormente         # Entra o tamanho da imagem -&gt; camada oculta de 400 par\u00e2metros -&gt; Camada oculta de 20 par\u00e2metros -&gt; volta para 400 par\u00e2metros -&gt; reconstroi a imagem original         self.fc1 = nn.Linear(784, 400)         self.fc21 = nn.Linear(400, 20)         self.fc22 = nn.Linear(400, 20)         self.fc3 = nn.Linear(20, 400)         self.fc4 = nn.Linear(400, 784)      def encode(self, x):         h1 = F.relu(self.fc1(x))         return self.fc21(h1), self.fc22(h1)      def reparameterize(self, mu, logvar):         std = torch.exp(0.5*logvar)         eps = torch.randn_like(std)         return mu + eps*std      def decode(self, z):         h3 = F.relu(self.fc3(z))         return torch.sigmoid(self.fc4(h3))      def forward(self, x):         mu, logvar = self.encode(x.view(-1, 784))         z = self.reparameterize(mu, logvar)         return self.decode(z), mu, logvar <p>Usei uma dimens\u00e3o latente de 20. Posteriormente, apliquei otimiza\u00e7\u00e3o do VAE com Adam, learning rate de 1e-3 e batch size de 128.</p> In\u00a0[53]: Copied! <pre>model = VAE().to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n</pre> model = VAE().to(device) optimizer = optim.Adam(model.parameters(), lr=1e-3) <p>Para a loss function, utilizei a Binary Cross Entropy (BCE) para medir a diferen\u00e7a entre a imagem original e a reconstru\u00edda, somada \u00e0 Kullback-Leibler Divergence (KLD) para regularizar o espa\u00e7o latente. Essas duas fun\u00e7\u00f5es de perda tamb\u00e9m foram recomendadas no github do pytorch, o que me enviesou um pouco a utiliz\u00e1-las.</p> In\u00a0[6]: Copied! <pre>def loss_function(recon_x, x, mu, logvar, reduction='sum'):\n    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=reduction)\n\n    # see Appendix B from VAE paper:\n    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n    # https://arxiv.org/abs/1312.6114\n    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    if reduction == 'mean':\n        KLD = KLD / x.size(0)\n\n    return BCE + KLD, BCE, KLD\n</pre> def loss_function(recon_x, x, mu, logvar, reduction='sum'):     BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=reduction)      # see Appendix B from VAE paper:     # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014     # https://arxiv.org/abs/1312.6114     # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)     KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())      if reduction == 'mean':         KLD = KLD / x.size(0)      return BCE + KLD, BCE, KLD In\u00a0[55]: Copied! <pre>from torchvision.utils import save_image\nimport os\n\nos.makedirs('results', exist_ok=True)\n\ndef train(epoch):\n    model.train()\n    train_total = 0.0\n    train_bce = 0.0\n    train_kld = 0.0\n    train_loss_total = 0\n    train_loss_bce = 0\n    train_loss_kld = 0\n\n    for batch_idx, (data, _) in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n\n        recon_batch, mu, logvar = model(data)\n        loss, bce, kld = loss_function(recon_batch, data, mu, logvar)  # unpack\n        loss.backward()\n        optimizer.step()\n\n        train_loss_total += loss.item() * len(data)\n        train_loss_bce += bce.item() * len(data)\n        train_loss_kld += kld.item() * len(data)\n\n        train_total += loss.item()\n        train_bce += bce.item()\n        train_kld += kld.item()\n\n        if batch_idx % log_interval == 0:\n            current_batch_size = data.size(0)\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tBCE: {:.6f}\\tKLD: {:.6f}'.format(\n                epoch, batch_idx * current_batch_size, len(train_loader.dataset),\n                100. * batch_idx / len(train_loader),\n                loss.item() / current_batch_size,\n                bce.item() / current_batch_size,\n                kld.item() / current_batch_size))\n\n    avg_loss = train_total / len(train_loader.dataset)\n    avg_bce = train_bce / len(train_loader.dataset)\n    avg_kld = train_kld / len(train_loader.dataset)\n\n    print('====&gt; Epoch: {} Average loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(\n          epoch, avg_loss, avg_bce, avg_kld))\n\n    avg_train_total = train_loss_total / len(train_loader.dataset)\n    avg_train_bce = train_loss_bce / len(train_loader.dataset)\n    avg_train_kld = train_loss_kld / len(train_loader.dataset)\n\n    vae_train_losses_total.append(avg_train_total)\n    vae_train_losses_bce.append(avg_train_bce)\n    vae_train_losses_kld.append(avg_train_kld)\n\n    print(f'Epoch {epoch}: Train Total: {avg_train_total:.4f}, BCE: {avg_train_bce:.4f}, KLD: {avg_train_kld:.4f}')\n    return avg_train_total, avg_train_bce, avg_train_kld\n\n\ndef validate(epoch):\n    \"\"\"\n    Usa val_loader para valida\u00e7\u00e3o por \u00e9poca. Mant\u00e9m formato de impress\u00e3o do tutorial.\n    \"\"\"\n    model.eval()\n    val_total = 0.0\n    val_bce = 0.0\n    val_kld = 0.0\n    val_loss_total = 0\n    val_loss_bce = 0\n    val_loss_kld = 0\n\n    with torch.no_grad():\n        for i, (data, _) in enumerate(val_loader):\n            data = data.to(device)\n            recon_batch, mu, logvar = model(data)\n            loss, bce, kld = loss_function(recon_batch, data, mu, logvar)  # unpack\n            val_total += loss.item()\n            val_bce += bce.item()\n            val_kld += kld.item()\n            val_loss_total += loss.item() * len(data)\n            val_loss_bce += bce.item() * len(data)\n            val_loss_kld += kld.item() * len(data)\n\n            if i == 0:\n                n = min(data.size(0), 8)\n                comparison = torch.cat([data[:n],\n                                        recon_batch.view(data.size(0), 1, 28, 28)[:n]])\n                save_image(comparison.cpu(),\n                           f'results/reconstruction_val_epoch{epoch}.png', nrow=n)\n\n    avg_loss = val_total / len(val_loader.dataset)\n    avg_bce = val_bce / len(val_loader.dataset)\n    avg_kld = val_kld / len(val_loader.dataset)\n\n    print('====&gt; Validation set loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(\n        avg_loss, avg_bce, avg_kld))\n\n    avg_val_total = val_loss_total / len(val_loader.dataset)\n    avg_val_bce = val_loss_bce / len(val_loader.dataset)\n    avg_val_kld = val_loss_kld / len(val_loader.dataset)\n\n    vae_val_losses_total.append(avg_val_total)\n    vae_val_losses_bce.append(avg_val_bce)\n    vae_val_losses_kld.append(avg_val_kld)\n\n    print(f'Validation Loss: {avg_val_total:.4f}, BCE: {avg_val_bce:.4f}, KLD: {avg_val_kld:.4f}')\n    return avg_val_total, avg_val_bce, avg_val_kld\n\n\ndef test():\n    model.eval()\n    test_total = 0.0\n    test_bce = 0.0\n    test_kld = 0.0\n\n    with torch.no_grad():\n        for i, (data, _) in enumerate(test_loader):\n            data = data.to(device)\n            recon_batch, mu, logvar = model(data)\n            loss, bce, kld = loss_function(recon_batch, data, mu, logvar)  # unpack\n            test_total += loss.item()\n            test_bce += bce.item()\n            test_kld += kld.item()\n\n            if i == 0:\n                n = min(data.size(0), 8)\n                comparison = torch.cat([\n                    data[:n],\n                    recon_batch.view(data.size(0), 1, 28, 28)[:n]\n                ])\n                save_image(comparison.cpu(),\n                           'results/reconstruction_test.png', nrow=n)\n\n    avg_loss = test_total / len(test_loader.dataset)\n    avg_bce = test_bce / len(test_loader.dataset)\n    avg_kld = test_kld / len(test_loader.dataset)\n\n    print('====&gt; Test set loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(\n        avg_loss, avg_bce, avg_kld))\n\n    return {\n        'Total Loss': avg_loss,\n        'BCE': avg_bce,\n        'KLD': avg_kld\n    }\n</pre> from torchvision.utils import save_image import os  os.makedirs('results', exist_ok=True)  def train(epoch):     model.train()     train_total = 0.0     train_bce = 0.0     train_kld = 0.0     train_loss_total = 0     train_loss_bce = 0     train_loss_kld = 0      for batch_idx, (data, _) in enumerate(train_loader):         data = data.to(device)         optimizer.zero_grad()          recon_batch, mu, logvar = model(data)         loss, bce, kld = loss_function(recon_batch, data, mu, logvar)  # unpack         loss.backward()         optimizer.step()          train_loss_total += loss.item() * len(data)         train_loss_bce += bce.item() * len(data)         train_loss_kld += kld.item() * len(data)          train_total += loss.item()         train_bce += bce.item()         train_kld += kld.item()          if batch_idx % log_interval == 0:             current_batch_size = data.size(0)             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tBCE: {:.6f}\\tKLD: {:.6f}'.format(                 epoch, batch_idx * current_batch_size, len(train_loader.dataset),                 100. * batch_idx / len(train_loader),                 loss.item() / current_batch_size,                 bce.item() / current_batch_size,                 kld.item() / current_batch_size))      avg_loss = train_total / len(train_loader.dataset)     avg_bce = train_bce / len(train_loader.dataset)     avg_kld = train_kld / len(train_loader.dataset)      print('====&gt; Epoch: {} Average loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(           epoch, avg_loss, avg_bce, avg_kld))      avg_train_total = train_loss_total / len(train_loader.dataset)     avg_train_bce = train_loss_bce / len(train_loader.dataset)     avg_train_kld = train_loss_kld / len(train_loader.dataset)      vae_train_losses_total.append(avg_train_total)     vae_train_losses_bce.append(avg_train_bce)     vae_train_losses_kld.append(avg_train_kld)      print(f'Epoch {epoch}: Train Total: {avg_train_total:.4f}, BCE: {avg_train_bce:.4f}, KLD: {avg_train_kld:.4f}')     return avg_train_total, avg_train_bce, avg_train_kld   def validate(epoch):     \"\"\"     Usa val_loader para valida\u00e7\u00e3o por \u00e9poca. Mant\u00e9m formato de impress\u00e3o do tutorial.     \"\"\"     model.eval()     val_total = 0.0     val_bce = 0.0     val_kld = 0.0     val_loss_total = 0     val_loss_bce = 0     val_loss_kld = 0      with torch.no_grad():         for i, (data, _) in enumerate(val_loader):             data = data.to(device)             recon_batch, mu, logvar = model(data)             loss, bce, kld = loss_function(recon_batch, data, mu, logvar)  # unpack             val_total += loss.item()             val_bce += bce.item()             val_kld += kld.item()             val_loss_total += loss.item() * len(data)             val_loss_bce += bce.item() * len(data)             val_loss_kld += kld.item() * len(data)              if i == 0:                 n = min(data.size(0), 8)                 comparison = torch.cat([data[:n],                                         recon_batch.view(data.size(0), 1, 28, 28)[:n]])                 save_image(comparison.cpu(),                            f'results/reconstruction_val_epoch{epoch}.png', nrow=n)      avg_loss = val_total / len(val_loader.dataset)     avg_bce = val_bce / len(val_loader.dataset)     avg_kld = val_kld / len(val_loader.dataset)      print('====&gt; Validation set loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(         avg_loss, avg_bce, avg_kld))      avg_val_total = val_loss_total / len(val_loader.dataset)     avg_val_bce = val_loss_bce / len(val_loader.dataset)     avg_val_kld = val_loss_kld / len(val_loader.dataset)      vae_val_losses_total.append(avg_val_total)     vae_val_losses_bce.append(avg_val_bce)     vae_val_losses_kld.append(avg_val_kld)      print(f'Validation Loss: {avg_val_total:.4f}, BCE: {avg_val_bce:.4f}, KLD: {avg_val_kld:.4f}')     return avg_val_total, avg_val_bce, avg_val_kld   def test():     model.eval()     test_total = 0.0     test_bce = 0.0     test_kld = 0.0      with torch.no_grad():         for i, (data, _) in enumerate(test_loader):             data = data.to(device)             recon_batch, mu, logvar = model(data)             loss, bce, kld = loss_function(recon_batch, data, mu, logvar)  # unpack             test_total += loss.item()             test_bce += bce.item()             test_kld += kld.item()              if i == 0:                 n = min(data.size(0), 8)                 comparison = torch.cat([                     data[:n],                     recon_batch.view(data.size(0), 1, 28, 28)[:n]                 ])                 save_image(comparison.cpu(),                            'results/reconstruction_test.png', nrow=n)      avg_loss = test_total / len(test_loader.dataset)     avg_bce = test_bce / len(test_loader.dataset)     avg_kld = test_kld / len(test_loader.dataset)      print('====&gt; Test set loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(         avg_loss, avg_bce, avg_kld))      return {         'Total Loss': avg_loss,         'BCE': avg_bce,         'KLD': avg_kld     }  In\u00a0[56]: Copied! <pre>for epoch in range(1, epochs + 1):\n    train(epoch)\n    validate(epoch)\n    with torch.no_grad():\n        sample = torch.randn(64, 20).to(device)\n        sample = model.decode(sample).cpu()\n        save_image(sample.view(64, 1, 28, 28),\n                    f'results/sample_' + str(epoch) + '.png')\n\nvae_test_metrics = test()\nprint(\"VAE Test Metrics:\", vae_test_metrics)\n</pre> for epoch in range(1, epochs + 1):     train(epoch)     validate(epoch)     with torch.no_grad():         sample = torch.randn(64, 20).to(device)         sample = model.decode(sample).cpu()         save_image(sample.view(64, 1, 28, 28),                     f'results/sample_' + str(epoch) + '.png')  vae_test_metrics = test() print(\"VAE Test Metrics:\", vae_test_metrics) <pre>Train Epoch: 1 [0/50000 (0%)]\tLoss: 550.211121\tBCE: 550.120728\tKLD: 0.090399\nTrain Epoch: 1 [1280/50000 (3%)]\tLoss: 289.226440\tBCE: 264.066772\tKLD: 25.159662\nTrain Epoch: 1 [2560/50000 (5%)]\tLoss: 243.657669\tBCE: 235.784790\tKLD: 7.872881\nTrain Epoch: 1 [3840/50000 (8%)]\tLoss: 218.838181\tBCE: 209.616791\tKLD: 9.221394\nTrain Epoch: 1 [5120/50000 (10%)]\tLoss: 215.704956\tBCE: 210.442398\tKLD: 5.262565\nTrain Epoch: 1 [6400/50000 (13%)]\tLoss: 211.141724\tBCE: 204.485077\tKLD: 6.656648\nTrain Epoch: 1 [7680/50000 (15%)]\tLoss: 202.255005\tBCE: 194.776535\tKLD: 7.478468\nTrain Epoch: 1 [8960/50000 (18%)]\tLoss: 195.976868\tBCE: 188.620850\tKLD: 7.356016\nTrain Epoch: 1 [10240/50000 (20%)]\tLoss: 195.914276\tBCE: 187.230042\tKLD: 8.684235\nTrain Epoch: 1 [11520/50000 (23%)]\tLoss: 189.182968\tBCE: 178.871475\tKLD: 10.311491\nTrain Epoch: 1 [12800/50000 (26%)]\tLoss: 177.554443\tBCE: 165.558319\tKLD: 11.996118\nTrain Epoch: 1 [14080/50000 (28%)]\tLoss: 179.909393\tBCE: 167.670441\tKLD: 12.238945\nTrain Epoch: 1 [15360/50000 (31%)]\tLoss: 169.322250\tBCE: 155.925308\tKLD: 13.396948\nTrain Epoch: 1 [16640/50000 (33%)]\tLoss: 167.260498\tBCE: 154.048309\tKLD: 13.212193\nTrain Epoch: 1 [17920/50000 (36%)]\tLoss: 164.198929\tBCE: 149.975464\tKLD: 14.223462\nTrain Epoch: 1 [19200/50000 (38%)]\tLoss: 160.270126\tBCE: 145.953094\tKLD: 14.317027\nTrain Epoch: 1 [20480/50000 (41%)]\tLoss: 163.723755\tBCE: 149.396469\tKLD: 14.327290\nTrain Epoch: 1 [21760/50000 (43%)]\tLoss: 155.971558\tBCE: 140.654739\tKLD: 15.316814\nTrain Epoch: 1 [23040/50000 (46%)]\tLoss: 160.914520\tBCE: 145.560379\tKLD: 15.354136\nTrain Epoch: 1 [24320/50000 (49%)]\tLoss: 153.888885\tBCE: 137.893677\tKLD: 15.995205\nTrain Epoch: 1 [25600/50000 (51%)]\tLoss: 150.000870\tBCE: 133.949600\tKLD: 16.051266\nTrain Epoch: 1 [26880/50000 (54%)]\tLoss: 149.451904\tBCE: 133.255707\tKLD: 16.196205\nTrain Epoch: 1 [28160/50000 (56%)]\tLoss: 155.134506\tBCE: 138.776016\tKLD: 16.358488\nTrain Epoch: 1 [29440/50000 (59%)]\tLoss: 147.109665\tBCE: 130.438934\tKLD: 16.670731\nTrain Epoch: 1 [30720/50000 (61%)]\tLoss: 143.125168\tBCE: 126.644897\tKLD: 16.480270\nTrain Epoch: 1 [32000/50000 (64%)]\tLoss: 144.106583\tBCE: 127.673889\tKLD: 16.432692\nTrain Epoch: 1 [33280/50000 (66%)]\tLoss: 141.602982\tBCE: 124.399246\tKLD: 17.203733\nTrain Epoch: 1 [34560/50000 (69%)]\tLoss: 143.468979\tBCE: 125.604149\tKLD: 17.864828\nTrain Epoch: 1 [35840/50000 (72%)]\tLoss: 145.110229\tBCE: 127.021675\tKLD: 18.088554\nTrain Epoch: 1 [37120/50000 (74%)]\tLoss: 140.804749\tBCE: 122.520889\tKLD: 18.283867\nTrain Epoch: 1 [38400/50000 (77%)]\tLoss: 137.867310\tBCE: 118.922646\tKLD: 18.944660\nTrain Epoch: 1 [39680/50000 (79%)]\tLoss: 134.770966\tBCE: 116.580002\tKLD: 18.190966\nTrain Epoch: 1 [40960/50000 (82%)]\tLoss: 141.029083\tBCE: 122.271667\tKLD: 18.757410\nTrain Epoch: 1 [42240/50000 (84%)]\tLoss: 137.586197\tBCE: 118.327324\tKLD: 19.258869\nTrain Epoch: 1 [43520/50000 (87%)]\tLoss: 131.637878\tBCE: 112.332832\tKLD: 19.305038\nTrain Epoch: 1 [44800/50000 (90%)]\tLoss: 136.631866\tBCE: 117.234406\tKLD: 19.397455\nTrain Epoch: 1 [46080/50000 (92%)]\tLoss: 129.228638\tBCE: 109.444397\tKLD: 19.784245\nTrain Epoch: 1 [47360/50000 (95%)]\tLoss: 135.786621\tBCE: 116.918144\tKLD: 18.868484\nTrain Epoch: 1 [48640/50000 (97%)]\tLoss: 134.206909\tBCE: 114.610153\tKLD: 19.596750\nTrain Epoch: 1 [31200/50000 (100%)]\tLoss: 131.246497\tBCE: 110.169275\tKLD: 21.077226\n====&gt; Epoch: 1 Average loss: 169.4902 (BCE: 154.6433, KLD: 14.8469)\nEpoch 1: Train Total: 21684.6635, BCE: 19785.8830, KLD: 1898.7805\n====&gt; Validation set loss: 132.1375 (BCE: 111.9984, KLD: 20.1391)\nValidation Loss: 16889.8639, BCE: 14315.4101, KLD: 2574.4538\nTrain Epoch: 2 [0/50000 (0%)]\tLoss: 135.702866\tBCE: 115.685745\tKLD: 20.017120\nTrain Epoch: 2 [1280/50000 (3%)]\tLoss: 129.625900\tBCE: 109.598648\tKLD: 20.027246\nTrain Epoch: 2 [2560/50000 (5%)]\tLoss: 129.648132\tBCE: 110.606506\tKLD: 19.041620\nTrain Epoch: 2 [3840/50000 (8%)]\tLoss: 132.641006\tBCE: 111.983810\tKLD: 20.657198\nTrain Epoch: 2 [5120/50000 (10%)]\tLoss: 134.883652\tBCE: 114.548592\tKLD: 20.335058\nTrain Epoch: 2 [6400/50000 (13%)]\tLoss: 132.100586\tBCE: 111.175209\tKLD: 20.925377\nTrain Epoch: 2 [7680/50000 (15%)]\tLoss: 127.826210\tBCE: 107.942307\tKLD: 19.883904\nTrain Epoch: 2 [8960/50000 (18%)]\tLoss: 128.945038\tBCE: 107.668457\tKLD: 21.276585\nTrain Epoch: 2 [10240/50000 (20%)]\tLoss: 126.793022\tBCE: 106.244141\tKLD: 20.548883\nTrain Epoch: 2 [11520/50000 (23%)]\tLoss: 125.410461\tBCE: 104.626297\tKLD: 20.784166\nTrain Epoch: 2 [12800/50000 (26%)]\tLoss: 126.053284\tBCE: 104.879570\tKLD: 21.173714\nTrain Epoch: 2 [14080/50000 (28%)]\tLoss: 129.545197\tBCE: 108.489609\tKLD: 21.055590\nTrain Epoch: 2 [15360/50000 (31%)]\tLoss: 126.833588\tBCE: 105.035400\tKLD: 21.798183\nTrain Epoch: 2 [16640/50000 (33%)]\tLoss: 124.237556\tBCE: 102.942001\tKLD: 21.295555\nTrain Epoch: 2 [17920/50000 (36%)]\tLoss: 122.654007\tBCE: 102.615700\tKLD: 20.038307\nTrain Epoch: 2 [19200/50000 (38%)]\tLoss: 128.330353\tBCE: 106.068420\tKLD: 22.261940\nTrain Epoch: 2 [20480/50000 (41%)]\tLoss: 114.687378\tBCE: 93.633148\tKLD: 21.054228\nTrain Epoch: 2 [21760/50000 (43%)]\tLoss: 123.614326\tBCE: 102.841240\tKLD: 20.773088\nTrain Epoch: 2 [23040/50000 (46%)]\tLoss: 122.893387\tBCE: 101.651100\tKLD: 21.242283\nTrain Epoch: 2 [24320/50000 (49%)]\tLoss: 124.291626\tBCE: 101.578125\tKLD: 22.713497\nTrain Epoch: 2 [25600/50000 (51%)]\tLoss: 119.751282\tBCE: 97.940262\tKLD: 21.811020\nTrain Epoch: 2 [26880/50000 (54%)]\tLoss: 123.426376\tBCE: 101.373230\tKLD: 22.053143\nTrain Epoch: 2 [28160/50000 (56%)]\tLoss: 125.357269\tBCE: 103.323532\tKLD: 22.033737\nTrain Epoch: 2 [29440/50000 (59%)]\tLoss: 121.870895\tBCE: 99.630249\tKLD: 22.240650\nTrain Epoch: 2 [30720/50000 (61%)]\tLoss: 122.744659\tBCE: 101.112640\tKLD: 21.632023\nTrain Epoch: 2 [32000/50000 (64%)]\tLoss: 126.175232\tBCE: 104.098969\tKLD: 22.076260\nTrain Epoch: 2 [33280/50000 (66%)]\tLoss: 118.942062\tBCE: 97.396271\tKLD: 21.545788\nTrain Epoch: 2 [34560/50000 (69%)]\tLoss: 122.733467\tBCE: 100.444702\tKLD: 22.288765\nTrain Epoch: 2 [35840/50000 (72%)]\tLoss: 116.940361\tBCE: 94.992538\tKLD: 21.947823\nTrain Epoch: 2 [37120/50000 (74%)]\tLoss: 122.095139\tBCE: 98.668365\tKLD: 23.426773\nTrain Epoch: 2 [38400/50000 (77%)]\tLoss: 123.915718\tBCE: 100.255371\tKLD: 23.660345\nTrain Epoch: 2 [39680/50000 (79%)]\tLoss: 123.203644\tBCE: 100.257050\tKLD: 22.946592\nTrain Epoch: 2 [40960/50000 (82%)]\tLoss: 124.686317\tBCE: 101.291672\tKLD: 23.394646\nTrain Epoch: 2 [42240/50000 (84%)]\tLoss: 121.265991\tBCE: 97.335846\tKLD: 23.930147\nTrain Epoch: 2 [43520/50000 (87%)]\tLoss: 117.734718\tBCE: 95.540710\tKLD: 22.194008\nTrain Epoch: 2 [44800/50000 (90%)]\tLoss: 121.918739\tBCE: 99.725555\tKLD: 22.193184\nTrain Epoch: 2 [46080/50000 (92%)]\tLoss: 120.328941\tBCE: 97.213310\tKLD: 23.115631\nTrain Epoch: 2 [47360/50000 (95%)]\tLoss: 120.813477\tBCE: 96.986542\tKLD: 23.826939\nTrain Epoch: 2 [48640/50000 (97%)]\tLoss: 123.491814\tBCE: 99.620895\tKLD: 23.870920\nTrain Epoch: 2 [31200/50000 (100%)]\tLoss: 116.767065\tBCE: 92.597754\tKLD: 24.169318\n====&gt; Epoch: 2 Average loss: 124.4355 (BCE: 102.6557, KLD: 21.7798)\nEpoch 2: Train Total: 15918.7808, BCE: 13132.8168, KLD: 2785.9640\n====&gt; Validation set loss: 119.0396 (BCE: 95.6804, KLD: 23.3592)\nValidation Loss: 15215.1566, BCE: 12229.1803, KLD: 2985.9764\nTrain Epoch: 3 [0/50000 (0%)]\tLoss: 118.983765\tBCE: 96.003853\tKLD: 22.979908\nTrain Epoch: 3 [1280/50000 (3%)]\tLoss: 115.270363\tBCE: 92.994133\tKLD: 22.276230\nTrain Epoch: 3 [2560/50000 (5%)]\tLoss: 120.576233\tBCE: 97.347977\tKLD: 23.228252\nTrain Epoch: 3 [3840/50000 (8%)]\tLoss: 115.354332\tBCE: 91.939873\tKLD: 23.414459\nTrain Epoch: 3 [5120/50000 (10%)]\tLoss: 115.257820\tBCE: 91.620102\tKLD: 23.637716\nTrain Epoch: 3 [6400/50000 (13%)]\tLoss: 120.351395\tBCE: 97.184723\tKLD: 23.166672\nTrain Epoch: 3 [7680/50000 (15%)]\tLoss: 119.801071\tBCE: 95.898727\tKLD: 23.902344\nTrain Epoch: 3 [8960/50000 (18%)]\tLoss: 115.417389\tBCE: 92.190414\tKLD: 23.226978\nTrain Epoch: 3 [10240/50000 (20%)]\tLoss: 116.495285\tBCE: 93.887657\tKLD: 22.607632\nTrain Epoch: 3 [11520/50000 (23%)]\tLoss: 119.291122\tBCE: 94.785065\tKLD: 24.506056\nTrain Epoch: 3 [12800/50000 (26%)]\tLoss: 119.122375\tBCE: 95.105255\tKLD: 24.017124\nTrain Epoch: 3 [14080/50000 (28%)]\tLoss: 114.211723\tBCE: 90.840027\tKLD: 23.371696\nTrain Epoch: 3 [15360/50000 (31%)]\tLoss: 115.090714\tBCE: 91.856758\tKLD: 23.233955\nTrain Epoch: 3 [16640/50000 (33%)]\tLoss: 113.924530\tBCE: 91.017235\tKLD: 22.907297\nTrain Epoch: 3 [17920/50000 (36%)]\tLoss: 112.161026\tBCE: 88.437508\tKLD: 23.723520\nTrain Epoch: 3 [19200/50000 (38%)]\tLoss: 119.531075\tBCE: 94.885841\tKLD: 24.645233\nTrain Epoch: 3 [20480/50000 (41%)]\tLoss: 115.625877\tBCE: 93.331665\tKLD: 22.294212\nTrain Epoch: 3 [21760/50000 (43%)]\tLoss: 115.690987\tBCE: 91.761314\tKLD: 23.929672\nTrain Epoch: 3 [23040/50000 (46%)]\tLoss: 115.236961\tBCE: 90.909897\tKLD: 24.327065\nTrain Epoch: 3 [24320/50000 (49%)]\tLoss: 112.380966\tBCE: 90.100227\tKLD: 22.280737\nTrain Epoch: 3 [25600/50000 (51%)]\tLoss: 116.310478\tBCE: 91.884033\tKLD: 24.426443\nTrain Epoch: 3 [26880/50000 (54%)]\tLoss: 114.566933\tBCE: 90.906494\tKLD: 23.660439\nTrain Epoch: 3 [28160/50000 (56%)]\tLoss: 119.632416\tBCE: 95.565933\tKLD: 24.066479\nTrain Epoch: 3 [29440/50000 (59%)]\tLoss: 118.577782\tBCE: 95.043411\tKLD: 23.534369\nTrain Epoch: 3 [30720/50000 (61%)]\tLoss: 116.307671\tBCE: 91.999405\tKLD: 24.308266\nTrain Epoch: 3 [32000/50000 (64%)]\tLoss: 112.912918\tBCE: 89.093102\tKLD: 23.819813\nTrain Epoch: 3 [33280/50000 (66%)]\tLoss: 116.139687\tBCE: 91.666718\tKLD: 24.472971\nTrain Epoch: 3 [34560/50000 (69%)]\tLoss: 116.825150\tBCE: 92.261139\tKLD: 24.564007\nTrain Epoch: 3 [35840/50000 (72%)]\tLoss: 111.790932\tBCE: 88.326401\tKLD: 23.464531\nTrain Epoch: 3 [37120/50000 (74%)]\tLoss: 110.515175\tBCE: 86.709694\tKLD: 23.805481\nTrain Epoch: 3 [38400/50000 (77%)]\tLoss: 120.302368\tBCE: 95.806984\tKLD: 24.495388\nTrain Epoch: 3 [39680/50000 (79%)]\tLoss: 111.414520\tBCE: 86.755486\tKLD: 24.659035\nTrain Epoch: 3 [40960/50000 (82%)]\tLoss: 112.677048\tBCE: 90.109245\tKLD: 22.567806\nTrain Epoch: 3 [42240/50000 (84%)]\tLoss: 119.957764\tBCE: 95.451706\tKLD: 24.506054\nTrain Epoch: 3 [43520/50000 (87%)]\tLoss: 114.973473\tBCE: 90.972656\tKLD: 24.000818\nTrain Epoch: 3 [44800/50000 (90%)]\tLoss: 116.880859\tBCE: 92.567596\tKLD: 24.313267\nTrain Epoch: 3 [46080/50000 (92%)]\tLoss: 115.525269\tBCE: 91.495331\tKLD: 24.029938\nTrain Epoch: 3 [47360/50000 (95%)]\tLoss: 119.840988\tBCE: 95.425308\tKLD: 24.415684\nTrain Epoch: 3 [48640/50000 (97%)]\tLoss: 115.501526\tBCE: 90.958557\tKLD: 24.542973\nTrain Epoch: 3 [31200/50000 (100%)]\tLoss: 113.692017\tBCE: 88.816162\tKLD: 24.875856\n====&gt; Epoch: 3 Average loss: 116.4143 (BCE: 92.6957, KLD: 23.7186)\nEpoch 3: Train Total: 14892.2962, BCE: 11858.2298, KLD: 3034.0664\n====&gt; Validation set loss: 114.6725 (BCE: 90.2791, KLD: 24.3935)\nValidation Loss: 14657.4540, BCE: 11539.3867, KLD: 3118.0673\nTrain Epoch: 4 [0/50000 (0%)]\tLoss: 113.097214\tBCE: 88.515579\tKLD: 24.581638\nTrain Epoch: 4 [1280/50000 (3%)]\tLoss: 114.328217\tBCE: 90.064438\tKLD: 24.263775\nTrain Epoch: 4 [2560/50000 (5%)]\tLoss: 115.551926\tBCE: 91.446381\tKLD: 24.105541\nTrain Epoch: 4 [3840/50000 (8%)]\tLoss: 117.094269\tBCE: 92.843513\tKLD: 24.250755\nTrain Epoch: 4 [5120/50000 (10%)]\tLoss: 114.400368\tBCE: 89.914734\tKLD: 24.485634\nTrain Epoch: 4 [6400/50000 (13%)]\tLoss: 113.392548\tBCE: 90.195084\tKLD: 23.197466\nTrain Epoch: 4 [7680/50000 (15%)]\tLoss: 112.276222\tBCE: 87.430679\tKLD: 24.845541\nTrain Epoch: 4 [8960/50000 (18%)]\tLoss: 116.798920\tBCE: 93.341507\tKLD: 23.457409\nTrain Epoch: 4 [10240/50000 (20%)]\tLoss: 110.827057\tBCE: 87.112854\tKLD: 23.714203\nTrain Epoch: 4 [11520/50000 (23%)]\tLoss: 119.734909\tBCE: 93.847992\tKLD: 25.886913\nTrain Epoch: 4 [12800/50000 (26%)]\tLoss: 110.277344\tBCE: 86.912735\tKLD: 23.364613\nTrain Epoch: 4 [14080/50000 (28%)]\tLoss: 114.593544\tBCE: 89.871689\tKLD: 24.721853\nTrain Epoch: 4 [15360/50000 (31%)]\tLoss: 117.321762\tBCE: 93.854813\tKLD: 23.466946\nTrain Epoch: 4 [16640/50000 (33%)]\tLoss: 113.161049\tBCE: 88.692749\tKLD: 24.468300\nTrain Epoch: 4 [17920/50000 (36%)]\tLoss: 106.608078\tBCE: 83.125366\tKLD: 23.482712\nTrain Epoch: 4 [19200/50000 (38%)]\tLoss: 111.527641\tBCE: 87.809830\tKLD: 23.717812\nTrain Epoch: 4 [20480/50000 (41%)]\tLoss: 115.927231\tBCE: 90.807632\tKLD: 25.119595\nTrain Epoch: 4 [21760/50000 (43%)]\tLoss: 117.702316\tBCE: 92.557083\tKLD: 25.145229\nTrain Epoch: 4 [23040/50000 (46%)]\tLoss: 113.902222\tBCE: 88.605476\tKLD: 25.296741\nTrain Epoch: 4 [24320/50000 (49%)]\tLoss: 110.354691\tBCE: 85.862854\tKLD: 24.491840\nTrain Epoch: 4 [25600/50000 (51%)]\tLoss: 118.031387\tBCE: 93.353363\tKLD: 24.678026\nTrain Epoch: 4 [26880/50000 (54%)]\tLoss: 113.526154\tBCE: 88.407967\tKLD: 25.118187\nTrain Epoch: 4 [28160/50000 (56%)]\tLoss: 112.606552\tBCE: 88.602463\tKLD: 24.004086\nTrain Epoch: 4 [29440/50000 (59%)]\tLoss: 107.680374\tBCE: 82.847214\tKLD: 24.833160\nTrain Epoch: 4 [30720/50000 (61%)]\tLoss: 115.930588\tBCE: 92.094231\tKLD: 23.836357\nTrain Epoch: 4 [32000/50000 (64%)]\tLoss: 109.142197\tBCE: 84.728073\tKLD: 24.414127\nTrain Epoch: 4 [33280/50000 (66%)]\tLoss: 112.717064\tBCE: 88.083832\tKLD: 24.633230\nTrain Epoch: 4 [34560/50000 (69%)]\tLoss: 112.374641\tBCE: 88.664253\tKLD: 23.710386\nTrain Epoch: 4 [35840/50000 (72%)]\tLoss: 112.847885\tBCE: 88.131081\tKLD: 24.716808\nTrain Epoch: 4 [37120/50000 (74%)]\tLoss: 115.486336\tBCE: 91.000183\tKLD: 24.486151\nTrain Epoch: 4 [38400/50000 (77%)]\tLoss: 117.263474\tBCE: 92.303337\tKLD: 24.960136\nTrain Epoch: 4 [39680/50000 (79%)]\tLoss: 111.202545\tBCE: 87.484421\tKLD: 23.718124\nTrain Epoch: 4 [40960/50000 (82%)]\tLoss: 113.658340\tBCE: 88.644066\tKLD: 25.014273\nTrain Epoch: 4 [42240/50000 (84%)]\tLoss: 113.830345\tBCE: 88.614960\tKLD: 25.215385\nTrain Epoch: 4 [43520/50000 (87%)]\tLoss: 109.403290\tBCE: 85.585220\tKLD: 23.818066\nTrain Epoch: 4 [44800/50000 (90%)]\tLoss: 114.138290\tBCE: 89.518242\tKLD: 24.620045\nTrain Epoch: 4 [46080/50000 (92%)]\tLoss: 110.796829\tBCE: 86.247284\tKLD: 24.549543\nTrain Epoch: 4 [47360/50000 (95%)]\tLoss: 109.790695\tBCE: 84.702141\tKLD: 25.088552\nTrain Epoch: 4 [48640/50000 (97%)]\tLoss: 112.164139\tBCE: 87.603821\tKLD: 24.560322\nTrain Epoch: 4 [31200/50000 (100%)]\tLoss: 108.785437\tBCE: 83.841638\tKLD: 24.943796\n====&gt; Epoch: 4 Average loss: 113.0687 (BCE: 88.6889, KLD: 24.3798)\nEpoch 4: Train Total: 14464.4423, BCE: 11345.7383, KLD: 3118.7041\n====&gt; Validation set loss: 112.2907 (BCE: 87.5642, KLD: 24.7265)\nValidation Loss: 14352.6780, BCE: 11191.9995, KLD: 3160.6784\nTrain Epoch: 5 [0/50000 (0%)]\tLoss: 113.488037\tBCE: 88.236572\tKLD: 25.251465\nTrain Epoch: 5 [1280/50000 (3%)]\tLoss: 111.975807\tBCE: 88.206520\tKLD: 23.769289\nTrain Epoch: 5 [2560/50000 (5%)]\tLoss: 114.127594\tBCE: 88.500534\tKLD: 25.627058\nTrain Epoch: 5 [3840/50000 (8%)]\tLoss: 113.850494\tBCE: 89.253571\tKLD: 24.596920\nTrain Epoch: 5 [5120/50000 (10%)]\tLoss: 110.570923\tBCE: 86.053444\tKLD: 24.517483\nTrain Epoch: 5 [6400/50000 (13%)]\tLoss: 111.459839\tBCE: 87.304535\tKLD: 24.155308\nTrain Epoch: 5 [7680/50000 (15%)]\tLoss: 108.444321\tBCE: 84.105240\tKLD: 24.339085\nTrain Epoch: 5 [8960/50000 (18%)]\tLoss: 110.068184\tBCE: 85.391006\tKLD: 24.677177\nTrain Epoch: 5 [10240/50000 (20%)]\tLoss: 111.241791\tBCE: 87.355255\tKLD: 23.886538\nTrain Epoch: 5 [11520/50000 (23%)]\tLoss: 116.545288\tBCE: 91.761780\tKLD: 24.783512\nTrain Epoch: 5 [12800/50000 (26%)]\tLoss: 112.369522\tBCE: 88.567566\tKLD: 23.801952\nTrain Epoch: 5 [14080/50000 (28%)]\tLoss: 114.026649\tBCE: 89.007751\tKLD: 25.018898\nTrain Epoch: 5 [15360/50000 (31%)]\tLoss: 110.402283\tBCE: 86.493164\tKLD: 23.909119\nTrain Epoch: 5 [16640/50000 (33%)]\tLoss: 111.761124\tBCE: 87.268173\tKLD: 24.492950\nTrain Epoch: 5 [17920/50000 (36%)]\tLoss: 110.991219\tBCE: 86.770584\tKLD: 24.220636\nTrain Epoch: 5 [19200/50000 (38%)]\tLoss: 111.682861\tBCE: 86.444000\tKLD: 25.238857\nTrain Epoch: 5 [20480/50000 (41%)]\tLoss: 111.863136\tBCE: 88.354279\tKLD: 23.508858\nTrain Epoch: 5 [21760/50000 (43%)]\tLoss: 110.050735\tBCE: 85.437164\tKLD: 24.613573\nTrain Epoch: 5 [23040/50000 (46%)]\tLoss: 110.773590\tBCE: 86.457222\tKLD: 24.316372\nTrain Epoch: 5 [24320/50000 (49%)]\tLoss: 111.345512\tBCE: 87.036346\tKLD: 24.309164\nTrain Epoch: 5 [25600/50000 (51%)]\tLoss: 112.164307\tBCE: 86.959549\tKLD: 25.204756\nTrain Epoch: 5 [26880/50000 (54%)]\tLoss: 115.372192\tBCE: 90.431091\tKLD: 24.941101\nTrain Epoch: 5 [28160/50000 (56%)]\tLoss: 110.286484\tBCE: 85.646767\tKLD: 24.639721\nTrain Epoch: 5 [29440/50000 (59%)]\tLoss: 109.529243\tBCE: 84.062172\tKLD: 25.467072\nTrain Epoch: 5 [30720/50000 (61%)]\tLoss: 108.787529\tBCE: 83.964142\tKLD: 24.823385\nTrain Epoch: 5 [32000/50000 (64%)]\tLoss: 107.202637\tBCE: 83.138847\tKLD: 24.063787\nTrain Epoch: 5 [33280/50000 (66%)]\tLoss: 109.502632\tBCE: 84.657272\tKLD: 24.845360\nTrain Epoch: 5 [34560/50000 (69%)]\tLoss: 112.409729\tBCE: 87.655457\tKLD: 24.754272\nTrain Epoch: 5 [35840/50000 (72%)]\tLoss: 109.564499\tBCE: 84.632126\tKLD: 24.932371\nTrain Epoch: 5 [37120/50000 (74%)]\tLoss: 108.262360\tBCE: 83.899353\tKLD: 24.363010\nTrain Epoch: 5 [38400/50000 (77%)]\tLoss: 111.979706\tBCE: 87.160461\tKLD: 24.819248\nTrain Epoch: 5 [39680/50000 (79%)]\tLoss: 111.794388\tBCE: 87.373016\tKLD: 24.421371\nTrain Epoch: 5 [40960/50000 (82%)]\tLoss: 110.842331\tBCE: 85.733856\tKLD: 25.108471\nTrain Epoch: 5 [42240/50000 (84%)]\tLoss: 119.023384\tBCE: 94.671196\tKLD: 24.352188\nTrain Epoch: 5 [43520/50000 (87%)]\tLoss: 111.611839\tBCE: 86.946503\tKLD: 24.665339\nTrain Epoch: 5 [44800/50000 (90%)]\tLoss: 115.076782\tBCE: 90.061714\tKLD: 25.015070\nTrain Epoch: 5 [46080/50000 (92%)]\tLoss: 106.958260\tBCE: 82.051697\tKLD: 24.906563\nTrain Epoch: 5 [47360/50000 (95%)]\tLoss: 109.971283\tBCE: 85.555305\tKLD: 24.415974\nTrain Epoch: 5 [48640/50000 (97%)]\tLoss: 112.393684\tBCE: 87.085999\tKLD: 25.307684\nTrain Epoch: 5 [31200/50000 (100%)]\tLoss: 105.550171\tBCE: 81.731476\tKLD: 23.818700\n====&gt; Epoch: 5 Average loss: 111.0873 (BCE: 86.4000, KLD: 24.6873)\nEpoch 5: Train Total: 14211.0733, BCE: 11052.9231, KLD: 3158.1502\n====&gt; Validation set loss: 110.7160 (BCE: 86.3240, KLD: 24.3920)\nValidation Loss: 14151.2037, BCE: 11033.2597, KLD: 3117.9440\nTrain Epoch: 6 [0/50000 (0%)]\tLoss: 109.089027\tBCE: 84.996674\tKLD: 24.092352\nTrain Epoch: 6 [1280/50000 (3%)]\tLoss: 114.246857\tBCE: 89.092178\tKLD: 25.154676\nTrain Epoch: 6 [2560/50000 (5%)]\tLoss: 112.018150\tBCE: 87.023857\tKLD: 24.994293\nTrain Epoch: 6 [3840/50000 (8%)]\tLoss: 110.307739\tBCE: 86.258102\tKLD: 24.049641\nTrain Epoch: 6 [5120/50000 (10%)]\tLoss: 110.305946\tBCE: 85.851791\tKLD: 24.454153\nTrain Epoch: 6 [6400/50000 (13%)]\tLoss: 107.782341\tBCE: 83.191238\tKLD: 24.591105\nTrain Epoch: 6 [7680/50000 (15%)]\tLoss: 114.231964\tBCE: 88.898331\tKLD: 25.333630\nTrain Epoch: 6 [8960/50000 (18%)]\tLoss: 110.807831\tBCE: 86.366211\tKLD: 24.441622\nTrain Epoch: 6 [10240/50000 (20%)]\tLoss: 110.156120\tBCE: 84.896194\tKLD: 25.259926\nTrain Epoch: 6 [11520/50000 (23%)]\tLoss: 112.315300\tBCE: 87.228821\tKLD: 25.086479\nTrain Epoch: 6 [12800/50000 (26%)]\tLoss: 109.752274\tBCE: 85.171097\tKLD: 24.581181\nTrain Epoch: 6 [14080/50000 (28%)]\tLoss: 110.869164\tBCE: 86.123032\tKLD: 24.746134\nTrain Epoch: 6 [15360/50000 (31%)]\tLoss: 107.362152\tBCE: 82.634827\tKLD: 24.727329\nTrain Epoch: 6 [16640/50000 (33%)]\tLoss: 107.143715\tBCE: 82.940056\tKLD: 24.203659\nTrain Epoch: 6 [17920/50000 (36%)]\tLoss: 109.533066\tBCE: 84.264313\tKLD: 25.268749\nTrain Epoch: 6 [19200/50000 (38%)]\tLoss: 107.611816\tBCE: 82.563995\tKLD: 25.047821\nTrain Epoch: 6 [20480/50000 (41%)]\tLoss: 111.860596\tBCE: 86.477966\tKLD: 25.382629\nTrain Epoch: 6 [21760/50000 (43%)]\tLoss: 104.885101\tBCE: 80.743492\tKLD: 24.141609\nTrain Epoch: 6 [23040/50000 (46%)]\tLoss: 109.933411\tBCE: 84.461548\tKLD: 25.471859\nTrain Epoch: 6 [24320/50000 (49%)]\tLoss: 106.768387\tBCE: 82.209419\tKLD: 24.558968\nTrain Epoch: 6 [25600/50000 (51%)]\tLoss: 113.604774\tBCE: 88.749481\tKLD: 24.855293\nTrain Epoch: 6 [26880/50000 (54%)]\tLoss: 109.759796\tBCE: 85.407547\tKLD: 24.352245\nTrain Epoch: 6 [28160/50000 (56%)]\tLoss: 107.698318\tBCE: 83.496826\tKLD: 24.201492\nTrain Epoch: 6 [29440/50000 (59%)]\tLoss: 112.305466\tBCE: 87.258087\tKLD: 25.047380\nTrain Epoch: 6 [30720/50000 (61%)]\tLoss: 109.668686\tBCE: 84.938889\tKLD: 24.729797\nTrain Epoch: 6 [32000/50000 (64%)]\tLoss: 109.818115\tBCE: 84.532043\tKLD: 25.286068\nTrain Epoch: 6 [33280/50000 (66%)]\tLoss: 113.031372\tBCE: 87.692307\tKLD: 25.339069\nTrain Epoch: 6 [34560/50000 (69%)]\tLoss: 106.167107\tBCE: 81.296455\tKLD: 24.870651\nTrain Epoch: 6 [35840/50000 (72%)]\tLoss: 109.294838\tBCE: 83.324600\tKLD: 25.970236\nTrain Epoch: 6 [37120/50000 (74%)]\tLoss: 107.358345\tBCE: 82.630714\tKLD: 24.727631\nTrain Epoch: 6 [38400/50000 (77%)]\tLoss: 105.990097\tBCE: 82.289635\tKLD: 23.700462\nTrain Epoch: 6 [39680/50000 (79%)]\tLoss: 109.452194\tBCE: 84.440346\tKLD: 25.011848\nTrain Epoch: 6 [40960/50000 (82%)]\tLoss: 108.696465\tBCE: 84.432198\tKLD: 24.264265\nTrain Epoch: 6 [42240/50000 (84%)]\tLoss: 107.197594\tBCE: 82.457367\tKLD: 24.740225\nTrain Epoch: 6 [43520/50000 (87%)]\tLoss: 109.542290\tBCE: 84.895081\tKLD: 24.647209\nTrain Epoch: 6 [44800/50000 (90%)]\tLoss: 114.437988\tBCE: 89.049759\tKLD: 25.388229\nTrain Epoch: 6 [46080/50000 (92%)]\tLoss: 108.653442\tBCE: 84.322845\tKLD: 24.330599\nTrain Epoch: 6 [47360/50000 (95%)]\tLoss: 111.088234\tBCE: 85.730957\tKLD: 25.357277\nTrain Epoch: 6 [48640/50000 (97%)]\tLoss: 110.841660\tBCE: 85.690453\tKLD: 25.151211\nTrain Epoch: 6 [31200/50000 (100%)]\tLoss: 110.528186\tBCE: 85.732922\tKLD: 24.795264\n====&gt; Epoch: 6 Average loss: 109.7799 (BCE: 84.8966, KLD: 24.8833)\nEpoch 6: Train Total: 14043.3371, BCE: 10860.1832, KLD: 3183.1539\n====&gt; Validation set loss: 109.5145 (BCE: 85.0602, KLD: 24.4543)\nValidation Loss: 13998.0802, BCE: 10872.2268, KLD: 3125.8534\nTrain Epoch: 7 [0/50000 (0%)]\tLoss: 107.675247\tBCE: 83.182083\tKLD: 24.493164\nTrain Epoch: 7 [1280/50000 (3%)]\tLoss: 109.296600\tBCE: 83.836891\tKLD: 25.459713\nTrain Epoch: 7 [2560/50000 (5%)]\tLoss: 109.203644\tBCE: 84.830032\tKLD: 24.373611\nTrain Epoch: 7 [3840/50000 (8%)]\tLoss: 108.268982\tBCE: 82.933243\tKLD: 25.335741\nTrain Epoch: 7 [5120/50000 (10%)]\tLoss: 110.190399\tBCE: 84.895798\tKLD: 25.294603\nTrain Epoch: 7 [6400/50000 (13%)]\tLoss: 108.565735\tBCE: 83.431633\tKLD: 25.134102\nTrain Epoch: 7 [7680/50000 (15%)]\tLoss: 109.196686\tBCE: 84.430450\tKLD: 24.766235\nTrain Epoch: 7 [8960/50000 (18%)]\tLoss: 108.438950\tBCE: 83.590981\tKLD: 24.847971\nTrain Epoch: 7 [10240/50000 (20%)]\tLoss: 108.531898\tBCE: 83.394424\tKLD: 25.137474\nTrain Epoch: 7 [11520/50000 (23%)]\tLoss: 107.719086\tBCE: 82.833786\tKLD: 24.885296\nTrain Epoch: 7 [12800/50000 (26%)]\tLoss: 107.743813\tBCE: 83.143517\tKLD: 24.600298\nTrain Epoch: 7 [14080/50000 (28%)]\tLoss: 104.757591\tBCE: 80.108841\tKLD: 24.648748\nTrain Epoch: 7 [15360/50000 (31%)]\tLoss: 114.618881\tBCE: 89.146835\tKLD: 25.472046\nTrain Epoch: 7 [16640/50000 (33%)]\tLoss: 115.412392\tBCE: 89.459503\tKLD: 25.952888\nTrain Epoch: 7 [17920/50000 (36%)]\tLoss: 102.843178\tBCE: 78.974327\tKLD: 23.868853\nTrain Epoch: 7 [19200/50000 (38%)]\tLoss: 112.298935\tBCE: 86.247711\tKLD: 26.051222\nTrain Epoch: 7 [20480/50000 (41%)]\tLoss: 107.553436\tBCE: 83.120941\tKLD: 24.432495\nTrain Epoch: 7 [21760/50000 (43%)]\tLoss: 112.388290\tBCE: 86.424149\tKLD: 25.964146\nTrain Epoch: 7 [23040/50000 (46%)]\tLoss: 107.950607\tBCE: 82.210434\tKLD: 25.740177\nTrain Epoch: 7 [24320/50000 (49%)]\tLoss: 106.719307\tBCE: 81.217079\tKLD: 25.502230\nTrain Epoch: 7 [25600/50000 (51%)]\tLoss: 108.206314\tBCE: 83.241089\tKLD: 24.965229\nTrain Epoch: 7 [26880/50000 (54%)]\tLoss: 108.364922\tBCE: 83.704544\tKLD: 24.660376\nTrain Epoch: 7 [28160/50000 (56%)]\tLoss: 108.472389\tBCE: 83.398209\tKLD: 25.074179\nTrain Epoch: 7 [29440/50000 (59%)]\tLoss: 109.595886\tBCE: 84.989105\tKLD: 24.606781\nTrain Epoch: 7 [30720/50000 (61%)]\tLoss: 107.244171\tBCE: 82.547974\tKLD: 24.696198\nTrain Epoch: 7 [32000/50000 (64%)]\tLoss: 106.693466\tBCE: 81.866211\tKLD: 24.827259\nTrain Epoch: 7 [33280/50000 (66%)]\tLoss: 105.903259\tBCE: 81.247009\tKLD: 24.656252\nTrain Epoch: 7 [34560/50000 (69%)]\tLoss: 108.591957\tBCE: 84.000465\tKLD: 24.591494\nTrain Epoch: 7 [35840/50000 (72%)]\tLoss: 104.416985\tBCE: 79.618553\tKLD: 24.798431\nTrain Epoch: 7 [37120/50000 (74%)]\tLoss: 109.533890\tBCE: 85.380333\tKLD: 24.153561\nTrain Epoch: 7 [38400/50000 (77%)]\tLoss: 108.805191\tBCE: 84.635971\tKLD: 24.169222\nTrain Epoch: 7 [39680/50000 (79%)]\tLoss: 114.172325\tBCE: 88.075516\tKLD: 26.096807\nTrain Epoch: 7 [40960/50000 (82%)]\tLoss: 113.218712\tBCE: 88.038322\tKLD: 25.180389\nTrain Epoch: 7 [42240/50000 (84%)]\tLoss: 111.211655\tBCE: 86.316185\tKLD: 24.895473\nTrain Epoch: 7 [43520/50000 (87%)]\tLoss: 109.335159\tBCE: 83.501923\tKLD: 25.833241\nTrain Epoch: 7 [44800/50000 (90%)]\tLoss: 105.653290\tBCE: 81.163406\tKLD: 24.489887\nTrain Epoch: 7 [46080/50000 (92%)]\tLoss: 110.406097\tBCE: 85.954742\tKLD: 24.451351\nTrain Epoch: 7 [47360/50000 (95%)]\tLoss: 107.218796\tBCE: 82.527168\tKLD: 24.691628\nTrain Epoch: 7 [48640/50000 (97%)]\tLoss: 107.967476\tBCE: 82.786865\tKLD: 25.180609\nTrain Epoch: 7 [31200/50000 (100%)]\tLoss: 105.797412\tBCE: 80.773499\tKLD: 25.023907\n====&gt; Epoch: 7 Average loss: 108.8128 (BCE: 83.8070, KLD: 25.0058)\nEpoch 7: Train Total: 13919.9133, BCE: 10721.0963, KLD: 3198.8170\n====&gt; Validation set loss: 108.6348 (BCE: 83.3697, KLD: 25.2650)\nValidation Loss: 13885.5163, BCE: 10656.0014, KLD: 3229.5149\nTrain Epoch: 8 [0/50000 (0%)]\tLoss: 110.374283\tBCE: 85.001350\tKLD: 25.372929\nTrain Epoch: 8 [1280/50000 (3%)]\tLoss: 105.417000\tBCE: 80.678764\tKLD: 24.738235\nTrain Epoch: 8 [2560/50000 (5%)]\tLoss: 105.393127\tBCE: 80.570618\tKLD: 24.822510\nTrain Epoch: 8 [3840/50000 (8%)]\tLoss: 109.082008\tBCE: 83.640434\tKLD: 25.441576\nTrain Epoch: 8 [5120/50000 (10%)]\tLoss: 106.989433\tBCE: 82.174820\tKLD: 24.814611\nTrain Epoch: 8 [6400/50000 (13%)]\tLoss: 107.643845\tBCE: 82.979767\tKLD: 24.664082\nTrain Epoch: 8 [7680/50000 (15%)]\tLoss: 110.915947\tBCE: 86.217751\tKLD: 24.698196\nTrain Epoch: 8 [8960/50000 (18%)]\tLoss: 105.339401\tBCE: 81.214584\tKLD: 24.124817\nTrain Epoch: 8 [10240/50000 (20%)]\tLoss: 103.865143\tBCE: 79.482979\tKLD: 24.382164\nTrain Epoch: 8 [11520/50000 (23%)]\tLoss: 108.179016\tBCE: 83.821136\tKLD: 24.357876\nTrain Epoch: 8 [12800/50000 (26%)]\tLoss: 103.396362\tBCE: 78.925934\tKLD: 24.470427\nTrain Epoch: 8 [14080/50000 (28%)]\tLoss: 108.255783\tBCE: 83.316704\tKLD: 24.939083\nTrain Epoch: 8 [15360/50000 (31%)]\tLoss: 108.227188\tBCE: 82.991913\tKLD: 25.235279\nTrain Epoch: 8 [16640/50000 (33%)]\tLoss: 105.730682\tBCE: 80.186371\tKLD: 25.544312\nTrain Epoch: 8 [17920/50000 (36%)]\tLoss: 106.413475\tBCE: 82.597336\tKLD: 23.816139\nTrain Epoch: 8 [19200/50000 (38%)]\tLoss: 109.191978\tBCE: 83.719543\tKLD: 25.472437\nTrain Epoch: 8 [20480/50000 (41%)]\tLoss: 111.651703\tBCE: 85.953850\tKLD: 25.697857\nTrain Epoch: 8 [21760/50000 (43%)]\tLoss: 105.337540\tBCE: 80.402489\tKLD: 24.935047\nTrain Epoch: 8 [23040/50000 (46%)]\tLoss: 106.163551\tBCE: 81.244278\tKLD: 24.919273\nTrain Epoch: 8 [24320/50000 (49%)]\tLoss: 106.849571\tBCE: 81.400047\tKLD: 25.449522\nTrain Epoch: 8 [25600/50000 (51%)]\tLoss: 104.103058\tBCE: 79.875420\tKLD: 24.227642\nTrain Epoch: 8 [26880/50000 (54%)]\tLoss: 108.835594\tBCE: 83.428963\tKLD: 25.406633\nTrain Epoch: 8 [28160/50000 (56%)]\tLoss: 111.069717\tBCE: 85.977036\tKLD: 25.092680\nTrain Epoch: 8 [29440/50000 (59%)]\tLoss: 106.984634\tBCE: 82.680969\tKLD: 24.303669\nTrain Epoch: 8 [30720/50000 (61%)]\tLoss: 107.983475\tBCE: 82.104599\tKLD: 25.878880\nTrain Epoch: 8 [32000/50000 (64%)]\tLoss: 110.448341\tBCE: 85.626190\tKLD: 24.822151\nTrain Epoch: 8 [33280/50000 (66%)]\tLoss: 103.842979\tBCE: 79.086548\tKLD: 24.756430\nTrain Epoch: 8 [34560/50000 (69%)]\tLoss: 108.493538\tBCE: 83.222839\tKLD: 25.270699\nTrain Epoch: 8 [35840/50000 (72%)]\tLoss: 108.826141\tBCE: 83.633453\tKLD: 25.192684\nTrain Epoch: 8 [37120/50000 (74%)]\tLoss: 106.443420\tBCE: 81.166771\tKLD: 25.276649\nTrain Epoch: 8 [38400/50000 (77%)]\tLoss: 109.268692\tBCE: 84.109833\tKLD: 25.158857\nTrain Epoch: 8 [39680/50000 (79%)]\tLoss: 106.966782\tBCE: 81.237137\tKLD: 25.729647\nTrain Epoch: 8 [40960/50000 (82%)]\tLoss: 105.090393\tBCE: 80.168213\tKLD: 24.922176\nTrain Epoch: 8 [42240/50000 (84%)]\tLoss: 108.178581\tBCE: 82.587997\tKLD: 25.590582\nTrain Epoch: 8 [43520/50000 (87%)]\tLoss: 106.313736\tBCE: 82.132446\tKLD: 24.181293\nTrain Epoch: 8 [44800/50000 (90%)]\tLoss: 103.550827\tBCE: 79.109444\tKLD: 24.441385\nTrain Epoch: 8 [46080/50000 (92%)]\tLoss: 107.806915\tBCE: 82.620049\tKLD: 25.186867\nTrain Epoch: 8 [47360/50000 (95%)]\tLoss: 105.435043\tBCE: 80.648438\tKLD: 24.786604\nTrain Epoch: 8 [48640/50000 (97%)]\tLoss: 113.063431\tBCE: 86.681847\tKLD: 26.381584\nTrain Epoch: 8 [31200/50000 (100%)]\tLoss: 107.489136\tBCE: 82.526184\tKLD: 24.962955\n====&gt; Epoch: 8 Average loss: 108.0394 (BCE: 82.9796, KLD: 25.0598)\nEpoch 8: Train Total: 13820.7925, BCE: 10615.0505, KLD: 3205.7420\n====&gt; Validation set loss: 108.0810 (BCE: 83.2428, KLD: 24.8382)\nValidation Loss: 13814.6979, BCE: 10639.8221, KLD: 3174.8758\nTrain Epoch: 9 [0/50000 (0%)]\tLoss: 110.359863\tBCE: 84.955124\tKLD: 25.404736\nTrain Epoch: 9 [1280/50000 (3%)]\tLoss: 111.235397\tBCE: 85.076324\tKLD: 26.159077\nTrain Epoch: 9 [2560/50000 (5%)]\tLoss: 108.281845\tBCE: 82.703186\tKLD: 25.578661\nTrain Epoch: 9 [3840/50000 (8%)]\tLoss: 106.587769\tBCE: 82.014519\tKLD: 24.573246\nTrain Epoch: 9 [5120/50000 (10%)]\tLoss: 105.441299\tBCE: 80.682678\tKLD: 24.758625\nTrain Epoch: 9 [6400/50000 (13%)]\tLoss: 108.459404\tBCE: 82.770050\tKLD: 25.689354\nTrain Epoch: 9 [7680/50000 (15%)]\tLoss: 106.840904\tBCE: 82.176910\tKLD: 24.663996\nTrain Epoch: 9 [8960/50000 (18%)]\tLoss: 108.040298\tBCE: 83.325157\tKLD: 24.715141\nTrain Epoch: 9 [10240/50000 (20%)]\tLoss: 110.976501\tBCE: 84.587776\tKLD: 26.388729\nTrain Epoch: 9 [11520/50000 (23%)]\tLoss: 110.072639\tBCE: 85.469559\tKLD: 24.603081\nTrain Epoch: 9 [12800/50000 (26%)]\tLoss: 106.670265\tBCE: 80.799271\tKLD: 25.870996\nTrain Epoch: 9 [14080/50000 (28%)]\tLoss: 106.481506\tBCE: 81.496895\tKLD: 24.984612\nTrain Epoch: 9 [15360/50000 (31%)]\tLoss: 109.478477\tBCE: 85.245583\tKLD: 24.232893\nTrain Epoch: 9 [16640/50000 (33%)]\tLoss: 107.958603\tBCE: 82.229370\tKLD: 25.729233\nTrain Epoch: 9 [17920/50000 (36%)]\tLoss: 108.178772\tBCE: 83.106339\tKLD: 25.072432\nTrain Epoch: 9 [19200/50000 (38%)]\tLoss: 108.346321\tBCE: 83.313797\tKLD: 25.032524\nTrain Epoch: 9 [20480/50000 (41%)]\tLoss: 104.512856\tBCE: 79.381844\tKLD: 25.131012\nTrain Epoch: 9 [21760/50000 (43%)]\tLoss: 109.663177\tBCE: 84.441322\tKLD: 25.221851\nTrain Epoch: 9 [23040/50000 (46%)]\tLoss: 108.834274\tBCE: 83.435234\tKLD: 25.399040\nTrain Epoch: 9 [24320/50000 (49%)]\tLoss: 109.640381\tBCE: 83.588348\tKLD: 26.052029\nTrain Epoch: 9 [25600/50000 (51%)]\tLoss: 109.869804\tBCE: 84.360550\tKLD: 25.509254\nTrain Epoch: 9 [26880/50000 (54%)]\tLoss: 108.859726\tBCE: 83.058060\tKLD: 25.801664\nTrain Epoch: 9 [28160/50000 (56%)]\tLoss: 104.726173\tBCE: 79.343704\tKLD: 25.382469\nTrain Epoch: 9 [29440/50000 (59%)]\tLoss: 106.284386\tBCE: 80.825470\tKLD: 25.458918\nTrain Epoch: 9 [30720/50000 (61%)]\tLoss: 105.441910\tBCE: 80.793999\tKLD: 24.647907\nTrain Epoch: 9 [32000/50000 (64%)]\tLoss: 108.690979\tBCE: 82.502304\tKLD: 26.188675\nTrain Epoch: 9 [33280/50000 (66%)]\tLoss: 108.908401\tBCE: 83.855026\tKLD: 25.053377\nTrain Epoch: 9 [34560/50000 (69%)]\tLoss: 106.421005\tBCE: 81.155930\tKLD: 25.265079\nTrain Epoch: 9 [35840/50000 (72%)]\tLoss: 103.261444\tBCE: 78.630692\tKLD: 24.630749\nTrain Epoch: 9 [37120/50000 (74%)]\tLoss: 109.860344\tBCE: 84.360443\tKLD: 25.499899\nTrain Epoch: 9 [38400/50000 (77%)]\tLoss: 106.584488\tBCE: 81.301544\tKLD: 25.282948\nTrain Epoch: 9 [39680/50000 (79%)]\tLoss: 109.263512\tBCE: 83.641220\tKLD: 25.622293\nTrain Epoch: 9 [40960/50000 (82%)]\tLoss: 109.256699\tBCE: 84.048615\tKLD: 25.208080\nTrain Epoch: 9 [42240/50000 (84%)]\tLoss: 108.165756\tBCE: 82.716209\tKLD: 25.449551\nTrain Epoch: 9 [43520/50000 (87%)]\tLoss: 109.471115\tBCE: 83.835724\tKLD: 25.635391\nTrain Epoch: 9 [44800/50000 (90%)]\tLoss: 108.760742\tBCE: 83.128418\tKLD: 25.632324\nTrain Epoch: 9 [46080/50000 (92%)]\tLoss: 107.231354\tBCE: 81.945747\tKLD: 25.285610\nTrain Epoch: 9 [47360/50000 (95%)]\tLoss: 105.240181\tBCE: 80.871078\tKLD: 24.369102\nTrain Epoch: 9 [48640/50000 (97%)]\tLoss: 106.490967\tBCE: 81.283447\tKLD: 25.207520\nTrain Epoch: 9 [31200/50000 (100%)]\tLoss: 110.908545\tBCE: 85.963770\tKLD: 24.944775\n====&gt; Epoch: 9 Average loss: 107.4854 (BCE: 82.3165, KLD: 25.1689)\nEpoch 9: Train Total: 13749.6114, BCE: 10529.9123, KLD: 3219.6991\n====&gt; Validation set loss: 108.0700 (BCE: 82.7804, KLD: 25.2896)\nValidation Loss: 13812.8211, BCE: 10580.2523, KLD: 3232.5689\nTrain Epoch: 10 [0/50000 (0%)]\tLoss: 106.607025\tBCE: 81.385956\tKLD: 25.221069\nTrain Epoch: 10 [1280/50000 (3%)]\tLoss: 106.062378\tBCE: 81.113144\tKLD: 24.949234\nTrain Epoch: 10 [2560/50000 (5%)]\tLoss: 110.670288\tBCE: 85.025452\tKLD: 25.644840\nTrain Epoch: 10 [3840/50000 (8%)]\tLoss: 102.512230\tBCE: 78.060379\tKLD: 24.451851\nTrain Epoch: 10 [5120/50000 (10%)]\tLoss: 108.497299\tBCE: 83.042938\tKLD: 25.454365\nTrain Epoch: 10 [6400/50000 (13%)]\tLoss: 106.696236\tBCE: 82.072632\tKLD: 24.623604\nTrain Epoch: 10 [7680/50000 (15%)]\tLoss: 112.211403\tBCE: 86.092026\tKLD: 26.119377\nTrain Epoch: 10 [8960/50000 (18%)]\tLoss: 108.025558\tBCE: 82.711075\tKLD: 25.314484\nTrain Epoch: 10 [10240/50000 (20%)]\tLoss: 108.231239\tBCE: 83.431389\tKLD: 24.799852\nTrain Epoch: 10 [11520/50000 (23%)]\tLoss: 108.684753\tBCE: 83.246971\tKLD: 25.437786\nTrain Epoch: 10 [12800/50000 (26%)]\tLoss: 107.889977\tBCE: 82.772995\tKLD: 25.116980\nTrain Epoch: 10 [14080/50000 (28%)]\tLoss: 105.767197\tBCE: 80.658905\tKLD: 25.108295\nTrain Epoch: 10 [15360/50000 (31%)]\tLoss: 110.423637\tBCE: 84.275978\tKLD: 26.147659\nTrain Epoch: 10 [16640/50000 (33%)]\tLoss: 108.122879\tBCE: 83.824249\tKLD: 24.298630\nTrain Epoch: 10 [17920/50000 (36%)]\tLoss: 110.723213\tBCE: 84.678085\tKLD: 26.045128\nTrain Epoch: 10 [19200/50000 (38%)]\tLoss: 108.043907\tBCE: 83.828362\tKLD: 24.215544\nTrain Epoch: 10 [20480/50000 (41%)]\tLoss: 102.204384\tBCE: 77.405273\tKLD: 24.799109\nTrain Epoch: 10 [21760/50000 (43%)]\tLoss: 111.812462\tBCE: 86.151932\tKLD: 25.660530\nTrain Epoch: 10 [23040/50000 (46%)]\tLoss: 104.710846\tBCE: 80.046204\tKLD: 24.664639\nTrain Epoch: 10 [24320/50000 (49%)]\tLoss: 104.632240\tBCE: 80.302017\tKLD: 24.330223\nTrain Epoch: 10 [25600/50000 (51%)]\tLoss: 108.067673\tBCE: 82.418076\tKLD: 25.649593\nTrain Epoch: 10 [26880/50000 (54%)]\tLoss: 105.009644\tBCE: 79.944885\tKLD: 25.064762\nTrain Epoch: 10 [28160/50000 (56%)]\tLoss: 100.399544\tBCE: 76.190765\tKLD: 24.208780\nTrain Epoch: 10 [29440/50000 (59%)]\tLoss: 107.467705\tBCE: 81.388107\tKLD: 26.079597\nTrain Epoch: 10 [30720/50000 (61%)]\tLoss: 110.562675\tBCE: 84.728149\tKLD: 25.834526\nTrain Epoch: 10 [32000/50000 (64%)]\tLoss: 109.943527\tBCE: 84.540604\tKLD: 25.402924\nTrain Epoch: 10 [33280/50000 (66%)]\tLoss: 106.446472\tBCE: 81.166267\tKLD: 25.280207\nTrain Epoch: 10 [34560/50000 (69%)]\tLoss: 110.750015\tBCE: 85.754189\tKLD: 24.995823\nTrain Epoch: 10 [35840/50000 (72%)]\tLoss: 105.068443\tBCE: 80.344978\tKLD: 24.723465\nTrain Epoch: 10 [37120/50000 (74%)]\tLoss: 107.844223\tBCE: 82.581604\tKLD: 25.262615\nTrain Epoch: 10 [38400/50000 (77%)]\tLoss: 106.013741\tBCE: 80.460693\tKLD: 25.553045\nTrain Epoch: 10 [39680/50000 (79%)]\tLoss: 105.521187\tBCE: 80.571487\tKLD: 24.949699\nTrain Epoch: 10 [40960/50000 (82%)]\tLoss: 102.160522\tBCE: 76.719971\tKLD: 25.440556\nTrain Epoch: 10 [42240/50000 (84%)]\tLoss: 103.367523\tBCE: 79.087311\tKLD: 24.280209\nTrain Epoch: 10 [43520/50000 (87%)]\tLoss: 111.419617\tBCE: 85.686188\tKLD: 25.733433\nTrain Epoch: 10 [44800/50000 (90%)]\tLoss: 107.293236\tBCE: 81.608635\tKLD: 25.684603\nTrain Epoch: 10 [46080/50000 (92%)]\tLoss: 106.172554\tBCE: 80.561737\tKLD: 25.610817\nTrain Epoch: 10 [47360/50000 (95%)]\tLoss: 105.778229\tBCE: 80.528931\tKLD: 25.249300\nTrain Epoch: 10 [48640/50000 (97%)]\tLoss: 106.682800\tBCE: 81.684113\tKLD: 24.998684\nTrain Epoch: 10 [31200/50000 (100%)]\tLoss: 108.509619\tBCE: 81.920935\tKLD: 26.588684\n====&gt; Epoch: 10 Average loss: 107.0307 (BCE: 81.8135, KLD: 25.2172)\nEpoch 10: Train Total: 13691.5981, BCE: 10465.8389, KLD: 3225.7592\n====&gt; Validation set loss: 107.4241 (BCE: 81.7805, KLD: 25.6436)\nValidation Loss: 13731.0363, BCE: 10453.2156, KLD: 3277.8207\n====&gt; Test set loss: 106.3869 (BCE: 80.6521, KLD: 25.7348)\nVAE Test Metrics: {'Total Loss': 106.38694890136719, 'BCE': 80.6521254638672, 'KLD': 25.73482388305664}\n</pre> In\u00a0[7]: Copied! <pre>class ConvVAE(nn.Module):\n    def __init__(self, z_dim=20):\n        super(ConvVAE, self).__init__()\n        self.z_dim = z_dim\n\n        # Encoder: Conv -&gt; BN -&gt; ReLU\n        self.enc_conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)   # -&gt; 32 x 14 x 14\n        self.enc_bn1   = nn.BatchNorm2d(32)\n        self.enc_conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # -&gt; 64 x 7 x 7\n        self.enc_bn2   = nn.BatchNorm2d(64)\n\n        self.fc_enc = nn.Linear(64 * 7 * 7, 256)\n        self.fc_mu = nn.Linear(256, z_dim)\n        self.fc_logvar = nn.Linear(256, z_dim)\n\n        # Decoder: z -&gt; fc -&gt; feature map -&gt; convtranspose -&gt; image\n        self.fc_dec = nn.Linear(z_dim, 64 * 7 * 7)\n\n        self.dec_deconv1 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)  # -&gt; 32 x 14 x 14\n        self.dec_bn1     = nn.BatchNorm2d(32)\n        self.dec_deconv2 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)  # -&gt; 16 x 28 x 28\n        self.dec_bn2     = nn.BatchNorm2d(16)\n        self.dec_conv_out = nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1)           # -&gt; 1 x 28 x 28\n\n    def encode(self, x):\n        h = F.relu(self.enc_bn1(self.enc_conv1(x)))\n        h = F.relu(self.enc_bn2(self.enc_conv2(h)))\n        h = h.view(h.size(0), -1)             # (B, 64*7*7)\n        h = F.relu(self.fc_enc(h))            # (B, 256)\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        h = F.relu(self.fc_dec(z))                         # (B, 64*7*7)\n        h = h.view(h.size(0), 64, 7, 7)                    # (B,64,7,7)\n        h = F.relu(self.dec_bn1(self.dec_deconv1(h)))      # (B,32,14,14)\n        h = F.relu(self.dec_bn2(self.dec_deconv2(h)))      # (B,16,28,28)\n        x_recon = torch.sigmoid(self.dec_conv_out(h))      # (B,1,28,28) in [0,1]\n        return x_recon\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        x_recon = self.decode(z)\n        return x_recon, mu, logvar\n</pre> class ConvVAE(nn.Module):     def __init__(self, z_dim=20):         super(ConvVAE, self).__init__()         self.z_dim = z_dim          # Encoder: Conv -&gt; BN -&gt; ReLU         self.enc_conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)   # -&gt; 32 x 14 x 14         self.enc_bn1   = nn.BatchNorm2d(32)         self.enc_conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # -&gt; 64 x 7 x 7         self.enc_bn2   = nn.BatchNorm2d(64)          self.fc_enc = nn.Linear(64 * 7 * 7, 256)         self.fc_mu = nn.Linear(256, z_dim)         self.fc_logvar = nn.Linear(256, z_dim)          # Decoder: z -&gt; fc -&gt; feature map -&gt; convtranspose -&gt; image         self.fc_dec = nn.Linear(z_dim, 64 * 7 * 7)          self.dec_deconv1 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)  # -&gt; 32 x 14 x 14         self.dec_bn1     = nn.BatchNorm2d(32)         self.dec_deconv2 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)  # -&gt; 16 x 28 x 28         self.dec_bn2     = nn.BatchNorm2d(16)         self.dec_conv_out = nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1)           # -&gt; 1 x 28 x 28      def encode(self, x):         h = F.relu(self.enc_bn1(self.enc_conv1(x)))         h = F.relu(self.enc_bn2(self.enc_conv2(h)))         h = h.view(h.size(0), -1)             # (B, 64*7*7)         h = F.relu(self.fc_enc(h))            # (B, 256)         mu = self.fc_mu(h)         logvar = self.fc_logvar(h)         return mu, logvar      def reparameterize(self, mu, logvar):         std = torch.exp(0.5 * logvar)         eps = torch.randn_like(std)         return mu + eps * std      def decode(self, z):         h = F.relu(self.fc_dec(z))                         # (B, 64*7*7)         h = h.view(h.size(0), 64, 7, 7)                    # (B,64,7,7)         h = F.relu(self.dec_bn1(self.dec_deconv1(h)))      # (B,32,14,14)         h = F.relu(self.dec_bn2(self.dec_deconv2(h)))      # (B,16,28,28)         x_recon = torch.sigmoid(self.dec_conv_out(h))      # (B,1,28,28) in [0,1]         return x_recon      def forward(self, x):         mu, logvar = self.encode(x)         z = self.reparameterize(mu, logvar)         x_recon = self.decode(z)         return x_recon, mu, logvar In\u00a0[8]: Copied! <pre>ConvVAE_model = ConvVAE(z_dim=20).to(device)\nconv_optimizer = optim.Adam(ConvVAE_model.parameters(), lr=3e-4)\n</pre> ConvVAE_model = ConvVAE(z_dim=20).to(device) conv_optimizer = optim.Adam(ConvVAE_model.parameters(), lr=3e-4) In\u00a0[9]: Copied! <pre>def Conv_loss_function(recon_x, x, mu, logvar, reduction='sum'):\n    recon_flat = recon_x.view(recon_x.size(0), -1)\n    x_flat = x.view(x.size(0), -1)\n    BCE = F.binary_cross_entropy(recon_flat, x_flat, reduction=reduction)\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    if reduction == 'mean':\n        KLD = KLD / x.size(0)\n    return BCE + KLD, BCE, KLD\n</pre> def Conv_loss_function(recon_x, x, mu, logvar, reduction='sum'):     recon_flat = recon_x.view(recon_x.size(0), -1)     x_flat = x.view(x.size(0), -1)     BCE = F.binary_cross_entropy(recon_flat, x_flat, reduction=reduction)     KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())     if reduction == 'mean':         KLD = KLD / x.size(0)     return BCE + KLD, BCE, KLD In\u00a0[10]: Copied! <pre>def ConvTrain(epoch):\n    ConvVAE_model.train()\n    train_loss_total = 0\n    train_loss_bce = 0\n    train_loss_kld = 0\n\n    for batch_idx, (data, _) in enumerate(train_loader):\n        data = data.to(device)\n        conv_optimizer.zero_grad()\n        recon_batch, mu, logvar = ConvVAE_model(data)\n        loss, bce, kld = Conv_loss_function(recon_batch, data, mu, logvar)\n        loss.backward()\n        conv_optimizer.step()\n\n        train_loss_total += loss.item() * len(data)\n        train_loss_bce += bce.item() * len(data)\n        train_loss_kld += kld.item() * len(data)\n\n        if batch_idx % log_interval == 0:\n            current_batch_size = data.size(0)\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tBCE: {:.6f}\\tKLD: {:.6f}'.format(\n                epoch, batch_idx * current_batch_size, len(train_loader.dataset),\n                100. * batch_idx / len(train_loader),\n                loss.item() / current_batch_size,\n                bce.item() / current_batch_size,\n                kld.item() / current_batch_size))\n\n    avg_train_total = train_loss_total / len(train_loader.dataset)\n    avg_train_bce = train_loss_bce / len(train_loader.dataset)\n    avg_train_kld = train_loss_kld / len(train_loader.dataset)\n\n    conv_train_losses_total.append(avg_train_total)\n    conv_train_losses_bce.append(avg_train_bce)\n    conv_train_losses_kld.append(avg_train_kld)\n\n    print('====&gt; Epoch: {} Average loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(\n          epoch, avg_train_total, avg_train_bce, avg_train_kld))\n\ndef ConvValidate(epoch):\n    ConvVAE_model.eval()\n    val_total = 0.0\n    val_bce = 0.0\n    val_kld = 0.0\n    val_loss_total = 0\n    val_loss_bce = 0\n    val_loss_kld = 0\n\n    with torch.no_grad():\n        for i, (data, _) in enumerate(val_loader):\n            data = data.to(device)\n            recon_batch, mu, logvar = ConvVAE_model(data)\n            loss, bce, kld = Conv_loss_function(recon_batch, data, mu, logvar)\n            val_loss_total += loss.item() * len(data)\n            val_loss_bce += bce.item() * len(data)\n            val_loss_kld += kld.item() * len(data)\n\n            if i == 0:\n                n = min(data.size(0), 8)\n                comparison = torch.cat([data[:n].cpu(), recon_batch[:n].cpu()])\n                save_image(comparison, f'results_Conv/reconstruction_val_epoch{epoch}.png', nrow=n)\n\n    avg_val_total = val_loss_total / len(val_loader.dataset)\n    avg_val_bce = val_loss_bce / len(val_loader.dataset)\n    avg_val_kld = val_loss_kld / len(val_loader.dataset)\n\n    conv_val_losses_total.append(avg_val_total)\n    conv_val_losses_bce.append(avg_val_bce)\n    conv_val_losses_kld.append(avg_val_kld)\n\n    print('====&gt; Validation set loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(\n        avg_val_total, avg_val_bce, avg_val_kld))\n\ndef ConvTest():\n    ConvVAE_model.eval()\n    test_total = 0.0\n    test_bce = 0.0\n    test_kld = 0.0\n\n    with torch.no_grad():\n        for i, (data, _) in enumerate(test_loader):\n            data = data.to(device)\n            recon_batch, mu, logvar = ConvVAE_model(data)\n            loss, bce, kld = Conv_loss_function(recon_batch, data, mu, logvar)\n            test_total += loss.item()\n            test_bce += bce.item()\n            test_kld += kld.item()\n\n            if i == 0:\n                n = min(data.size(0), 8)\n                comparison = torch.cat([data[:n].cpu(), recon_batch[:n].cpu()])\n                save_image(comparison, 'results_Conv/reconstruction_test.png', nrow=n)\n\n    avg_loss = test_total / len(test_loader.dataset)\n    avg_bce = test_bce / len(test_loader.dataset)\n    avg_kld = test_kld / len(test_loader.dataset)\n    print('====&gt; Test set loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(\n        avg_loss, avg_bce, avg_kld))\n\n    return {\n        'Total Loss': avg_loss,\n        'BCE': avg_bce,\n        'KLD': avg_kld\n    }\n</pre> def ConvTrain(epoch):     ConvVAE_model.train()     train_loss_total = 0     train_loss_bce = 0     train_loss_kld = 0      for batch_idx, (data, _) in enumerate(train_loader):         data = data.to(device)         conv_optimizer.zero_grad()         recon_batch, mu, logvar = ConvVAE_model(data)         loss, bce, kld = Conv_loss_function(recon_batch, data, mu, logvar)         loss.backward()         conv_optimizer.step()          train_loss_total += loss.item() * len(data)         train_loss_bce += bce.item() * len(data)         train_loss_kld += kld.item() * len(data)          if batch_idx % log_interval == 0:             current_batch_size = data.size(0)             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tBCE: {:.6f}\\tKLD: {:.6f}'.format(                 epoch, batch_idx * current_batch_size, len(train_loader.dataset),                 100. * batch_idx / len(train_loader),                 loss.item() / current_batch_size,                 bce.item() / current_batch_size,                 kld.item() / current_batch_size))      avg_train_total = train_loss_total / len(train_loader.dataset)     avg_train_bce = train_loss_bce / len(train_loader.dataset)     avg_train_kld = train_loss_kld / len(train_loader.dataset)      conv_train_losses_total.append(avg_train_total)     conv_train_losses_bce.append(avg_train_bce)     conv_train_losses_kld.append(avg_train_kld)      print('====&gt; Epoch: {} Average loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(           epoch, avg_train_total, avg_train_bce, avg_train_kld))  def ConvValidate(epoch):     ConvVAE_model.eval()     val_total = 0.0     val_bce = 0.0     val_kld = 0.0     val_loss_total = 0     val_loss_bce = 0     val_loss_kld = 0      with torch.no_grad():         for i, (data, _) in enumerate(val_loader):             data = data.to(device)             recon_batch, mu, logvar = ConvVAE_model(data)             loss, bce, kld = Conv_loss_function(recon_batch, data, mu, logvar)             val_loss_total += loss.item() * len(data)             val_loss_bce += bce.item() * len(data)             val_loss_kld += kld.item() * len(data)              if i == 0:                 n = min(data.size(0), 8)                 comparison = torch.cat([data[:n].cpu(), recon_batch[:n].cpu()])                 save_image(comparison, f'results_Conv/reconstruction_val_epoch{epoch}.png', nrow=n)      avg_val_total = val_loss_total / len(val_loader.dataset)     avg_val_bce = val_loss_bce / len(val_loader.dataset)     avg_val_kld = val_loss_kld / len(val_loader.dataset)      conv_val_losses_total.append(avg_val_total)     conv_val_losses_bce.append(avg_val_bce)     conv_val_losses_kld.append(avg_val_kld)      print('====&gt; Validation set loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(         avg_val_total, avg_val_bce, avg_val_kld))  def ConvTest():     ConvVAE_model.eval()     test_total = 0.0     test_bce = 0.0     test_kld = 0.0      with torch.no_grad():         for i, (data, _) in enumerate(test_loader):             data = data.to(device)             recon_batch, mu, logvar = ConvVAE_model(data)             loss, bce, kld = Conv_loss_function(recon_batch, data, mu, logvar)             test_total += loss.item()             test_bce += bce.item()             test_kld += kld.item()              if i == 0:                 n = min(data.size(0), 8)                 comparison = torch.cat([data[:n].cpu(), recon_batch[:n].cpu()])                 save_image(comparison, 'results_Conv/reconstruction_test.png', nrow=n)      avg_loss = test_total / len(test_loader.dataset)     avg_bce = test_bce / len(test_loader.dataset)     avg_kld = test_kld / len(test_loader.dataset)     print('====&gt; Test set loss: {:.4f} (BCE: {:.4f}, KLD: {:.4f})'.format(         avg_loss, avg_bce, avg_kld))      return {         'Total Loss': avg_loss,         'BCE': avg_bce,         'KLD': avg_kld     }  In\u00a0[11]: Copied! <pre>os.makedirs('results_Conv', exist_ok=True)\nfor epoch in range(1, epochs + 1):\n    ConvTrain(epoch)\n    ConvValidate(epoch)\n    with torch.no_grad():\n        sample = torch.randn(64, ConvVAE_model.z_dim).to(device)\n        sample = ConvVAE_model.decode(sample).cpu()\n        save_image(sample.view(64, 1, 28, 28),\n                   f'results_Conv/sample_' + str(epoch) + '.png')\n\nconv_test_metrics = ConvTest()\nprint(\"Convolutional VAE Test Metrics:\", conv_test_metrics)\n</pre> os.makedirs('results_Conv', exist_ok=True) for epoch in range(1, epochs + 1):     ConvTrain(epoch)     ConvValidate(epoch)     with torch.no_grad():         sample = torch.randn(64, ConvVAE_model.z_dim).to(device)         sample = ConvVAE_model.decode(sample).cpu()         save_image(sample.view(64, 1, 28, 28),                    f'results_Conv/sample_' + str(epoch) + '.png')  conv_test_metrics = ConvTest() print(\"Convolutional VAE Test Metrics:\", conv_test_metrics) <pre>Train Epoch: 1 [0/50000 (0%)]\tLoss: 549.112671\tBCE: 548.661133\tKLD: 0.451556\nTrain Epoch: 1 [1280/50000 (3%)]\tLoss: 485.298859\tBCE: 483.433624\tKLD: 1.865246\nTrain Epoch: 1 [2560/50000 (5%)]\tLoss: 430.186676\tBCE: 426.639404\tKLD: 3.547277\nTrain Epoch: 1 [3840/50000 (8%)]\tLoss: 381.358246\tBCE: 376.839783\tKLD: 4.518462\nTrain Epoch: 1 [5120/50000 (10%)]\tLoss: 342.513031\tBCE: 337.297577\tKLD: 5.215454\nTrain Epoch: 1 [6400/50000 (13%)]\tLoss: 313.038696\tBCE: 307.433868\tKLD: 5.604815\nTrain Epoch: 1 [7680/50000 (15%)]\tLoss: 289.285187\tBCE: 283.315155\tKLD: 5.970038\nTrain Epoch: 1 [8960/50000 (18%)]\tLoss: 266.366974\tBCE: 259.230133\tKLD: 7.136844\nTrain Epoch: 1 [10240/50000 (20%)]\tLoss: 252.472321\tBCE: 244.837402\tKLD: 7.634912\nTrain Epoch: 1 [11520/50000 (23%)]\tLoss: 239.452530\tBCE: 230.835571\tKLD: 8.616958\nTrain Epoch: 1 [12800/50000 (26%)]\tLoss: 229.623428\tBCE: 220.918640\tKLD: 8.704792\nTrain Epoch: 1 [14080/50000 (28%)]\tLoss: 221.135178\tBCE: 211.247742\tKLD: 9.887436\nTrain Epoch: 1 [15360/50000 (31%)]\tLoss: 209.762634\tBCE: 198.884155\tKLD: 10.878486\nTrain Epoch: 1 [16640/50000 (33%)]\tLoss: 209.528214\tBCE: 198.985107\tKLD: 10.543106\nTrain Epoch: 1 [17920/50000 (36%)]\tLoss: 201.109924\tBCE: 189.777252\tKLD: 11.332674\nTrain Epoch: 1 [19200/50000 (38%)]\tLoss: 192.567551\tBCE: 181.271011\tKLD: 11.296543\nTrain Epoch: 1 [20480/50000 (41%)]\tLoss: 189.797546\tBCE: 177.138641\tKLD: 12.658911\nTrain Epoch: 1 [21760/50000 (43%)]\tLoss: 188.560623\tBCE: 175.785736\tKLD: 12.774887\nTrain Epoch: 1 [23040/50000 (46%)]\tLoss: 180.211472\tBCE: 167.174744\tKLD: 13.036727\nTrain Epoch: 1 [24320/50000 (49%)]\tLoss: 176.183090\tBCE: 163.206970\tKLD: 12.976113\nTrain Epoch: 1 [25600/50000 (51%)]\tLoss: 176.918076\tBCE: 163.766098\tKLD: 13.151971\nTrain Epoch: 1 [26880/50000 (54%)]\tLoss: 177.224960\tBCE: 163.143753\tKLD: 14.081207\nTrain Epoch: 1 [28160/50000 (56%)]\tLoss: 170.449524\tBCE: 155.789383\tKLD: 14.660143\nTrain Epoch: 1 [29440/50000 (59%)]\tLoss: 170.489075\tBCE: 155.676422\tKLD: 14.812652\nTrain Epoch: 1 [30720/50000 (61%)]\tLoss: 164.640320\tBCE: 150.383789\tKLD: 14.256536\nTrain Epoch: 1 [32000/50000 (64%)]\tLoss: 168.166916\tBCE: 153.093079\tKLD: 15.073838\nTrain Epoch: 1 [33280/50000 (66%)]\tLoss: 161.305573\tBCE: 145.390289\tKLD: 15.915291\nTrain Epoch: 1 [34560/50000 (69%)]\tLoss: 159.739395\tBCE: 143.746552\tKLD: 15.992844\nTrain Epoch: 1 [35840/50000 (72%)]\tLoss: 157.074509\tBCE: 140.716400\tKLD: 16.358110\nTrain Epoch: 1 [37120/50000 (74%)]\tLoss: 158.383209\tBCE: 141.807480\tKLD: 16.575733\nTrain Epoch: 1 [38400/50000 (77%)]\tLoss: 146.951172\tBCE: 129.819016\tKLD: 17.132149\nTrain Epoch: 1 [39680/50000 (79%)]\tLoss: 152.357162\tBCE: 135.291229\tKLD: 17.065939\nTrain Epoch: 1 [40960/50000 (82%)]\tLoss: 150.147537\tBCE: 131.974274\tKLD: 18.173267\nTrain Epoch: 1 [42240/50000 (84%)]\tLoss: 147.130020\tBCE: 129.312531\tKLD: 17.817488\nTrain Epoch: 1 [43520/50000 (87%)]\tLoss: 144.592987\tBCE: 126.583603\tKLD: 18.009380\nTrain Epoch: 1 [44800/50000 (90%)]\tLoss: 142.493668\tBCE: 124.107124\tKLD: 18.386539\nTrain Epoch: 1 [46080/50000 (92%)]\tLoss: 139.405502\tBCE: 121.087303\tKLD: 18.318195\nTrain Epoch: 1 [47360/50000 (95%)]\tLoss: 138.462952\tBCE: 119.374344\tKLD: 19.088600\nTrain Epoch: 1 [48640/50000 (97%)]\tLoss: 139.919525\tBCE: 120.577103\tKLD: 19.342426\nTrain Epoch: 1 [31200/50000 (100%)]\tLoss: 138.660193\tBCE: 119.400781\tKLD: 19.259415\n====&gt; Epoch: 1 Average loss: 27310.4538 (BCE: 25717.1249, KLD: 1593.3289)\n====&gt; Validation set loss: 17526.3713 (BCE: 15105.8155, KLD: 2420.5557)\nTrain Epoch: 2 [0/50000 (0%)]\tLoss: 138.368622\tBCE: 118.940262\tKLD: 19.428360\nTrain Epoch: 2 [1280/50000 (3%)]\tLoss: 139.070007\tBCE: 119.058838\tKLD: 20.011175\nTrain Epoch: 2 [2560/50000 (5%)]\tLoss: 136.289612\tBCE: 116.359444\tKLD: 19.930170\nTrain Epoch: 2 [3840/50000 (8%)]\tLoss: 129.289673\tBCE: 109.378036\tKLD: 19.911634\nTrain Epoch: 2 [5120/50000 (10%)]\tLoss: 132.685669\tBCE: 112.537262\tKLD: 20.148403\nTrain Epoch: 2 [6400/50000 (13%)]\tLoss: 131.879486\tBCE: 111.404724\tKLD: 20.474770\nTrain Epoch: 2 [7680/50000 (15%)]\tLoss: 129.944656\tBCE: 109.530823\tKLD: 20.413828\nTrain Epoch: 2 [8960/50000 (18%)]\tLoss: 128.978439\tBCE: 108.105179\tKLD: 20.873264\nTrain Epoch: 2 [10240/50000 (20%)]\tLoss: 128.697845\tBCE: 107.584572\tKLD: 21.113276\nTrain Epoch: 2 [11520/50000 (23%)]\tLoss: 133.420197\tBCE: 112.364365\tKLD: 21.055826\nTrain Epoch: 2 [12800/50000 (26%)]\tLoss: 126.226105\tBCE: 104.925713\tKLD: 21.300392\nTrain Epoch: 2 [14080/50000 (28%)]\tLoss: 131.077484\tBCE: 109.356125\tKLD: 21.721355\nTrain Epoch: 2 [15360/50000 (31%)]\tLoss: 126.692062\tBCE: 105.064423\tKLD: 21.627636\nTrain Epoch: 2 [16640/50000 (33%)]\tLoss: 124.873909\tBCE: 103.481384\tKLD: 21.392525\nTrain Epoch: 2 [17920/50000 (36%)]\tLoss: 126.711594\tBCE: 104.625336\tKLD: 22.086254\nTrain Epoch: 2 [19200/50000 (38%)]\tLoss: 122.711922\tBCE: 101.689560\tKLD: 21.022362\nTrain Epoch: 2 [20480/50000 (41%)]\tLoss: 118.704163\tBCE: 96.583031\tKLD: 22.121132\nTrain Epoch: 2 [21760/50000 (43%)]\tLoss: 122.163208\tBCE: 100.216148\tKLD: 21.947063\nTrain Epoch: 2 [23040/50000 (46%)]\tLoss: 125.730865\tBCE: 103.143784\tKLD: 22.587086\nTrain Epoch: 2 [24320/50000 (49%)]\tLoss: 126.304909\tBCE: 103.566887\tKLD: 22.738022\nTrain Epoch: 2 [25600/50000 (51%)]\tLoss: 128.320892\tBCE: 105.913597\tKLD: 22.407295\nTrain Epoch: 2 [26880/50000 (54%)]\tLoss: 124.452072\tBCE: 101.989014\tKLD: 22.463058\nTrain Epoch: 2 [28160/50000 (56%)]\tLoss: 122.828293\tBCE: 99.595589\tKLD: 23.232704\nTrain Epoch: 2 [29440/50000 (59%)]\tLoss: 127.836517\tBCE: 105.234863\tKLD: 22.601652\nTrain Epoch: 2 [30720/50000 (61%)]\tLoss: 123.425079\tBCE: 100.743782\tKLD: 22.681301\nTrain Epoch: 2 [32000/50000 (64%)]\tLoss: 123.392792\tBCE: 100.445946\tKLD: 22.946842\nTrain Epoch: 2 [33280/50000 (66%)]\tLoss: 118.988770\tBCE: 96.155952\tKLD: 22.832819\nTrain Epoch: 2 [34560/50000 (69%)]\tLoss: 122.492210\tBCE: 99.738571\tKLD: 22.753641\nTrain Epoch: 2 [35840/50000 (72%)]\tLoss: 121.169937\tBCE: 97.789200\tKLD: 23.380735\nTrain Epoch: 2 [37120/50000 (74%)]\tLoss: 119.700302\tBCE: 96.623253\tKLD: 23.077047\nTrain Epoch: 2 [38400/50000 (77%)]\tLoss: 115.867477\tBCE: 92.952286\tKLD: 22.915188\nTrain Epoch: 2 [39680/50000 (79%)]\tLoss: 121.534981\tBCE: 98.390244\tKLD: 23.144735\nTrain Epoch: 2 [40960/50000 (82%)]\tLoss: 118.326538\tBCE: 95.428329\tKLD: 22.898211\nTrain Epoch: 2 [42240/50000 (84%)]\tLoss: 120.265686\tBCE: 96.747917\tKLD: 23.517771\nTrain Epoch: 2 [43520/50000 (87%)]\tLoss: 116.210838\tBCE: 93.386597\tKLD: 22.824240\nTrain Epoch: 2 [44800/50000 (90%)]\tLoss: 119.716202\tBCE: 95.731148\tKLD: 23.985050\nTrain Epoch: 2 [46080/50000 (92%)]\tLoss: 115.548470\tBCE: 92.551880\tKLD: 22.996588\nTrain Epoch: 2 [47360/50000 (95%)]\tLoss: 115.406128\tBCE: 92.219978\tKLD: 23.186152\nTrain Epoch: 2 [48640/50000 (97%)]\tLoss: 114.259041\tBCE: 91.009659\tKLD: 23.249382\nTrain Epoch: 2 [31200/50000 (100%)]\tLoss: 117.822375\tBCE: 94.324023\tKLD: 23.498349\n====&gt; Epoch: 2 Average loss: 15948.1738 (BCE: 13124.1253, KLD: 2824.0485)\n====&gt; Validation set loss: 14966.7641 (BCE: 11944.4480, KLD: 3022.3162)\nTrain Epoch: 3 [0/50000 (0%)]\tLoss: 117.851181\tBCE: 94.401070\tKLD: 23.450111\nTrain Epoch: 3 [1280/50000 (3%)]\tLoss: 116.619331\tBCE: 92.858040\tKLD: 23.761290\nTrain Epoch: 3 [2560/50000 (5%)]\tLoss: 116.980743\tBCE: 93.428024\tKLD: 23.552719\nTrain Epoch: 3 [3840/50000 (8%)]\tLoss: 115.116913\tBCE: 91.595123\tKLD: 23.521791\nTrain Epoch: 3 [5120/50000 (10%)]\tLoss: 115.039482\tBCE: 91.871437\tKLD: 23.168045\nTrain Epoch: 3 [6400/50000 (13%)]\tLoss: 115.562408\tBCE: 91.475357\tKLD: 24.087048\nTrain Epoch: 3 [7680/50000 (15%)]\tLoss: 117.247452\tBCE: 93.406281\tKLD: 23.841167\nTrain Epoch: 3 [8960/50000 (18%)]\tLoss: 115.806297\tBCE: 92.053680\tKLD: 23.752617\nTrain Epoch: 3 [10240/50000 (20%)]\tLoss: 116.407211\tBCE: 92.377083\tKLD: 24.030132\nTrain Epoch: 3 [11520/50000 (23%)]\tLoss: 114.626488\tBCE: 90.305649\tKLD: 24.320839\nTrain Epoch: 3 [12800/50000 (26%)]\tLoss: 114.861214\tBCE: 90.958389\tKLD: 23.902824\nTrain Epoch: 3 [14080/50000 (28%)]\tLoss: 119.493423\tBCE: 95.024414\tKLD: 24.469009\nTrain Epoch: 3 [15360/50000 (31%)]\tLoss: 111.953186\tBCE: 88.304520\tKLD: 23.648664\nTrain Epoch: 3 [16640/50000 (33%)]\tLoss: 113.776810\tBCE: 89.393517\tKLD: 24.383297\nTrain Epoch: 3 [17920/50000 (36%)]\tLoss: 113.764000\tBCE: 89.610176\tKLD: 24.153826\nTrain Epoch: 3 [19200/50000 (38%)]\tLoss: 114.782730\tBCE: 90.727837\tKLD: 24.054897\nTrain Epoch: 3 [20480/50000 (41%)]\tLoss: 112.671021\tBCE: 88.126877\tKLD: 24.544144\nTrain Epoch: 3 [21760/50000 (43%)]\tLoss: 113.693909\tBCE: 89.929314\tKLD: 23.764591\nTrain Epoch: 3 [23040/50000 (46%)]\tLoss: 112.220459\tBCE: 88.640259\tKLD: 23.580196\nTrain Epoch: 3 [24320/50000 (49%)]\tLoss: 113.971085\tBCE: 89.257690\tKLD: 24.713392\nTrain Epoch: 3 [25600/50000 (51%)]\tLoss: 113.255440\tBCE: 89.687134\tKLD: 23.568306\nTrain Epoch: 3 [26880/50000 (54%)]\tLoss: 112.584953\tBCE: 88.377289\tKLD: 24.207666\nTrain Epoch: 3 [28160/50000 (56%)]\tLoss: 115.867340\tBCE: 91.049988\tKLD: 24.817348\nTrain Epoch: 3 [29440/50000 (59%)]\tLoss: 117.237762\tBCE: 92.482803\tKLD: 24.754959\nTrain Epoch: 3 [30720/50000 (61%)]\tLoss: 110.883293\tBCE: 86.801987\tKLD: 24.081305\nTrain Epoch: 3 [32000/50000 (64%)]\tLoss: 112.595596\tBCE: 88.009293\tKLD: 24.586302\nTrain Epoch: 3 [33280/50000 (66%)]\tLoss: 108.802567\tBCE: 84.388626\tKLD: 24.413942\nTrain Epoch: 3 [34560/50000 (69%)]\tLoss: 114.886215\tBCE: 90.078949\tKLD: 24.807268\nTrain Epoch: 3 [35840/50000 (72%)]\tLoss: 115.446487\tBCE: 89.999924\tKLD: 25.446566\nTrain Epoch: 3 [37120/50000 (74%)]\tLoss: 109.739182\tBCE: 85.817749\tKLD: 23.921432\nTrain Epoch: 3 [38400/50000 (77%)]\tLoss: 111.595169\tBCE: 86.905838\tKLD: 24.689333\nTrain Epoch: 3 [39680/50000 (79%)]\tLoss: 112.320847\tBCE: 87.750084\tKLD: 24.570765\nTrain Epoch: 3 [40960/50000 (82%)]\tLoss: 115.009262\tBCE: 89.999870\tKLD: 25.009388\nTrain Epoch: 3 [42240/50000 (84%)]\tLoss: 108.179680\tBCE: 84.021439\tKLD: 24.158239\nTrain Epoch: 3 [43520/50000 (87%)]\tLoss: 113.052208\tBCE: 88.569702\tKLD: 24.482506\nTrain Epoch: 3 [44800/50000 (90%)]\tLoss: 113.439423\tBCE: 88.383621\tKLD: 25.055805\nTrain Epoch: 3 [46080/50000 (92%)]\tLoss: 111.424362\tBCE: 87.320808\tKLD: 24.103552\nTrain Epoch: 3 [47360/50000 (95%)]\tLoss: 108.859016\tBCE: 84.210915\tKLD: 24.648104\nTrain Epoch: 3 [48640/50000 (97%)]\tLoss: 110.253731\tBCE: 85.986183\tKLD: 24.267548\nTrain Epoch: 3 [31200/50000 (100%)]\tLoss: 110.157983\tBCE: 85.157617\tKLD: 25.000363\n====&gt; Epoch: 3 Average loss: 14585.5828 (BCE: 11478.7609, KLD: 3106.8219)\n====&gt; Validation set loss: 14313.2785 (BCE: 11147.1976, KLD: 3166.0809)\nTrain Epoch: 4 [0/50000 (0%)]\tLoss: 112.558655\tBCE: 87.551033\tKLD: 25.007618\nTrain Epoch: 4 [1280/50000 (3%)]\tLoss: 107.140198\tBCE: 82.588455\tKLD: 24.551739\nTrain Epoch: 4 [2560/50000 (5%)]\tLoss: 110.670654\tBCE: 86.058868\tKLD: 24.611786\nTrain Epoch: 4 [3840/50000 (8%)]\tLoss: 111.921524\tBCE: 86.868950\tKLD: 25.052570\nTrain Epoch: 4 [5120/50000 (10%)]\tLoss: 113.355377\tBCE: 88.205002\tKLD: 25.150375\nTrain Epoch: 4 [6400/50000 (13%)]\tLoss: 108.158150\tBCE: 83.477203\tKLD: 24.680944\nTrain Epoch: 4 [7680/50000 (15%)]\tLoss: 108.748680\tBCE: 83.846764\tKLD: 24.901918\nTrain Epoch: 4 [8960/50000 (18%)]\tLoss: 107.865891\tBCE: 83.346031\tKLD: 24.519859\nTrain Epoch: 4 [10240/50000 (20%)]\tLoss: 115.938431\tBCE: 90.330002\tKLD: 25.608425\nTrain Epoch: 4 [11520/50000 (23%)]\tLoss: 109.813797\tBCE: 85.187035\tKLD: 24.626762\nTrain Epoch: 4 [12800/50000 (26%)]\tLoss: 110.912148\tBCE: 85.968788\tKLD: 24.943359\nTrain Epoch: 4 [14080/50000 (28%)]\tLoss: 109.984543\tBCE: 85.101357\tKLD: 24.883183\nTrain Epoch: 4 [15360/50000 (31%)]\tLoss: 109.820770\tBCE: 84.886253\tKLD: 24.934517\nTrain Epoch: 4 [16640/50000 (33%)]\tLoss: 111.428116\tBCE: 86.498657\tKLD: 24.929461\nTrain Epoch: 4 [17920/50000 (36%)]\tLoss: 109.544876\tBCE: 84.482346\tKLD: 25.062532\nTrain Epoch: 4 [19200/50000 (38%)]\tLoss: 107.986893\tBCE: 83.276123\tKLD: 24.710773\nTrain Epoch: 4 [20480/50000 (41%)]\tLoss: 110.444443\tBCE: 84.961533\tKLD: 25.482912\nTrain Epoch: 4 [21760/50000 (43%)]\tLoss: 111.874100\tBCE: 87.180946\tKLD: 24.693150\nTrain Epoch: 4 [23040/50000 (46%)]\tLoss: 111.342506\tBCE: 86.106033\tKLD: 25.236473\nTrain Epoch: 4 [24320/50000 (49%)]\tLoss: 107.899704\tBCE: 82.993668\tKLD: 24.906038\nTrain Epoch: 4 [25600/50000 (51%)]\tLoss: 111.205215\tBCE: 86.010483\tKLD: 25.194731\nTrain Epoch: 4 [26880/50000 (54%)]\tLoss: 108.901169\tBCE: 83.949585\tKLD: 24.951584\nTrain Epoch: 4 [28160/50000 (56%)]\tLoss: 110.436012\tBCE: 85.148132\tKLD: 25.287880\nTrain Epoch: 4 [29440/50000 (59%)]\tLoss: 109.838806\tBCE: 84.977463\tKLD: 24.861341\nTrain Epoch: 4 [30720/50000 (61%)]\tLoss: 108.528549\tBCE: 83.538109\tKLD: 24.990437\nTrain Epoch: 4 [32000/50000 (64%)]\tLoss: 111.270897\tBCE: 85.971169\tKLD: 25.299728\nTrain Epoch: 4 [33280/50000 (66%)]\tLoss: 111.422928\tBCE: 86.067314\tKLD: 25.355614\nTrain Epoch: 4 [34560/50000 (69%)]\tLoss: 112.676872\tBCE: 86.906059\tKLD: 25.770813\nTrain Epoch: 4 [35840/50000 (72%)]\tLoss: 110.270493\tBCE: 85.070328\tKLD: 25.200167\nTrain Epoch: 4 [37120/50000 (74%)]\tLoss: 112.712334\tBCE: 87.069786\tKLD: 25.642550\nTrain Epoch: 4 [38400/50000 (77%)]\tLoss: 106.066864\tBCE: 81.848068\tKLD: 24.218796\nTrain Epoch: 4 [39680/50000 (79%)]\tLoss: 109.770447\tBCE: 84.199799\tKLD: 25.570644\nTrain Epoch: 4 [40960/50000 (82%)]\tLoss: 108.787216\tBCE: 83.848877\tKLD: 24.938339\nTrain Epoch: 4 [42240/50000 (84%)]\tLoss: 112.303329\tBCE: 86.513313\tKLD: 25.790018\nTrain Epoch: 4 [43520/50000 (87%)]\tLoss: 107.432259\tBCE: 82.884323\tKLD: 24.547935\nTrain Epoch: 4 [44800/50000 (90%)]\tLoss: 107.935112\tBCE: 82.882782\tKLD: 25.052330\nTrain Epoch: 4 [46080/50000 (92%)]\tLoss: 105.743622\tBCE: 80.701042\tKLD: 25.042582\nTrain Epoch: 4 [47360/50000 (95%)]\tLoss: 107.287613\tBCE: 82.335052\tKLD: 24.952562\nTrain Epoch: 4 [48640/50000 (97%)]\tLoss: 108.027641\tBCE: 82.774155\tKLD: 25.253489\nTrain Epoch: 4 [31200/50000 (100%)]\tLoss: 116.876355\tBCE: 91.488129\tKLD: 25.388225\n====&gt; Epoch: 4 Average loss: 14083.0687 (BCE: 10875.7332, KLD: 3207.3355)\n====&gt; Validation set loss: 13963.7640 (BCE: 10789.3793, KLD: 3174.3846)\nTrain Epoch: 5 [0/50000 (0%)]\tLoss: 108.690582\tBCE: 84.163162\tKLD: 24.527424\nTrain Epoch: 5 [1280/50000 (3%)]\tLoss: 109.370819\tBCE: 83.825142\tKLD: 25.545673\nTrain Epoch: 5 [2560/50000 (5%)]\tLoss: 112.815025\tBCE: 86.927200\tKLD: 25.887825\nTrain Epoch: 5 [3840/50000 (8%)]\tLoss: 108.995621\tBCE: 83.728226\tKLD: 25.267399\nTrain Epoch: 5 [5120/50000 (10%)]\tLoss: 108.473282\tBCE: 83.230118\tKLD: 25.243162\nTrain Epoch: 5 [6400/50000 (13%)]\tLoss: 112.472633\tBCE: 86.946487\tKLD: 25.526146\nTrain Epoch: 5 [7680/50000 (15%)]\tLoss: 106.100510\tBCE: 81.308800\tKLD: 24.791710\nTrain Epoch: 5 [8960/50000 (18%)]\tLoss: 111.412445\tBCE: 85.018707\tKLD: 26.393740\nTrain Epoch: 5 [10240/50000 (20%)]\tLoss: 110.067993\tBCE: 84.543365\tKLD: 25.524626\nTrain Epoch: 5 [11520/50000 (23%)]\tLoss: 105.996185\tBCE: 80.830986\tKLD: 25.165199\nTrain Epoch: 5 [12800/50000 (26%)]\tLoss: 108.974548\tBCE: 83.045486\tKLD: 25.929062\nTrain Epoch: 5 [14080/50000 (28%)]\tLoss: 108.916435\tBCE: 83.389481\tKLD: 25.526953\nTrain Epoch: 5 [15360/50000 (31%)]\tLoss: 107.502159\tBCE: 81.583191\tKLD: 25.918966\nTrain Epoch: 5 [16640/50000 (33%)]\tLoss: 111.258820\tBCE: 85.364601\tKLD: 25.894215\nTrain Epoch: 5 [17920/50000 (36%)]\tLoss: 109.166397\tBCE: 83.853516\tKLD: 25.312881\nTrain Epoch: 5 [19200/50000 (38%)]\tLoss: 110.891434\tBCE: 85.215401\tKLD: 25.676035\nTrain Epoch: 5 [20480/50000 (41%)]\tLoss: 107.597397\tBCE: 82.209572\tKLD: 25.387823\nTrain Epoch: 5 [21760/50000 (43%)]\tLoss: 108.772202\tBCE: 82.937775\tKLD: 25.834431\nTrain Epoch: 5 [23040/50000 (46%)]\tLoss: 102.345886\tBCE: 77.865318\tKLD: 24.480568\nTrain Epoch: 5 [24320/50000 (49%)]\tLoss: 106.107452\tBCE: 80.891937\tKLD: 25.215515\nTrain Epoch: 5 [25600/50000 (51%)]\tLoss: 110.313858\tBCE: 84.427574\tKLD: 25.886288\nTrain Epoch: 5 [26880/50000 (54%)]\tLoss: 106.998528\tBCE: 81.466522\tKLD: 25.532003\nTrain Epoch: 5 [28160/50000 (56%)]\tLoss: 108.263763\tBCE: 83.156807\tKLD: 25.106955\nTrain Epoch: 5 [29440/50000 (59%)]\tLoss: 107.936386\tBCE: 82.424698\tKLD: 25.511686\nTrain Epoch: 5 [30720/50000 (61%)]\tLoss: 109.706581\tBCE: 84.062057\tKLD: 25.644522\nTrain Epoch: 5 [32000/50000 (64%)]\tLoss: 103.452103\tBCE: 78.351089\tKLD: 25.101013\nTrain Epoch: 5 [33280/50000 (66%)]\tLoss: 112.259964\tBCE: 85.943207\tKLD: 26.316755\nTrain Epoch: 5 [34560/50000 (69%)]\tLoss: 111.747604\tBCE: 86.038490\tKLD: 25.709114\nTrain Epoch: 5 [35840/50000 (72%)]\tLoss: 110.681610\tBCE: 84.956955\tKLD: 25.724657\nTrain Epoch: 5 [37120/50000 (74%)]\tLoss: 105.216766\tBCE: 79.521118\tKLD: 25.695650\nTrain Epoch: 5 [38400/50000 (77%)]\tLoss: 105.600388\tBCE: 80.045349\tKLD: 25.555037\nTrain Epoch: 5 [39680/50000 (79%)]\tLoss: 105.978622\tBCE: 80.680138\tKLD: 25.298489\nTrain Epoch: 5 [40960/50000 (82%)]\tLoss: 110.263718\tBCE: 83.929459\tKLD: 26.334259\nTrain Epoch: 5 [42240/50000 (84%)]\tLoss: 106.980415\tBCE: 81.810280\tKLD: 25.170135\nTrain Epoch: 5 [43520/50000 (87%)]\tLoss: 108.812927\tBCE: 82.473885\tKLD: 26.339046\nTrain Epoch: 5 [44800/50000 (90%)]\tLoss: 106.394356\tBCE: 81.531998\tKLD: 24.862358\nTrain Epoch: 5 [46080/50000 (92%)]\tLoss: 109.214302\tBCE: 83.618462\tKLD: 25.595840\nTrain Epoch: 5 [47360/50000 (95%)]\tLoss: 107.946533\tBCE: 82.309738\tKLD: 25.636791\nTrain Epoch: 5 [48640/50000 (97%)]\tLoss: 105.685463\tBCE: 80.178207\tKLD: 25.507257\nTrain Epoch: 5 [31200/50000 (100%)]\tLoss: 108.738721\tBCE: 82.993188\tKLD: 25.745532\n====&gt; Epoch: 5 Average loss: 13825.9673 (BCE: 10566.8101, KLD: 3259.1572)\n====&gt; Validation set loss: 13727.5662 (BCE: 10418.3609, KLD: 3309.2053)\nTrain Epoch: 6 [0/50000 (0%)]\tLoss: 105.753441\tBCE: 79.922577\tKLD: 25.830866\nTrain Epoch: 6 [1280/50000 (3%)]\tLoss: 106.626602\tBCE: 81.175735\tKLD: 25.450865\nTrain Epoch: 6 [2560/50000 (5%)]\tLoss: 103.824043\tBCE: 78.712303\tKLD: 25.111742\nTrain Epoch: 6 [3840/50000 (8%)]\tLoss: 107.139198\tBCE: 81.606461\tKLD: 25.532740\nTrain Epoch: 6 [5120/50000 (10%)]\tLoss: 110.436478\tBCE: 84.429092\tKLD: 26.007383\nTrain Epoch: 6 [6400/50000 (13%)]\tLoss: 107.947098\tBCE: 82.452065\tKLD: 25.495031\nTrain Epoch: 6 [7680/50000 (15%)]\tLoss: 108.476028\tBCE: 83.088257\tKLD: 25.387772\nTrain Epoch: 6 [8960/50000 (18%)]\tLoss: 106.382904\tBCE: 80.419693\tKLD: 25.963213\nTrain Epoch: 6 [10240/50000 (20%)]\tLoss: 107.169556\tBCE: 81.863014\tKLD: 25.306540\nTrain Epoch: 6 [11520/50000 (23%)]\tLoss: 107.181030\tBCE: 81.607437\tKLD: 25.573589\nTrain Epoch: 6 [12800/50000 (26%)]\tLoss: 109.771713\tBCE: 84.104095\tKLD: 25.667614\nTrain Epoch: 6 [14080/50000 (28%)]\tLoss: 106.569794\tBCE: 80.505333\tKLD: 26.064457\nTrain Epoch: 6 [15360/50000 (31%)]\tLoss: 105.370850\tBCE: 80.326164\tKLD: 25.044682\nTrain Epoch: 6 [16640/50000 (33%)]\tLoss: 103.611633\tBCE: 78.313057\tKLD: 25.298580\nTrain Epoch: 6 [17920/50000 (36%)]\tLoss: 112.200142\tBCE: 85.687477\tKLD: 26.512663\nTrain Epoch: 6 [19200/50000 (38%)]\tLoss: 107.228073\tBCE: 82.008919\tKLD: 25.219152\nTrain Epoch: 6 [20480/50000 (41%)]\tLoss: 107.153496\tBCE: 81.328094\tKLD: 25.825399\nTrain Epoch: 6 [21760/50000 (43%)]\tLoss: 106.045502\tBCE: 80.098457\tKLD: 25.947046\nTrain Epoch: 6 [23040/50000 (46%)]\tLoss: 103.549591\tBCE: 78.138184\tKLD: 25.411409\nTrain Epoch: 6 [24320/50000 (49%)]\tLoss: 113.635643\tBCE: 86.928543\tKLD: 26.707100\nTrain Epoch: 6 [25600/50000 (51%)]\tLoss: 104.954918\tBCE: 79.296104\tKLD: 25.658812\nTrain Epoch: 6 [26880/50000 (54%)]\tLoss: 108.944122\tBCE: 83.052681\tKLD: 25.891443\nTrain Epoch: 6 [28160/50000 (56%)]\tLoss: 108.507378\tBCE: 82.574570\tKLD: 25.932808\nTrain Epoch: 6 [29440/50000 (59%)]\tLoss: 106.467262\tBCE: 80.057213\tKLD: 26.410049\nTrain Epoch: 6 [30720/50000 (61%)]\tLoss: 106.315231\tBCE: 80.589813\tKLD: 25.725420\nTrain Epoch: 6 [32000/50000 (64%)]\tLoss: 103.130264\tBCE: 77.869240\tKLD: 25.261028\nTrain Epoch: 6 [33280/50000 (66%)]\tLoss: 108.032623\tBCE: 82.004494\tKLD: 26.028130\nTrain Epoch: 6 [34560/50000 (69%)]\tLoss: 108.002625\tBCE: 82.201546\tKLD: 25.801081\nTrain Epoch: 6 [35840/50000 (72%)]\tLoss: 106.306877\tBCE: 81.037598\tKLD: 25.269278\nTrain Epoch: 6 [37120/50000 (74%)]\tLoss: 106.975189\tBCE: 80.797562\tKLD: 26.177624\nTrain Epoch: 6 [38400/50000 (77%)]\tLoss: 111.391930\tBCE: 85.464233\tKLD: 25.927696\nTrain Epoch: 6 [39680/50000 (79%)]\tLoss: 113.087456\tBCE: 86.208878\tKLD: 26.878578\nTrain Epoch: 6 [40960/50000 (82%)]\tLoss: 103.654999\tBCE: 78.233780\tKLD: 25.421223\nTrain Epoch: 6 [42240/50000 (84%)]\tLoss: 103.734711\tBCE: 78.572121\tKLD: 25.162588\nTrain Epoch: 6 [43520/50000 (87%)]\tLoss: 107.043564\tBCE: 80.883247\tKLD: 26.160313\nTrain Epoch: 6 [44800/50000 (90%)]\tLoss: 106.071953\tBCE: 80.755402\tKLD: 25.316551\nTrain Epoch: 6 [46080/50000 (92%)]\tLoss: 107.347595\tBCE: 81.332863\tKLD: 26.014730\nTrain Epoch: 6 [47360/50000 (95%)]\tLoss: 103.462784\tBCE: 78.619835\tKLD: 24.842949\nTrain Epoch: 6 [48640/50000 (97%)]\tLoss: 104.607132\tBCE: 78.687057\tKLD: 25.920073\nTrain Epoch: 6 [31200/50000 (100%)]\tLoss: 105.180603\tBCE: 79.489282\tKLD: 25.691318\n====&gt; Epoch: 6 Average loss: 13662.2923 (BCE: 10375.5857, KLD: 3286.7066)\n====&gt; Validation set loss: 13605.9106 (BCE: 10359.6622, KLD: 3246.2483)\nTrain Epoch: 7 [0/50000 (0%)]\tLoss: 104.228577\tBCE: 78.798538\tKLD: 25.430035\nTrain Epoch: 7 [1280/50000 (3%)]\tLoss: 102.847527\tBCE: 77.922165\tKLD: 24.925364\nTrain Epoch: 7 [2560/50000 (5%)]\tLoss: 104.375267\tBCE: 78.677246\tKLD: 25.698021\nTrain Epoch: 7 [3840/50000 (8%)]\tLoss: 106.200150\tBCE: 80.615967\tKLD: 25.584179\nTrain Epoch: 7 [5120/50000 (10%)]\tLoss: 105.193665\tBCE: 79.468788\tKLD: 25.724876\nTrain Epoch: 7 [6400/50000 (13%)]\tLoss: 105.982170\tBCE: 80.036346\tKLD: 25.945824\nTrain Epoch: 7 [7680/50000 (15%)]\tLoss: 106.502914\tBCE: 80.312515\tKLD: 26.190397\nTrain Epoch: 7 [8960/50000 (18%)]\tLoss: 105.235504\tBCE: 79.733406\tKLD: 25.502102\nTrain Epoch: 7 [10240/50000 (20%)]\tLoss: 108.076485\tBCE: 82.188431\tKLD: 25.888052\nTrain Epoch: 7 [11520/50000 (23%)]\tLoss: 101.977905\tBCE: 76.505676\tKLD: 25.472225\nTrain Epoch: 7 [12800/50000 (26%)]\tLoss: 106.232376\tBCE: 80.908524\tKLD: 25.323856\nTrain Epoch: 7 [14080/50000 (28%)]\tLoss: 108.352852\tBCE: 81.799316\tKLD: 26.553534\nTrain Epoch: 7 [15360/50000 (31%)]\tLoss: 106.758850\tBCE: 81.171204\tKLD: 25.587650\nTrain Epoch: 7 [16640/50000 (33%)]\tLoss: 107.851547\tBCE: 81.910904\tKLD: 25.940643\nTrain Epoch: 7 [17920/50000 (36%)]\tLoss: 104.538101\tBCE: 79.424789\tKLD: 25.113308\nTrain Epoch: 7 [19200/50000 (38%)]\tLoss: 105.209854\tBCE: 78.906319\tKLD: 26.303532\nTrain Epoch: 7 [20480/50000 (41%)]\tLoss: 106.920242\tBCE: 81.166351\tKLD: 25.753887\nTrain Epoch: 7 [21760/50000 (43%)]\tLoss: 107.504822\tBCE: 81.227844\tKLD: 26.276978\nTrain Epoch: 7 [23040/50000 (46%)]\tLoss: 110.382889\tBCE: 83.588776\tKLD: 26.794109\nTrain Epoch: 7 [24320/50000 (49%)]\tLoss: 107.166870\tBCE: 81.723007\tKLD: 25.443859\nTrain Epoch: 7 [25600/50000 (51%)]\tLoss: 109.997635\tBCE: 83.523727\tKLD: 26.473904\nTrain Epoch: 7 [26880/50000 (54%)]\tLoss: 104.518410\tBCE: 79.026459\tKLD: 25.491951\nTrain Epoch: 7 [28160/50000 (56%)]\tLoss: 109.873878\tBCE: 83.414894\tKLD: 26.458982\nTrain Epoch: 7 [29440/50000 (59%)]\tLoss: 109.991898\tBCE: 83.592758\tKLD: 26.399141\nTrain Epoch: 7 [30720/50000 (61%)]\tLoss: 108.252037\tBCE: 81.691780\tKLD: 26.560257\nTrain Epoch: 7 [32000/50000 (64%)]\tLoss: 106.411865\tBCE: 80.248756\tKLD: 26.163111\nTrain Epoch: 7 [33280/50000 (66%)]\tLoss: 106.571960\tBCE: 81.109238\tKLD: 25.462723\nTrain Epoch: 7 [34560/50000 (69%)]\tLoss: 106.500679\tBCE: 80.825424\tKLD: 25.675255\nTrain Epoch: 7 [35840/50000 (72%)]\tLoss: 104.530838\tBCE: 78.825119\tKLD: 25.705721\nTrain Epoch: 7 [37120/50000 (74%)]\tLoss: 110.427689\tBCE: 83.273285\tKLD: 27.154408\nTrain Epoch: 7 [38400/50000 (77%)]\tLoss: 106.783127\tBCE: 81.152695\tKLD: 25.630432\nTrain Epoch: 7 [39680/50000 (79%)]\tLoss: 108.290314\tBCE: 81.991333\tKLD: 26.298985\nTrain Epoch: 7 [40960/50000 (82%)]\tLoss: 106.749542\tBCE: 80.988678\tKLD: 25.760868\nTrain Epoch: 7 [42240/50000 (84%)]\tLoss: 106.134079\tBCE: 79.875366\tKLD: 26.258709\nTrain Epoch: 7 [43520/50000 (87%)]\tLoss: 105.559341\tBCE: 79.919144\tKLD: 25.640194\nTrain Epoch: 7 [44800/50000 (90%)]\tLoss: 107.829590\tBCE: 81.330147\tKLD: 26.499441\nTrain Epoch: 7 [46080/50000 (92%)]\tLoss: 105.984146\tBCE: 80.520226\tKLD: 25.463921\nTrain Epoch: 7 [47360/50000 (95%)]\tLoss: 104.692940\tBCE: 79.389603\tKLD: 25.303339\nTrain Epoch: 7 [48640/50000 (97%)]\tLoss: 101.557655\tBCE: 76.330994\tKLD: 25.226664\nTrain Epoch: 7 [31200/50000 (100%)]\tLoss: 105.171704\tBCE: 79.871838\tKLD: 25.299864\n====&gt; Epoch: 7 Average loss: 13530.1305 (BCE: 10228.6802, KLD: 3301.4503)\n====&gt; Validation set loss: 13579.0885 (BCE: 10271.9327, KLD: 3307.1559)\nTrain Epoch: 8 [0/50000 (0%)]\tLoss: 104.247589\tBCE: 78.625237\tKLD: 25.622353\nTrain Epoch: 8 [1280/50000 (3%)]\tLoss: 107.984161\tBCE: 81.607872\tKLD: 26.376291\nTrain Epoch: 8 [2560/50000 (5%)]\tLoss: 103.290276\tBCE: 77.643303\tKLD: 25.646975\nTrain Epoch: 8 [3840/50000 (8%)]\tLoss: 102.927444\tBCE: 77.413452\tKLD: 25.513988\nTrain Epoch: 8 [5120/50000 (10%)]\tLoss: 107.940018\tBCE: 82.114052\tKLD: 25.825970\nTrain Epoch: 8 [6400/50000 (13%)]\tLoss: 108.395874\tBCE: 81.593758\tKLD: 26.802114\nTrain Epoch: 8 [7680/50000 (15%)]\tLoss: 104.421677\tBCE: 78.627075\tKLD: 25.794603\nTrain Epoch: 8 [8960/50000 (18%)]\tLoss: 109.222771\tBCE: 82.790039\tKLD: 26.432732\nTrain Epoch: 8 [10240/50000 (20%)]\tLoss: 100.479202\tBCE: 74.951477\tKLD: 25.527723\nTrain Epoch: 8 [11520/50000 (23%)]\tLoss: 103.099800\tBCE: 77.190155\tKLD: 25.909645\nTrain Epoch: 8 [12800/50000 (26%)]\tLoss: 104.725250\tBCE: 79.211899\tKLD: 25.513350\nTrain Epoch: 8 [14080/50000 (28%)]\tLoss: 104.223717\tBCE: 78.405487\tKLD: 25.818230\nTrain Epoch: 8 [15360/50000 (31%)]\tLoss: 102.022583\tBCE: 76.642319\tKLD: 25.380260\nTrain Epoch: 8 [16640/50000 (33%)]\tLoss: 104.869278\tBCE: 78.853226\tKLD: 26.016048\nTrain Epoch: 8 [17920/50000 (36%)]\tLoss: 107.364120\tBCE: 81.845146\tKLD: 25.518970\nTrain Epoch: 8 [19200/50000 (38%)]\tLoss: 102.105545\tBCE: 76.509766\tKLD: 25.595781\nTrain Epoch: 8 [20480/50000 (41%)]\tLoss: 103.746017\tBCE: 77.862076\tKLD: 25.883938\nTrain Epoch: 8 [21760/50000 (43%)]\tLoss: 107.998512\tBCE: 81.672318\tKLD: 26.326195\nTrain Epoch: 8 [23040/50000 (46%)]\tLoss: 106.386726\tBCE: 80.250771\tKLD: 26.135958\nTrain Epoch: 8 [24320/50000 (49%)]\tLoss: 108.196632\tBCE: 81.691727\tKLD: 26.504906\nTrain Epoch: 8 [25600/50000 (51%)]\tLoss: 106.110146\tBCE: 80.357063\tKLD: 25.753082\nTrain Epoch: 8 [26880/50000 (54%)]\tLoss: 106.408157\tBCE: 79.955521\tKLD: 26.452635\nTrain Epoch: 8 [28160/50000 (56%)]\tLoss: 104.538239\tBCE: 78.972961\tKLD: 25.565273\nTrain Epoch: 8 [29440/50000 (59%)]\tLoss: 105.860512\tBCE: 79.896347\tKLD: 25.964165\nTrain Epoch: 8 [30720/50000 (61%)]\tLoss: 105.405655\tBCE: 79.220428\tKLD: 26.185230\nTrain Epoch: 8 [32000/50000 (64%)]\tLoss: 104.487335\tBCE: 78.818542\tKLD: 25.668789\nTrain Epoch: 8 [33280/50000 (66%)]\tLoss: 107.538605\tBCE: 81.134026\tKLD: 26.404579\nTrain Epoch: 8 [34560/50000 (69%)]\tLoss: 103.232368\tBCE: 77.252686\tKLD: 25.979683\nTrain Epoch: 8 [35840/50000 (72%)]\tLoss: 107.065254\tBCE: 80.702286\tKLD: 26.362967\nTrain Epoch: 8 [37120/50000 (74%)]\tLoss: 105.058701\tBCE: 79.044350\tKLD: 26.014353\nTrain Epoch: 8 [38400/50000 (77%)]\tLoss: 102.375381\tBCE: 77.240829\tKLD: 25.134550\nTrain Epoch: 8 [39680/50000 (79%)]\tLoss: 105.630203\tBCE: 79.448090\tKLD: 26.182110\nTrain Epoch: 8 [40960/50000 (82%)]\tLoss: 103.736412\tBCE: 77.935966\tKLD: 25.800446\nTrain Epoch: 8 [42240/50000 (84%)]\tLoss: 106.556557\tBCE: 79.860611\tKLD: 26.695946\nTrain Epoch: 8 [43520/50000 (87%)]\tLoss: 104.377792\tBCE: 78.515221\tKLD: 25.862572\nTrain Epoch: 8 [44800/50000 (90%)]\tLoss: 104.193245\tBCE: 77.757698\tKLD: 26.435547\nTrain Epoch: 8 [46080/50000 (92%)]\tLoss: 102.014748\tBCE: 76.932953\tKLD: 25.081797\nTrain Epoch: 8 [47360/50000 (95%)]\tLoss: 102.848038\tBCE: 77.225571\tKLD: 25.622469\nTrain Epoch: 8 [48640/50000 (97%)]\tLoss: 103.044182\tBCE: 76.790146\tKLD: 26.254038\nTrain Epoch: 8 [31200/50000 (100%)]\tLoss: 110.356165\tBCE: 83.863086\tKLD: 26.493082\n====&gt; Epoch: 8 Average loss: 13430.3738 (BCE: 10116.1651, KLD: 3314.2087)\n====&gt; Validation set loss: 13421.3969 (BCE: 10103.2530, KLD: 3318.1439)\nTrain Epoch: 9 [0/50000 (0%)]\tLoss: 100.603546\tBCE: 75.406998\tKLD: 25.196548\nTrain Epoch: 9 [1280/50000 (3%)]\tLoss: 106.157028\tBCE: 79.585548\tKLD: 26.571480\nTrain Epoch: 9 [2560/50000 (5%)]\tLoss: 107.275864\tBCE: 80.704300\tKLD: 26.571564\nTrain Epoch: 9 [3840/50000 (8%)]\tLoss: 106.204758\tBCE: 80.329056\tKLD: 25.875700\nTrain Epoch: 9 [5120/50000 (10%)]\tLoss: 107.226227\tBCE: 81.211349\tKLD: 26.014874\nTrain Epoch: 9 [6400/50000 (13%)]\tLoss: 103.174561\tBCE: 76.825302\tKLD: 26.349255\nTrain Epoch: 9 [7680/50000 (15%)]\tLoss: 104.183990\tBCE: 78.439873\tKLD: 25.744116\nTrain Epoch: 9 [8960/50000 (18%)]\tLoss: 102.585426\tBCE: 76.865486\tKLD: 25.719938\nTrain Epoch: 9 [10240/50000 (20%)]\tLoss: 104.054169\tBCE: 77.914078\tKLD: 26.140089\nTrain Epoch: 9 [11520/50000 (23%)]\tLoss: 102.195740\tBCE: 76.132523\tKLD: 26.063217\nTrain Epoch: 9 [12800/50000 (26%)]\tLoss: 104.480675\tBCE: 78.970428\tKLD: 25.510246\nTrain Epoch: 9 [14080/50000 (28%)]\tLoss: 103.904739\tBCE: 77.795189\tKLD: 26.109549\nTrain Epoch: 9 [15360/50000 (31%)]\tLoss: 109.399834\tBCE: 83.209267\tKLD: 26.190569\nTrain Epoch: 9 [16640/50000 (33%)]\tLoss: 101.752861\tBCE: 76.402390\tKLD: 25.350473\nTrain Epoch: 9 [17920/50000 (36%)]\tLoss: 103.360580\tBCE: 76.981728\tKLD: 26.378849\nTrain Epoch: 9 [19200/50000 (38%)]\tLoss: 105.564484\tBCE: 79.267059\tKLD: 26.297421\nTrain Epoch: 9 [20480/50000 (41%)]\tLoss: 102.008102\tBCE: 76.884964\tKLD: 25.123142\nTrain Epoch: 9 [21760/50000 (43%)]\tLoss: 104.330528\tBCE: 78.418846\tKLD: 25.911682\nTrain Epoch: 9 [23040/50000 (46%)]\tLoss: 101.796623\tBCE: 76.617798\tKLD: 25.178823\nTrain Epoch: 9 [24320/50000 (49%)]\tLoss: 105.107552\tBCE: 79.364586\tKLD: 25.742966\nTrain Epoch: 9 [25600/50000 (51%)]\tLoss: 107.107162\tBCE: 80.207108\tKLD: 26.900057\nTrain Epoch: 9 [26880/50000 (54%)]\tLoss: 105.589050\tBCE: 79.529488\tKLD: 26.059566\nTrain Epoch: 9 [28160/50000 (56%)]\tLoss: 107.353699\tBCE: 80.373978\tKLD: 26.979719\nTrain Epoch: 9 [29440/50000 (59%)]\tLoss: 102.594551\tBCE: 76.882034\tKLD: 25.712515\nTrain Epoch: 9 [30720/50000 (61%)]\tLoss: 102.738831\tBCE: 76.702423\tKLD: 26.036406\nTrain Epoch: 9 [32000/50000 (64%)]\tLoss: 101.926025\tBCE: 77.092590\tKLD: 24.833439\nTrain Epoch: 9 [33280/50000 (66%)]\tLoss: 107.273895\tBCE: 80.401474\tKLD: 26.872419\nTrain Epoch: 9 [34560/50000 (69%)]\tLoss: 104.228577\tBCE: 78.219978\tKLD: 26.008602\nTrain Epoch: 9 [35840/50000 (72%)]\tLoss: 105.988205\tBCE: 79.097031\tKLD: 26.891174\nTrain Epoch: 9 [37120/50000 (74%)]\tLoss: 105.751259\tBCE: 80.343910\tKLD: 25.407351\nTrain Epoch: 9 [38400/50000 (77%)]\tLoss: 103.361946\tBCE: 77.386116\tKLD: 25.975832\nTrain Epoch: 9 [39680/50000 (79%)]\tLoss: 105.839066\tBCE: 80.004715\tKLD: 25.834352\nTrain Epoch: 9 [40960/50000 (82%)]\tLoss: 104.523651\tBCE: 77.977768\tKLD: 26.545885\nTrain Epoch: 9 [42240/50000 (84%)]\tLoss: 103.833832\tBCE: 77.908539\tKLD: 25.925291\nTrain Epoch: 9 [43520/50000 (87%)]\tLoss: 105.248970\tBCE: 79.138138\tKLD: 26.110834\nTrain Epoch: 9 [44800/50000 (90%)]\tLoss: 107.482925\tBCE: 81.371307\tKLD: 26.111614\nTrain Epoch: 9 [46080/50000 (92%)]\tLoss: 106.264023\tBCE: 79.982079\tKLD: 26.281946\nTrain Epoch: 9 [47360/50000 (95%)]\tLoss: 105.464081\tBCE: 79.099983\tKLD: 26.364098\nTrain Epoch: 9 [48640/50000 (97%)]\tLoss: 102.948380\tBCE: 77.759552\tKLD: 25.188831\nTrain Epoch: 9 [31200/50000 (100%)]\tLoss: 102.863086\tBCE: 76.894763\tKLD: 25.968326\n====&gt; Epoch: 9 Average loss: 13370.5870 (BCE: 10043.2409, KLD: 3327.3461)\n====&gt; Validation set loss: 13350.3582 (BCE: 9940.5313, KLD: 3409.8269)\nTrain Epoch: 10 [0/50000 (0%)]\tLoss: 102.478226\tBCE: 76.147591\tKLD: 26.330639\nTrain Epoch: 10 [1280/50000 (3%)]\tLoss: 105.693405\tBCE: 80.488625\tKLD: 25.204779\nTrain Epoch: 10 [2560/50000 (5%)]\tLoss: 102.854301\tBCE: 76.748947\tKLD: 26.105356\nTrain Epoch: 10 [3840/50000 (8%)]\tLoss: 104.128418\tBCE: 78.404160\tKLD: 25.724258\nTrain Epoch: 10 [5120/50000 (10%)]\tLoss: 104.522430\tBCE: 78.122238\tKLD: 26.400194\nTrain Epoch: 10 [6400/50000 (13%)]\tLoss: 104.795975\tBCE: 78.451599\tKLD: 26.344376\nTrain Epoch: 10 [7680/50000 (15%)]\tLoss: 102.274704\tBCE: 76.800034\tKLD: 25.474672\nTrain Epoch: 10 [8960/50000 (18%)]\tLoss: 103.103424\tBCE: 76.808929\tKLD: 26.294497\nTrain Epoch: 10 [10240/50000 (20%)]\tLoss: 105.153412\tBCE: 78.979630\tKLD: 26.173784\nTrain Epoch: 10 [11520/50000 (23%)]\tLoss: 102.213486\tBCE: 76.342316\tKLD: 25.871166\nTrain Epoch: 10 [12800/50000 (26%)]\tLoss: 100.727829\tBCE: 75.159912\tKLD: 25.567915\nTrain Epoch: 10 [14080/50000 (28%)]\tLoss: 106.464775\tBCE: 80.461655\tKLD: 26.003122\nTrain Epoch: 10 [15360/50000 (31%)]\tLoss: 102.762810\tBCE: 76.800049\tKLD: 25.962759\nTrain Epoch: 10 [16640/50000 (33%)]\tLoss: 106.593925\tBCE: 80.006546\tKLD: 26.587378\nTrain Epoch: 10 [17920/50000 (36%)]\tLoss: 106.401131\tBCE: 80.004272\tKLD: 26.396858\nTrain Epoch: 10 [19200/50000 (38%)]\tLoss: 104.018982\tBCE: 77.910751\tKLD: 26.108234\nTrain Epoch: 10 [20480/50000 (41%)]\tLoss: 109.686859\tBCE: 82.642059\tKLD: 27.044802\nTrain Epoch: 10 [21760/50000 (43%)]\tLoss: 103.208740\tBCE: 77.549416\tKLD: 25.659321\nTrain Epoch: 10 [23040/50000 (46%)]\tLoss: 108.987305\tBCE: 81.652313\tKLD: 27.334993\nTrain Epoch: 10 [24320/50000 (49%)]\tLoss: 102.269707\tBCE: 76.575531\tKLD: 25.694178\nTrain Epoch: 10 [25600/50000 (51%)]\tLoss: 102.481735\tBCE: 76.874741\tKLD: 25.606995\nTrain Epoch: 10 [26880/50000 (54%)]\tLoss: 105.534302\tBCE: 79.297630\tKLD: 26.236675\nTrain Epoch: 10 [28160/50000 (56%)]\tLoss: 106.951286\tBCE: 80.902626\tKLD: 26.048662\nTrain Epoch: 10 [29440/50000 (59%)]\tLoss: 103.508820\tBCE: 77.483063\tKLD: 26.025755\nTrain Epoch: 10 [30720/50000 (61%)]\tLoss: 102.529846\tBCE: 77.140976\tKLD: 25.388866\nTrain Epoch: 10 [32000/50000 (64%)]\tLoss: 106.908279\tBCE: 79.889870\tKLD: 27.018406\nTrain Epoch: 10 [33280/50000 (66%)]\tLoss: 97.845772\tBCE: 73.092270\tKLD: 24.753502\nTrain Epoch: 10 [34560/50000 (69%)]\tLoss: 102.363541\tBCE: 76.567398\tKLD: 25.796143\nTrain Epoch: 10 [35840/50000 (72%)]\tLoss: 105.486740\tBCE: 79.265961\tKLD: 26.220776\nTrain Epoch: 10 [37120/50000 (74%)]\tLoss: 102.956612\tBCE: 76.803070\tKLD: 26.153542\nTrain Epoch: 10 [38400/50000 (77%)]\tLoss: 106.458130\tBCE: 79.664993\tKLD: 26.793137\nTrain Epoch: 10 [39680/50000 (79%)]\tLoss: 103.366493\tBCE: 77.299835\tKLD: 26.066658\nTrain Epoch: 10 [40960/50000 (82%)]\tLoss: 100.875229\tBCE: 75.112152\tKLD: 25.763079\nTrain Epoch: 10 [42240/50000 (84%)]\tLoss: 102.870872\tBCE: 77.323463\tKLD: 25.547407\nTrain Epoch: 10 [43520/50000 (87%)]\tLoss: 107.005348\tBCE: 80.338028\tKLD: 26.667318\nTrain Epoch: 10 [44800/50000 (90%)]\tLoss: 104.526489\tBCE: 78.485291\tKLD: 26.041197\nTrain Epoch: 10 [46080/50000 (92%)]\tLoss: 102.577271\tBCE: 76.235809\tKLD: 26.341461\nTrain Epoch: 10 [47360/50000 (95%)]\tLoss: 102.798264\tBCE: 76.897461\tKLD: 25.900806\nTrain Epoch: 10 [48640/50000 (97%)]\tLoss: 101.205498\tBCE: 75.442627\tKLD: 25.762869\nTrain Epoch: 10 [31200/50000 (100%)]\tLoss: 99.767627\tBCE: 74.514136\tKLD: 25.253491\n====&gt; Epoch: 10 Average loss: 13293.1929 (BCE: 9964.5811, KLD: 3328.6119)\n====&gt; Validation set loss: 13287.4655 (BCE: 9958.7066, KLD: 3328.7588)\n====&gt; Test set loss: 103.3470 (BCE: 77.3167, KLD: 26.0303)\nConvolutional VAE Test Metrics: {'Total Loss': 103.34704172363281, 'BCE': 77.3167458984375, 'KLD': 26.030295431518553}\n</pre> In\u00a0[62]: Copied! <pre>epochs_range = range(1, len(vae_train_losses_total) + 1)\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Total Loss\naxes[0, 0].plot(epochs_range, vae_train_losses_total, 'b-o', label='VAE Train')\naxes[0, 0].plot(epochs_range, vae_val_losses_total, 'b--s', label='VAE Val')\naxes[0, 0].plot(epochs_range, conv_train_losses_total, 'r-o', label='ConvVAE Train')\naxes[0, 0].plot(epochs_range, conv_val_losses_total, 'r--s', label='ConvVAE Val')\naxes[0, 0].set_title('Total Loss')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# BCE Loss\naxes[0, 1].plot(epochs_range, vae_train_losses_bce, 'b-o', label='VAE Train')\naxes[0, 1].plot(epochs_range, vae_val_losses_bce, 'b--s', label='VAE Val')\naxes[0, 1].plot(epochs_range, conv_train_losses_bce, 'r-o', label='ConvVAE Train')\naxes[0, 1].plot(epochs_range, conv_val_losses_bce, 'r--s', label='ConvVAE Val')\naxes[0, 1].set_title('Reconstruction Loss (BCE)')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# KLD Loss\naxes[1, 0].plot(epochs_range, vae_train_losses_kld, 'b-o', label='VAE Train')\naxes[1, 0].plot(epochs_range, vae_val_losses_kld, 'b--s', label='VAE Val')\naxes[1, 0].plot(epochs_range, conv_train_losses_kld, 'r-o', label='ConvVAE Train')\naxes[1, 0].plot(epochs_range, conv_val_losses_kld, 'r--s', label='ConvVAE Val')\naxes[1, 0].set_title('KLD Loss')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Test Comparison\ntest_total_vae = vae_test_metrics['Total Loss']\ntest_total_conv = conv_test_metrics['Total Loss']\ntest_bce_vae = vae_test_metrics['BCE']\ntest_bce_conv = conv_test_metrics['BCE']\ntest_kld_vae = vae_test_metrics['KLD']\ntest_kld_conv = conv_test_metrics['KLD']\n\nx = np.arange(2)\nwidth = 0.2\naxes[1, 1].bar(x - width, [test_total_vae, test_total_conv], width, label='Total Loss', color='skyblue')\naxes[1, 1].bar(x, [test_bce_vae, test_bce_conv], width, label='BCE', color='lightcoral')\naxes[1, 1].bar(x + width, [test_kld_vae, test_kld_conv], width, label='KLD', color='lightgreen')\naxes[1, 1].set_title('Test Set Comparison')\naxes[1, 1].set_xticks(x)\naxes[1, 1].set_xticklabels(['VAE', 'ConvVAE'])\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n</pre> epochs_range = range(1, len(vae_train_losses_total) + 1)  fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # Total Loss axes[0, 0].plot(epochs_range, vae_train_losses_total, 'b-o', label='VAE Train') axes[0, 0].plot(epochs_range, vae_val_losses_total, 'b--s', label='VAE Val') axes[0, 0].plot(epochs_range, conv_train_losses_total, 'r-o', label='ConvVAE Train') axes[0, 0].plot(epochs_range, conv_val_losses_total, 'r--s', label='ConvVAE Val') axes[0, 0].set_title('Total Loss') axes[0, 0].set_xlabel('Epoch') axes[0, 0].legend() axes[0, 0].grid(True, alpha=0.3)  # BCE Loss axes[0, 1].plot(epochs_range, vae_train_losses_bce, 'b-o', label='VAE Train') axes[0, 1].plot(epochs_range, vae_val_losses_bce, 'b--s', label='VAE Val') axes[0, 1].plot(epochs_range, conv_train_losses_bce, 'r-o', label='ConvVAE Train') axes[0, 1].plot(epochs_range, conv_val_losses_bce, 'r--s', label='ConvVAE Val') axes[0, 1].set_title('Reconstruction Loss (BCE)') axes[0, 1].set_xlabel('Epoch') axes[0, 1].legend() axes[0, 1].grid(True, alpha=0.3)  # KLD Loss axes[1, 0].plot(epochs_range, vae_train_losses_kld, 'b-o', label='VAE Train') axes[1, 0].plot(epochs_range, vae_val_losses_kld, 'b--s', label='VAE Val') axes[1, 0].plot(epochs_range, conv_train_losses_kld, 'r-o', label='ConvVAE Train') axes[1, 0].plot(epochs_range, conv_val_losses_kld, 'r--s', label='ConvVAE Val') axes[1, 0].set_title('KLD Loss') axes[1, 0].set_xlabel('Epoch') axes[1, 0].legend() axes[1, 0].grid(True, alpha=0.3)  # Test Comparison test_total_vae = vae_test_metrics['Total Loss'] test_total_conv = conv_test_metrics['Total Loss'] test_bce_vae = vae_test_metrics['BCE'] test_bce_conv = conv_test_metrics['BCE'] test_kld_vae = vae_test_metrics['KLD'] test_kld_conv = conv_test_metrics['KLD']  x = np.arange(2) width = 0.2 axes[1, 1].bar(x - width, [test_total_vae, test_total_conv], width, label='Total Loss', color='skyblue') axes[1, 1].bar(x, [test_bce_vae, test_bce_conv], width, label='BCE', color='lightcoral') axes[1, 1].bar(x + width, [test_kld_vae, test_kld_conv], width, label='KLD', color='lightgreen') axes[1, 1].set_title('Test Set Comparison') axes[1, 1].set_xticks(x) axes[1, 1].set_xticklabels(['VAE', 'ConvVAE']) axes[1, 1].legend() axes[1, 1].grid(True, alpha=0.3, axis='y')  plt.tight_layout() plt.show() <p>Aqui foi realizada uma avalia\u00e7\u00e3o final dos modelos treinados no conjunto de teste, al\u00e9m de gerar amostras aleat\u00f3rias a partir do espa\u00e7o latente para visualizar os espa\u00e7os latentes gerados pelos modelos e a separa\u00e7\u00e3o dos d\u00edgitos em clusters distintos.</p> In\u00a0[63]: Copied! <pre>def get_latent_space(model_name, loader, n_samples=2000):\n    if model_name == 'VAE':\n        model_to_use = model\n    else:\n        model_to_use = ConvVAE_model\n    model_to_use.eval()\n    z_list, labels_list = [], []\n    with torch.no_grad():\n        for data, labels in loader:\n            data = data.to(device)\n            if model_name == 'VAE':\n                mu, _ = model_to_use.encode(data.view(-1, 784))\n            else:\n                mu, _ = model_to_use.encode(data)\n            z_list.append(mu.cpu().numpy())\n            labels_list.append(labels.numpy())\n            if sum(len(z) for z in z_list) &gt;= n_samples:\n                break\n    return np.concatenate(z_list)[:n_samples], np.concatenate(labels_list)[:n_samples]\n\nz_vae, labels_vae = get_latent_space('VAE', val_loader)\nz_conv, labels_conv = get_latent_space('ConvVAE', val_loader)\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nz_vae_2d = tsne.fit_transform(z_vae)\nz_conv_2d = tsne.fit_transform(z_conv)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nscatter1 = axes[0].scatter(z_vae_2d[:, 0], z_vae_2d[:, 1], c=labels_vae, cmap='tab10', s=5, alpha=0.7)\naxes[0].set_title('VAE Latent Space (t-SNE)')\naxes[0].set_xlabel('Dim 1')\naxes[0].set_ylabel('Dim 2')\nplt.colorbar(scatter1, ax=axes[0], label='Digit')\n\nscatter2 = axes[1].scatter(z_conv_2d[:, 0], z_conv_2d[:, 1], c=labels_conv, cmap='tab10', s=5, alpha=0.7)\naxes[1].set_title('ConvVAE Latent Space (t-SNE)')\naxes[1].set_xlabel('Dim 1')\naxes[1].set_ylabel('Dim 2')\nplt.colorbar(scatter2, ax=axes[1], label='Digit')\n\nplt.tight_layout()\nplt.show()\n</pre> def get_latent_space(model_name, loader, n_samples=2000):     if model_name == 'VAE':         model_to_use = model     else:         model_to_use = ConvVAE_model     model_to_use.eval()     z_list, labels_list = [], []     with torch.no_grad():         for data, labels in loader:             data = data.to(device)             if model_name == 'VAE':                 mu, _ = model_to_use.encode(data.view(-1, 784))             else:                 mu, _ = model_to_use.encode(data)             z_list.append(mu.cpu().numpy())             labels_list.append(labels.numpy())             if sum(len(z) for z in z_list) &gt;= n_samples:                 break     return np.concatenate(z_list)[:n_samples], np.concatenate(labels_list)[:n_samples]  z_vae, labels_vae = get_latent_space('VAE', val_loader) z_conv, labels_conv = get_latent_space('ConvVAE', val_loader)  tsne = TSNE(n_components=2, random_state=42, perplexity=30) z_vae_2d = tsne.fit_transform(z_vae) z_conv_2d = tsne.fit_transform(z_conv)  fig, axes = plt.subplots(1, 2, figsize=(12, 5)) scatter1 = axes[0].scatter(z_vae_2d[:, 0], z_vae_2d[:, 1], c=labels_vae, cmap='tab10', s=5, alpha=0.7) axes[0].set_title('VAE Latent Space (t-SNE)') axes[0].set_xlabel('Dim 1') axes[0].set_ylabel('Dim 2') plt.colorbar(scatter1, ax=axes[0], label='Digit')  scatter2 = axes[1].scatter(z_conv_2d[:, 0], z_conv_2d[:, 1], c=labels_conv, cmap='tab10', s=5, alpha=0.7) axes[1].set_title('ConvVAE Latent Space (t-SNE)') axes[1].set_xlabel('Dim 1') axes[1].set_ylabel('Dim 2') plt.colorbar(scatter2, ax=axes[1], label='Digit')  plt.tight_layout() plt.show() In\u00a0[64]: Copied! <pre>import torch.nn.functional as F\n\ndef test_recon_and_metrics(model_name, loader, n_test=1000):\n    if model_name == 'VAE':\n        model_to_use = model\n    else:\n        model_to_use = ConvVAE_model\n    model_to_use.eval()\n    total_loss, total_bce, total_kld, total_mse_val = 0, 0, 0, 0\n    n_samples = 0\n    with torch.no_grad():\n        for data, _ in loader:\n            data = data.to(device)\n            if model_name == 'VAE':\n                recon, mu, logvar = model_to_use(data.view(-1, 784))\n                data_flat = data.view(-1, 784)\n                loss, bce, kld = loss_function(recon, data_flat, mu, logvar)\n                mse_val = F.mse_loss(recon, data_flat, reduction='sum').item()\n            else:\n                recon, mu, logvar = model_to_use(data)\n                # Flatten for BCE calculation if loss_function expects it\n                loss, bce, kld = Conv_loss_function(recon.view(-1, 784), data.view(-1, 784), mu, logvar)\n                mse_val = F.mse_loss(recon, data, reduction='sum').item()\n\n            total_loss += loss.item() * len(data)\n            total_bce += bce.item() * len(data)\n            total_kld += kld.item() * len(data)\n            total_mse_val += mse_val\n            n_samples += len(data)\n            if n_samples &gt;= n_test:\n                break\n\n    psnr = 10 * np.log10(1.0 / (total_mse_val / n_samples)) if total_mse_val &gt; 0 else 0\n    if model_name == 'VAE':\n        recon_viz = recon.view(recon.size(0), 1, 28, 28)\n    else:\n        recon_viz = recon\n\n    return {\n        'Total Loss': total_loss / n_samples,\n        'BCE': total_bce / n_samples,\n        'KLD': total_kld / n_samples,\n        'MSE': total_mse_val / n_samples,\n        'PSNR': psnr\n    }, recon_viz[:8], data[:8]\n\nvae_metrics, recon_vae, orig = test_recon_and_metrics('VAE', test_loader)\nconv_metrics, recon_conv, _ = test_recon_and_metrics('ConvVAE', test_loader)\n\nprint(\"M\u00e9tricas de Compara\u00e7\u00e3o:\")\nprint(f\"{'M\u00e9trica':&lt;12} {'VAE':&lt;10} {'ConvVAE':&lt;10} {'Melhor':&lt;8}\")\nprint(\"-\" * 45)\nfor key in vae_metrics:\n    v_vae = vae_metrics[key]\n    v_conv = conv_metrics[key]\n    melhor = 'ConvVAE' if (key == 'PSNR' and v_conv &gt; v_vae) or (key != 'PSNR' and v_conv &lt; v_vae) else 'VAE'\n    print(f\"{key:&lt;12} {v_vae:&lt;10.4f} {v_conv:&lt;10.4f} {melhor:&lt;8}\")\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\ngrid_orig = make_grid(orig[:8].cpu(), nrow=8, normalize=True)\naxes[0].imshow(grid_orig.permute(1, 2, 0), cmap='gray')\naxes[0].set_title('Original')\naxes[0].axis('off')\n\ngrid_vae = make_grid(recon_vae.cpu(), nrow=8, normalize=True)\naxes[1].imshow(grid_vae.permute(1, 2, 0), cmap='gray')\naxes[1].set_title('VAE Reconstru\u00e7\u00f5es')\naxes[1].axis('off')\n\ngrid_conv = make_grid(recon_conv[:8].cpu(), nrow=8, normalize=True)\naxes[2].imshow(grid_conv.permute(1, 2, 0), cmap='gray')\naxes[2].set_title('ConvVAE Reconstru\u00e7\u00f5es')\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> import torch.nn.functional as F  def test_recon_and_metrics(model_name, loader, n_test=1000):     if model_name == 'VAE':         model_to_use = model     else:         model_to_use = ConvVAE_model     model_to_use.eval()     total_loss, total_bce, total_kld, total_mse_val = 0, 0, 0, 0     n_samples = 0     with torch.no_grad():         for data, _ in loader:             data = data.to(device)             if model_name == 'VAE':                 recon, mu, logvar = model_to_use(data.view(-1, 784))                 data_flat = data.view(-1, 784)                 loss, bce, kld = loss_function(recon, data_flat, mu, logvar)                 mse_val = F.mse_loss(recon, data_flat, reduction='sum').item()             else:                 recon, mu, logvar = model_to_use(data)                 # Flatten for BCE calculation if loss_function expects it                 loss, bce, kld = Conv_loss_function(recon.view(-1, 784), data.view(-1, 784), mu, logvar)                 mse_val = F.mse_loss(recon, data, reduction='sum').item()              total_loss += loss.item() * len(data)             total_bce += bce.item() * len(data)             total_kld += kld.item() * len(data)             total_mse_val += mse_val             n_samples += len(data)             if n_samples &gt;= n_test:                 break      psnr = 10 * np.log10(1.0 / (total_mse_val / n_samples)) if total_mse_val &gt; 0 else 0     if model_name == 'VAE':         recon_viz = recon.view(recon.size(0), 1, 28, 28)     else:         recon_viz = recon      return {         'Total Loss': total_loss / n_samples,         'BCE': total_bce / n_samples,         'KLD': total_kld / n_samples,         'MSE': total_mse_val / n_samples,         'PSNR': psnr     }, recon_viz[:8], data[:8]  vae_metrics, recon_vae, orig = test_recon_and_metrics('VAE', test_loader) conv_metrics, recon_conv, _ = test_recon_and_metrics('ConvVAE', test_loader)  print(\"M\u00e9tricas de Compara\u00e7\u00e3o:\") print(f\"{'M\u00e9trica':&lt;12} {'VAE':&lt;10} {'ConvVAE':&lt;10} {'Melhor':&lt;8}\") print(\"-\" * 45) for key in vae_metrics:     v_vae = vae_metrics[key]     v_conv = conv_metrics[key]     melhor = 'ConvVAE' if (key == 'PSNR' and v_conv &gt; v_vae) or (key != 'PSNR' and v_conv &lt; v_vae) else 'VAE'     print(f\"{key:&lt;12} {v_vae:&lt;10.4f} {v_conv:&lt;10.4f} {melhor:&lt;8}\")  fig, axes = plt.subplots(1, 3, figsize=(15, 5)) grid_orig = make_grid(orig[:8].cpu(), nrow=8, normalize=True) axes[0].imshow(grid_orig.permute(1, 2, 0), cmap='gray') axes[0].set_title('Original') axes[0].axis('off')  grid_vae = make_grid(recon_vae.cpu(), nrow=8, normalize=True) axes[1].imshow(grid_vae.permute(1, 2, 0), cmap='gray') axes[1].set_title('VAE Reconstru\u00e7\u00f5es') axes[1].axis('off')  grid_conv = make_grid(recon_conv[:8].cpu(), nrow=8, normalize=True) axes[2].imshow(grid_conv.permute(1, 2, 0), cmap='gray') axes[2].set_title('ConvVAE Reconstru\u00e7\u00f5es') axes[2].axis('off')  plt.tight_layout() plt.show() <pre>M\u00e9tricas de Compara\u00e7\u00e3o:\nM\u00e9trica      VAE        ConvVAE    Melhor  \n---------------------------------------------\nTotal Loss   13512.8116 13149.8904 ConvVAE \nBCE          10276.8264 9847.1964  ConvVAE \nKLD          3235.9854  3302.6939  VAE     \nMSE          10.8774    9.7493     ConvVAE \nPSNR         -10.3653   -9.8897    ConvVAE \n</pre>"},{"location":"VAE_Exercise/#vae-mnist","title":"VAE - MNIST\u00b6","text":"<p>Aluno: Henrique Fazzio Badin</p>"},{"location":"VAE_Exercise/#sumario","title":"Sum\u00e1rio\u00b6","text":"<ol> <li>Objetivo &amp; Contexto</li> <li>Prepara\u00e7\u00e3o dos Dados</li> <li>Arquitetura do VAE</li> <li>Truque da Reparametriza\u00e7\u00e3o</li> <li>Treinamento</li> <li>Avalia\u00e7\u00e3o &amp; Amostragem</li> <li>Visualiza\u00e7\u00f5es</li> <li>Relat\u00f3rio &amp; Conclus\u00f5es</li> </ol>"},{"location":"VAE_Exercise/#objetivo-contexto","title":"Objetivo &amp; Contexto \u00b6","text":"<p>Implementar e avaliar um Variational Autoencoder (VAE) no MNIST, entendendo:</p> <ul> <li>Organiza\u00e7\u00e3o do dataset e normaliza\u00e7\u00e3o;</li> <li>Encoder/Decoder e dimensionalidade do espa\u00e7o latente;</li> <li>Reconstru\u00e7\u00e3o das imagens + KL Divergence;</li> <li>Processo de treino, valida\u00e7\u00e3o e amostragem;</li> <li>Visualiza\u00e7\u00e3o de reconstru\u00e7\u00f5es e do espa\u00e7o latente.</li> </ul>"},{"location":"VAE_Exercise/#preparacao-dos-dados","title":"Prepara\u00e7\u00e3o dos Dados \u00b6","text":"<ul> <li>Download/Carregamento do MNIST;</li> <li>Normaliza\u00e7\u00e3o para <code>[0, 1]</code> e cria\u00e7\u00e3o de DataLoaders;</li> <li>Split entre treino e valida\u00e7\u00e3o (quando aplic\u00e1vel).</li> </ul>"},{"location":"VAE_Exercise/#arquitetura-do-vae","title":"Arquitetura do VAE \u00b6","text":"<p>Encoder: projeta a imagem em par\u00e2metros no espa\u00e7o latente. Decoder: reconstr\u00f3i a imagem a partir de uma amostra z do espa\u00e7o latente.</p>"},{"location":"VAE_Exercise/#configuracao-do-loop-de-treinamento","title":"Configura\u00e7\u00e3o do Loop de Treinamento\u00b6","text":"<p>Aqui preparamos as fun\u00e7\u00f5es que v\u00e3o treinar, validar e testar o VAE. A fun\u00e7\u00e3o <code>train()</code> atualiza os pesos do modelo usando os dados de treino, a <code>validate()</code> checa o desempenho em dados n\u00e3o vistos durante o treino (sem atualizar pesos), e a <code>test()</code> faz a avalia\u00e7\u00e3o final. Tamb\u00e9m criamos uma pasta <code>results</code> para salvar as imagens reconstru\u00eddas e geradas ao longo do processo, permitindo acompanhar visualmente a evolu\u00e7\u00e3o do modelo.</p>"},{"location":"VAE_Exercise/#treinamento-do-vae","title":"Treinamento do VAE \u00b6","text":"<p>Aqui rodamos o loop principal de treinamento por 10 \u00e9pocas. Em cada \u00e9poca, chamamos <code>train()</code> para atualizar os pesos, <code>validate()</code> para checar o desempenho, e geramos 64 amostras aleat\u00f3rias do espa\u00e7o latente para ver que tipo de d\u00edgitos o modelo est\u00e1 aprendendo a criar. Ao final, testamos no conjunto de teste para obter as m\u00e9tricas finais de desempenho.</p>"},{"location":"VAE_Exercise/#analise-dos-resultados-com-base-no-output-acima","title":"An\u00e1lise dos Resultados (com base no output acima)\u00b6","text":"<p>Resumo do Treinamento (a partir dos logs):</p> \u00c9poca Train Loss Val Loss 1 169.49 132.14 2 124.44 119.04 3 116.41 114.67 4 113.07 112.29 5 111.09 110.72 6 109.78 109.51 7 108.81 108.63 8 108.04 108.08 9 107.49 108.07 10 107.03 107.42 <ul> <li>Tend\u00eancia geral do Val Loss: converg\u00eancia suave e consistente ao longo das 10 \u00e9pocas, partindo de 132.14 e atingindo 107.42, indicando aprendizado efetivo sem overfitting aparente.</li> </ul> <p>M\u00e9tricas finais no Test: Total Loss = 106.39, BCE (Recon Loss) = 80.65, KL Divergence = 25.73.</p> <ul> <li>A Recon Loss reflete a qualidade da reconstru\u00e7\u00e3o (menor \u00e9 melhor) \u2014 valor de 80.65 indica boas reconstru\u00e7\u00f5es.</li> <li>A KL Divergence regula a proximidade da distribui\u00e7\u00e3o latente \u00e0 Normal padr\u00e3o (equil\u00edbrio entre reconstru\u00e7\u00e3o e variabilidade) \u2014 valor de 25.73 mostra regulariza\u00e7\u00e3o adequada sem colapso posterior.</li> <li>O modelo conseguiu balancear bem ambos os termos, com loss total final pr\u00f3xima da valida\u00e7\u00e3o, sugerindo boa generaliza\u00e7\u00e3o.</li> </ul>"},{"location":"VAE_Exercise/#motivacao-para-testar-convvae","title":"Motiva\u00e7\u00e3o para Testar ConvVAE\u00b6","text":"<p>Decidi implementar uma vers\u00e3o Convolutional VAE (ConvVAE) al\u00e9m do VAE tradicional para comparar o impacto das camadas convolucionais em dados de imagem como MNIST. As convolu\u00e7\u00f5es capturam melhor padr\u00f5es espaciais locais (ex.: bordas de d\u00edgitos), o que pode melhorar:</p> <ul> <li>Qualidade de reconstru\u00e7\u00e3o (menos borr\u00f5es)</li> <li>Converg\u00eancia (mais r\u00e1pida inicial)</li> </ul> <p>Usei a mesma dimens\u00e3o latente (20D) e dataset para isolamento de vari\u00e1veis. M\u00e9tricas como MSE e PSNR foram calculadas para quantificar melhorias.</p>"},{"location":"VAE_Exercise/#analise-dos-resultados-com-base-no-output-acima","title":"An\u00e1lise dos Resultados (com base no output acima)\u00b6","text":"<p>Resumo do Treinamento (a partir dos logs):</p> \u00c9poca Train Loss Val Loss 1 546.21 350.53 2 318.96 299.34 3 291.71 286.27 4 281.66 279.28 5 276.52 274.55 6 273.25 272.12 7 270.60 271.58 8 268.61 268.43 9 267.41 267.01 10 265.86 265.75 <ul> <li>Tend\u00eancia geral do Val Loss: converg\u00eancia consistente e acentuada ao longo das 10 \u00e9pocas, partindo de 350.53 e atingindo 265.75, demonstrando aprendizado progressivo e est\u00e1vel. A diferen\u00e7a entre Train e Val Loss diminui gradualmente, sugerindo boa capacidade de generaliza\u00e7\u00e3o.</li> </ul> <p>M\u00e9tricas finais no Test: Total Loss = 103.35, BCE (Recon Loss) = 77.32, KL Divergence = 26.03.</p> <ul> <li>A Recon Loss de 77.32 \u00e9 inferior \u00e0 do VAE tradicional (80.65), indicando que o ConvVAE produz reconstru\u00e7\u00f5es de melhor qualidade gra\u00e7as \u00e0s camadas convolucionais que capturam melhor os padr\u00f5es espaciais locais.</li> <li>A KL Divergence de 26.03 \u00e9 ligeiramente superior \u00e0 do VAE (25.73), mas permanece em faixa saud\u00e1vel, mantendo boa regulariza\u00e7\u00e3o do espa\u00e7o latente sem colapso posterior.</li> <li>O Total Loss de 103.35 \u00e9 inferior ao VAE (106.39), confirmando superioridade geral do ConvVAE em termos de performance.</li> <li>O modelo ConvVAE demonstrou converg\u00eancia mais r\u00e1pida nas \u00e9pocas iniciais (queda de ~200 pontos de loss entre \u00e9pocas 1-3) comparado ao VAE, validando a hip\u00f3tese de que convolu\u00e7\u00f5es aceleram o aprendizado em dados de imagem.</li> </ul>"},{"location":"VAE_Exercise/#visualizacoes","title":"Visualiza\u00e7\u00f5es \u00b6","text":"<p>Nesta se\u00e7\u00e3o, criei gr\u00e1ficos comparativos para analisar o desempenho dos dois modelos (VAE e ConvVAE) ao longo do treinamento. A visualiza\u00e7\u00e3o est\u00e1 organizada em 4 pain\u00e9is:</p> <p>1. Total Loss (superior esquerdo): Mostra a evolu\u00e7\u00e3o da perda total durante as 10 \u00e9pocas. Permite verificar se os modelos est\u00e3o convergindo adequadamente e se h\u00e1 sinais de overfitting (quando a linha de valida\u00e7\u00e3o come\u00e7a a subir enquanto o treino continua caindo).</p> <p>2. Reconstruction Loss - BCE (superior direito): Foca especificamente na qualidade da reconstru\u00e7\u00e3o das imagens. Valores menores indicam que o modelo est\u00e1 reproduzindo melhor as imagens originais. Este \u00e9 um dos componentes mais importantes para avaliar a performance visual do VAE.</p> <p>3. KL Divergence (inferior esquerdo): Mede o quanto o espa\u00e7o latente aprendido se desvia de uma distribui\u00e7\u00e3o normal padr\u00e3o. Um valor muito baixo pode indicar \"posterior collapse\" (o modelo ignora o espa\u00e7o latente), enquanto valores muito altos podem prejudicar a reconstru\u00e7\u00e3o. O ideal \u00e9 um equil\u00edbrio saud\u00e1vel.</p> <p>4. Compara\u00e7\u00e3o no Test Set (inferior direito): Gr\u00e1fico de barras que resume as m\u00e9tricas finais dos dois modelos no conjunto de teste, facilitando a compara\u00e7\u00e3o direta entre VAE e ConvVAE em termos de loss total, reconstru\u00e7\u00e3o e regulariza\u00e7\u00e3o.</p> <p>Essas visualiza\u00e7\u00f5es s\u00e3o essenciais para entender n\u00e3o apenas qual modelo performa melhor, mas tamb\u00e9m como e quando cada arquitetura atinge sua melhor performance durante o treinamento.</p>"},{"location":"VAE_Exercise/#analise-dos-graficos-de-treinamento","title":"An\u00e1lise dos Gr\u00e1ficos de Treinamento\u00b6","text":"<p>Analisando os gr\u00e1ficos gerados, pode-se extrair alguns insights interessantes sobre o comportamento dos dois modelos durante o treinamento.</p>"},{"location":"VAE_Exercise/#total-loss","title":"Total Loss\u00b6","text":"<p>O ConvVAE come\u00e7ou com um loss muito mais alto (~28.000) comparado ao VAE tradicional (~17.000). Isso acontece porque as camadas convolucionais precisam de algumas \u00e9pocas para \"aprender\" os filtros \u00fateis.</p> <p>O interessante \u00e9 que depois da \u00e9poca 3, os dois modelos ficaram bem pr\u00f3ximos, e o ConvVAE at\u00e9 passou a ter uma performance ligeiramente melhor.</p>"},{"location":"VAE_Exercise/#reconstrucao-bce-onde-o-convvae-brilha","title":"Reconstru\u00e7\u00e3o (BCE): Onde o ConvVAE Brilha\u00b6","text":"<p>No gr\u00e1fico de BCE, que mostra especificamente a qualidade da reconstru\u00e7\u00e3o das imagens, o padr\u00e3o foi similar. Mas aqui o ConvVAE, no final do treinamento, alcan\u00e7ou uma BCE um pouco menor (~10.400 vs ~10.600), confirmando que as convolu\u00e7\u00f5es realmente ajudam a capturar melhor os detalhes espaciais das imagens.</p> <p>A diferen\u00e7a n\u00e3o \u00e9 gigante, mas quando olhamos as imagens reconstru\u00eddas visualmente, d\u00e1 pra perceber que o ConvVAE gera d\u00edgitos mais n\u00edtidos e definidos.</p>"},{"location":"VAE_Exercise/#kl-divergence-o-trade-off-necessario","title":"KL Divergence: O Trade-off Necess\u00e1rio\u00b6","text":"<p>Uma coisa que me surpreendeu foi a KLD crescente em ambos os modelos (de ~1.800 para ~3.200). Pesquisando, descobri que isso \u00e9 esperado, j\u00e1 que, conforme o modelo aprende, ele passa a usar mais o espa\u00e7o latente, ent\u00e3o a diverg\u00eancia aumenta. O importante \u00e9 que ela n\u00e3o ficou pr\u00f3xima de zero.</p> <p>O ConvVAE teve uma KLD consistentemente um pouco mais alta ap\u00f3s a \u00e9poca 2, especialmente na valida\u00e7\u00e3o (~3.400 vs ~3.200). Segundo minhas pesquisas, isso explica porque os clusters no t-SNE ficaram mais definidos. O modelo est\u00e1 \"for\u00e7ando\" mais regulariza\u00e7\u00e3o no espa\u00e7o latente, o que no final das contas ajuda na capacidade generativa.</p>"},{"location":"VAE_Exercise/#comparacao-final-no-test-set","title":"Compara\u00e7\u00e3o Final no Test Set\u00b6","text":"<p>O gr\u00e1fico de barras resume as m\u00e9tricas. O ConvVAE ganhou na BCE (melhor reconstru\u00e7\u00e3o), empatou praticamente na Total Loss, e teve uma KLD ligeiramente superior. Para uma aplica\u00e7\u00e3o focada em gerar ou reconstruir imagens, esse trade-off vale muito a pena.</p>"},{"location":"VAE_Exercise/#avaliacao-amostragem","title":"Avalia\u00e7\u00e3o &amp; Amostragem \u00b6","text":""},{"location":"VAE_Exercise/#analise-dos-resultados","title":"An\u00e1lise dos Resultados\u00b6","text":""},{"location":"VAE_Exercise/#espaco-latente","title":"Espa\u00e7o Latente\u00b6","text":"<p>O t-SNE mostra clusters separados por d\u00edgito em ambos, mas ConvVAE tem clusters mais compactos e definidos, sugerindo representa\u00e7\u00f5es mais discriminativas gra\u00e7as \u00e0s features locais capturadas por conv layers.</p> <p>M\u00e9tricas finais: ConvVAE \u00e9 ligeiramente superior em todas, como visto na tabela gerada no output acima.</p>"},{"location":"VAE_Exercise/#disclaimer","title":"Disclaimer\u00b6","text":"<p>Este trabalho foi desenvolvido com o apoio de ferramentas de IA generativa (GitHub Copilot, Claude, ChatGPT e Grok) para:</p> <ul> <li>Cria\u00e7\u00e3o e formata\u00e7\u00e3o de documenta\u00e7\u00e3o em Markdown</li> <li>Gera\u00e7\u00e3o de gr\u00e1ficos e visualiza\u00e7\u00f5es</li> <li>Aux\u00edlio na interpreta\u00e7\u00e3o dos resultados e insights t\u00e9cnicos</li> </ul>"},{"location":"VAE_Exercise/#fundamentacao-teorica","title":"Fundamenta\u00e7\u00e3o Te\u00f3rica\u00b6","text":"<p>A base te\u00f3rica sobre Variational Autoencoders (VAE) foi estudada a partir do material disponibilizado no site da disciplina. Ap\u00f3s compreender os conceitos fundamentais (espa\u00e7o latente, reparametriza\u00e7\u00e3o, fun\u00e7\u00e3o de perda, etc.), al\u00e9m das aulas em sala de aula. Utilizei tamb\u00e9m a biblioteca PyTorch para implementa\u00e7\u00e3o pr\u00e1tica, seguindo como refer\u00eancia o exemplo oficial de VAE do PyTorch.</p> <p>Importante: O c\u00f3digo foi constru\u00eddo com base no exemplo do PyTorch mencionado, adaptado para o contexto do trabalho e enriquecido com an\u00e1lises pr\u00f3prias.</p>"},{"location":"VAE_Exercise/#secao-convvae-convolutional-vae","title":"Se\u00e7\u00e3o ConvVAE (Convolutional VAE)\u00b6","text":"<p>A implementa\u00e7\u00e3o do ConvVAE n\u00e3o fazia parte dos requisitos da atividade. Esta se\u00e7\u00e3o foi desenvolvida por iniciativa e curiosidade pr\u00f3pria, ap\u00f3s assistir a um v\u00eddeo no YouTube que explicava como camadas convolucionais podem melhorar a captura de padr\u00f5es espaciais em dados de imagem.</p> <p>Quis explorar empiricamente essa teoria e comparar os resultados com o VAE tradicional, observando diferen\u00e7as em:</p> <ul> <li>Qualidade de reconstru\u00e7\u00e3o</li> <li>Velocidade de converg\u00eancia</li> <li>Estrutura do espa\u00e7o latente</li> </ul> <p>Todas as an\u00e1lises comparativas e conclus\u00f5es apresentadas refletem meu entendimento do assunto, constru\u00eddo a partir do estudo te\u00f3rico e da experimenta\u00e7\u00e3o pr\u00e1tica com ambas as arquiteturas.</p>"},{"location":"mlp/","title":"Understanding Multi-Layer Perceptrons (MLPs)","text":""},{"location":"mlp/#activity-understanding-multi-layer-perceptrons-mlps","title":"Activity: Understanding Multi-Layer Perceptrons (MLPs)","text":"Edi\u00e7\u00e3o <p>2025.2</p> <p>Nesta documenta\u00e7\u00e3o s\u00e3o mostradas as realiza\u00e7\u00f5es de quatro exerc\u00edcios pr\u00e1ticos de MLP que realizei ao longo do semestre. Cada exerc\u00edcio teve como objetivo aprofundar a compreens\u00e3o sobre o funcionamento interno das redes neurais, desde o c\u00e1lculo manual de forward pass e backpropagation at\u00e9 a implementa\u00e7\u00e3o de MLPs para tarefas de classifica\u00e7\u00e3o bin\u00e1ria e multiclasse. A seguir, descrevo cada exerc\u00edcio, apresento os c\u00f3digos utilizados e discuto os resultados obtidos.</p>"},{"location":"mlp/#exercicio-1-calculo-manual-de-um-mlp","title":"Exerc\u00edcio\u00a01 \u2014 C\u00e1lculo manual de um MLP","text":"<p>Para o primeiro exerc\u00edcio considerei um MLP simples com 2 features de entrada, 2 neur\u00f4nios em 1 camada oculta e 1 neur\u00f4nio de sa\u00edda. Para tal, utilizei a fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code> tanto para a camada oculta como para a camada de sa\u00edda. A perda foi calculada usando o erro quadr\u00e1tico m\u00e9dio (MSE). Os pesos e vieses foram inicializados com valores fixos para facilitar a verifica\u00e7\u00e3o dos c\u00e1lculos.</p> <p>Usei os seguintes valores:</p> <ul> <li>Entrada <code>x</code>: <code>[0.5, -0.2]</code></li> <li>R\u00f3tulo <code>y</code>: <code>1</code></li> <li>Pesos <code>W1</code> (camada oculta): <code>[[0.3, -0.1], [0.2, 0.4]]</code></li> <li>Vieses <code>b1</code> (camada oculta): <code>[0.1, -0.2]</code></li> <li>Pesos <code>W2</code> (camada de sa\u00edda): <code>[[0.5, -0.3]]</code></li> <li>Vieses <code>b2</code> (camada de sa\u00edda): <code>[0.2]</code></li> <li>Taxa de aprendizado <code>\u03b7</code>: <code>0.3</code></li> </ul> <p>Primeiro, calculei o forward pass para obter a predi\u00e7\u00e3o <code>y_pred</code> e a perda <code>L</code>. Em seguida, realizei o backpropagation para calcular os gradientes dos pesos e vieses. Finalmente, atualizei os par\u00e2metros usando o gradiente descendente. A execu\u00e7\u00e3o completa do c\u00f3digo est\u00e1 abaixo:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\nx = np.array([0.5, -0.2])\ny = 1\nW1 = np.array([[0.3, -0.1], [0.2, 0.4]])\nb1 = np.array([0.1, -0.2])\nW2 = np.array([[0.5, -0.3]])\nb2 = np.array([0.2])\neta = 0.3\n\ndef tanh(u):\n    return (np.exp(2 * u) - 1) / (np.exp(2 * u) + 1)\n\n# Forward\nz1 = W1.dot(x) + b1\nh1 = tanh(z1)\nu2 = W2.dot(h1) + b2\ny_pred = tanh(u2)\nL = (y - y_pred)**2 / len(x)\n\n# Backward\ndelL_dely_pred = -2 * (y - y_pred) / len(x)\ndely_pred_du2 = 1 - y_pred**2\ndelL_delW2 = delL_dely_pred * dely_pred_du2 * h1\ndelL_delb2 = delL_dely_pred * dely_pred_du2\ndelL_delh1 = delL_dely_pred * dely_pred_du2 * W2\ndelh1_dz1 = 1 - h1**2\ndelL_delW1 = np.outer((delL_delh1 * delh1_dz1).reshape(-1), x)\ndelL_delb1 = (delL_delh1 * delh1_dz1)\n\n# Update\nW2_new = W2 - eta * delL_delW2\nb2_new = b2 - eta * delL_delb2\nW1_new = W1 - eta * delL_delW1\nb1_new = b1 - eta * delL_delb1\n\nprint(\"z1 =\", z1)\nprint(\"h1 =\", h1)\nprint(\"u2 =\", u2)\nprint(\"y_pred =\", y_pred)\nprint(\"loss =\", L)\nprint(\"grad W2 =\", delL_delW2)\nprint(\"grad b2 =\", delL_delb2)\nprint(\"grad W1 =\\n\", delL_delW1)\nprint(\"grad b1 =\", delL_delb1)\nprint(\"W1 updated =\\n\", W1_new)\nprint(\"b1 updated =\", b1_new)\nprint(\"W2 updated =\", W2_new)\nprint(\"b2 updated =\", b2_new)</pre> Output Clear <pre><code></code></pre> <p></p>"},{"location":"mlp/#exercicio-2-classificacao-binaria-sintetica","title":"Exerc\u00edcio\u00a02 \u2014 Classifica\u00e7\u00e3o bin\u00e1ria sint\u00e9tica","text":"<p>Para essa tarefa, criei um conjunto de 1000 dados sint\u00e9tico com duas classes. Usei a fun\u00e7\u00e3o <code>make_classification</code> do <code>scikit-learn</code> para gerar os dados, garantindo que cada classe tivesse uma distribui\u00e7\u00e3o diferente.</p> <p>Gerei 500 pontos para a classe\u202f0 com um \u00fanico cluster e, separadamente, dois grupos de 250 pontos cada para a classe\u202f1, de modo que ela ficasse com dois clusters. Todos os conjuntos foram combinados, resultando em um dataset com 2 features, ambas informativas (n_informative=2), sem atributos redundantes (n_redundant=0), e o random_state=42 garantiu a reprodutibilidade.</p> <p>Por fim, ajustei class_sep para cerca de 1,3 e mantive flip_y=0 para evitar r\u00f3tulos incorretos.</p> <p>Implementei ent\u00e3o um MLP simples com uma camada oculta de 12 neur\u00f4nios (<code>tanh</code>) e uma sa\u00edda com um neur\u00f4nio (<code>tanh</code>). A fun\u00e7\u00e3o de perda foi a MSE, usada na etapa passada. O treinamento foi realizado por 500 \u00e9pocas, com uma taxa de aprendizado de 0,05. Para chegar nesses valores, testei diferentes quantidades de neur\u00f4nios e taxas de aprendizado, sem exagerar para manter a simplicidade. O c\u00f3digo completo est\u00e1 abaixo:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nn = 1000\nX0, _ = make_classification(\n    n_samples=n // 2,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    n_classes=2,\n    class_sep=1.3,\n    random_state=42,\n)\ny0 = np.zeros(X0.shape[0], dtype=int)\n\nX1a, _ = make_classification(\n    n_samples=n // 4,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    n_classes=2,\n    class_sep=1.3,\n    random_state=42,\n)\nX1b, _ = make_classification(\n    n_samples=n - (n // 2 + n // 4),\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    n_classes=2,\n    class_sep=1.3,\n    random_state=42,\n)\ny1a = np.ones(X1a.shape[0], dtype=int)\ny1b = np.ones(X1b.shape[0], dtype=int)\n\nX = np.vstack([X0, X1a, X1b])\ny = np.hstack([y0, y1a, y1b])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nrng = np.random.default_rng(42)\nW1 = rng.normal(0, 1, (12, 2)) / np.sqrt(2)\nb1 = np.zeros(12)\nW2 = rng.normal(0, 1, (1, 12)) / np.sqrt(12)\nb2 = np.zeros(1)\n\n\ndef tanh(x):\n    return np.tanh(x)\n\n\neta = 0.05\nepochs = 500\ny_train = y_train.reshape(-1, 1).astype(float)\n\nfor ep in range(1, epochs + 1):\n    # forward\n    z1 = X_train @ W1.T + b1\n    h1 = tanh(z1)\n    z2 = h1 @ W2.T + b2\n    y_pred = tanh(z2)\n\n    # loss (MSE)\n    loss = np.mean((y_train - y_pred) ** 2)\n\n    # backward\n    grad_z2 = 2 * (y_pred - y_train) * (1 - y_pred**2) / len(X_train)\n    grad_W2 = grad_z2.T @ h1\n    grad_b2 = grad_z2.sum(axis=0)\n\n    grad_h1 = grad_z2 @ W2\n    grad_z1 = grad_h1 * (1 - h1**2)\n    grad_W1 = grad_z1.T @ X_train\n    grad_b1 = grad_z1.sum(axis=0)\n\n    # update\n    W2 -= eta * grad_W2\n    b2 -= eta * grad_b2\n    W1 -= eta * grad_W1\n    b1 -= eta * grad_b1\n\n    if ep % 50 == 0 or ep == 1:\n        print(f\"\u00e9poca {ep} | loss {loss:.4f}\")\n\nz1 = X_test @ W1.T + b1\nh1 = tanh(z1)\nz2 = h1 @ W2.T + b2\ny_pred_test = tanh(z2)\n\nypred = (y_pred_test.ravel() &gt;= 0).astype(int)\n\nacc = (ypred == y_test).mean()\nprint(f\"\\nacur\u00e1cia teste: {(acc*100):.4f}%\")</pre> Output Clear <pre><code></code></pre> <p></p> <p>Depois, com a ajuda do chatGPT, adicionei uma visualiza\u00e7\u00e3o animada do processo de treinamento, mostrando como a fronteira de decis\u00e3o evolui ao longo das \u00e9pocas.</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nn = 1000\nX0, _ = make_classification(\n    n_samples=n // 2,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    n_classes=2,\n    class_sep=1.3,\n    random_state=42,\n)\ny0 = np.zeros(X0.shape[0], dtype=int)\n\nX1a, _ = make_classification(\n    n_samples=n // 4,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    n_classes=2,\n    class_sep=1.3,\n    random_state=42,\n)\nX1b, _ = make_classification(\n    n_samples=n - (n // 2 + n // 4),\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    n_classes=2,\n    class_sep=1.3,\n    random_state=42,\n)\ny1a = np.ones(X1a.shape[0], dtype=int)\ny1b = np.ones(X1b.shape[0], dtype=int)\n\nX = np.vstack([X0, X1a, X1b])\ny = np.hstack([y0, y1a, y1b])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nrng = np.random.default_rng(42)\nW1 = rng.normal(0, 1, (12, 2)) / np.sqrt(2)\nb1 = np.zeros(12)\nW2 = rng.normal(0, 1, (1, 12)) / np.sqrt(12)\nb2 = np.zeros(1)\n\n\ndef tanh(x):\n    return np.tanh(x)\n\n\neta = 0.05\nepochs = 500\ny_train = y_train.reshape(-1, 1).astype(float)\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n\n\ndef forward(X_in):\n    z1 = X_in @ W1.T + b1\n    h1 = tanh(z1)\n    z2 = h1 @ W2.T + b2\n    return tanh(z2), h1\n\n\ndef update(frame):\n    global W1, b1, W2, b2\n\n    # treina algumas \u00e9pocas por frame\n    for _ in range(5):\n        y_pred, h1 = forward(X_train)\n        loss = np.mean((y_train - y_pred) ** 2)\n\n        # backward\n        grad_z2 = 2 * (y_pred - y_train) * (1 - y_pred**2) / len(X_train)\n        grad_W2 = grad_z2.T @ h1\n        grad_b2 = grad_z2.sum(axis=0)\n\n        grad_h1 = grad_z2 @ W2\n        grad_z1 = grad_h1 * (1 - h1**2)\n        grad_W1 = grad_z1.T @ X_train\n        grad_b1 = grad_z1.sum(axis=0)\n\n        # update\n        W2 -= eta * grad_W2\n        b2 -= eta * grad_b2\n        W1 -= eta * grad_W1\n        b1 -= eta * grad_b1\n\n    # recalcula a fronteira\n    y_grid, _ = forward(np.c_[xx.ravel(), yy.ravel()])\n    y_grid_class = (y_grid.ravel() &gt;= 0).astype(int)\n\n    # predi\u00e7\u00f5es no treino\n    y_pred_train, _ = forward(X_train)\n    y_pred_class = (y_pred_train.ravel() &gt;= 0).astype(int)\n\n    # identifica erros e acertos\n    errors = y_pred_class != y_train.ravel()\n    correct = ~errors\n    acc = (y_pred_class == y_train.ravel()).mean() * 100\n\n    ax.clear()\n    ax.contourf(xx, yy, y_grid_class.reshape(xx.shape), alpha=0.3, cmap=plt.cm.coolwarm)\n\n    # pontos corretos\n    ax.scatter(\n        X_train[correct, 0],\n        X_train[correct, 1],\n        c=y_train.ravel()[correct],\n        cmap=plt.cm.coolwarm,\n        edgecolors=\"k\",\n        s=30,\n        marker=\"o\",\n        label=\"Corretos\",\n    )\n\n    # pontos errados (X)\n    ax.scatter(\n        X_train[errors, 0],\n        X_train[errors, 1],\n        c=y_train.ravel()[errors],\n        cmap=plt.cm.coolwarm,\n        edgecolors=\"k\",\n        s=60,\n        marker=\"x\",\n        label=\"Errados\",\n    )\n\n    ax.set_title(f\"\u00c9poca {frame*5} | Loss={loss:.4f} | Acc={acc:.2f}%\")\n    ax.legend(loc=\"upper right\")\n    return ax\n\n\nani = FuncAnimation(fig, update, frames=epochs // 5, interval=200, repeat=False)\n\nplt.show()</pre> Output Clear <pre><code></code></pre> <p></p> <p>O gif resultante mostra claramente como a fronteira de decis\u00e3o se ajusta ao longo do treinamento. \u00c9 poss\u00edvel observ\u00e1-la abaixo:</p> <p></p>"},{"location":"mlp/#exercicio-3-classificacao-multiclasse","title":"Exerc\u00edcio\u00a03 \u2014 Classifica\u00e7\u00e3o multiclasse","text":"<p>Neste exerc\u00edcio, o objetivo foi estender o MLP para lidar com um problema de classifica\u00e7\u00e3o multiclasse. Para isso, criei um conjunto de 1500 dados sint\u00e9tico com tr\u00eas classes e 4 features. Usei a fun\u00e7\u00e3o <code>make_classification</code> do <code>scikit-learn</code> para gerar os dados, garantindo que cada classe tivesse uma distribui\u00e7\u00e3o diferente. Para a primeria classe eu utilizei 1/3 dos dados com 2 clusters, para a segunda classe 1/3 dos dados com 3 clusters e para a terceira classe o restante dos dados com 4 clusters. Todas as features foram informativas (n_informative=4), sem atributos redundantes (n_redundant=0), e o random_state=42 garantiu a reprodutibilidade. Para essa implementa\u00e7\u00e3o, foi utilizada como base o c\u00f3digo da etapa anterior, com o m\u00ednimo de altera\u00e7\u00f5es necess\u00e1rias para adaptar o MLP \u00e0 classifica\u00e7\u00e3o multiclasse, sem mudar a estrutura geral. O c\u00f3digo completo est\u00e1 abaixo:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n\nn = 1500\nX0, _ = make_classification(\n    n_samples=n // 3,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=2,\n    n_classes=3,\n    random_state=42,\n)\ny0 = np.zeros(X0.shape[0], dtype=int)\n\nX1, _ = make_classification(\n    n_samples=n // 3,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=3,\n    n_classes=3,\n    random_state=43,\n)\ny1 = np.ones(X1.shape[0], dtype=int)\n\nX2, _ = make_classification(\n    n_samples=n - 2 * (n // 3),\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=4,\n    n_classes=3,\n    random_state=44,\n)\ny2 = np.full(X2.shape[0], 2)\n\nX = np.vstack([X0, X1, X2])\ny = np.hstack([y0, y1, y2])\n\ny_onehot = np.zeros((y.size, y.max() + 1))\ny_onehot[np.arange(y.size), y] = 1\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_onehot, test_size=0.2, random_state=42, stratify=y\n)\n\nrng = np.random.default_rng(42)\nW1 = rng.normal(0, 1, (12, 4)) / np.sqrt(4)\nb1 = np.zeros(12)\nW2 = rng.normal(0, 1, (3, 12)) / np.sqrt(12)\nb2 = np.zeros(3) # 3 sa\u00eddas\n\n\ndef tanh(x):\n    return np.tanh(x)\n\n\neta = 0.05\nepochs = 500\ny_train = y_train.astype(float)\n\nfor ep in range(1, epochs + 1):\n    # forward\n    z1 = X_train @ W1.T + b1\n    h1 = tanh(z1)\n    z2 = h1 @ W2.T + b2\n    y_pred = tanh(z2)\n\n    # loss (MSE)\n    loss = np.mean((y_train - y_pred) ** 2)\n\n    # backward\n    grad_z2 = 2 * (y_pred - y_train) * (1 - y_pred**2) / len(X_train)\n    grad_W2 = grad_z2.T @ h1\n    grad_b2 = grad_z2.sum(axis=0)\n\n    grad_h1 = grad_z2 @ W2\n    grad_z1 = grad_h1 * (1 - h1**2)\n    grad_W1 = grad_z1.T @ X_train\n    grad_b1 = grad_z1.sum(axis=0)\n\n    # update\n    W2 -= eta * grad_W2\n    b2 -= eta * grad_b2\n    W1 -= eta * grad_W1\n    b1 -= eta * grad_b1\n\n    if ep % 50 == 0 or ep == 1:\n        print(f\"\u00e9poca {ep} | loss {loss:.4f}\")\n\n# avalia\u00e7\u00e3o\nz1 = X_test @ W1.T + b1\nh1 = tanh(z1)\nz2 = h1 @ W2.T + b2\ny_pred_test = tanh(z2)\nypred = np.argmax(y_pred_test, axis=1)\nytrue = np.argmax(y_test, axis=1)\n\nacc = (ypred == ytrue).mean()\nprint(f\"\\nacur\u00e1cia teste: {(acc*100):.4f}%\")</pre> Output Clear <pre><code></code></pre> <p></p> <p>Foi poss\u00edvel notar que a acur\u00e1cia no conjunto de teste foi bem menos satisfat\u00f3ria do que no exerc\u00edcio anterior, provavelmente devido \u00e0 maior complexidade do problema. Como o problema \u00e9 mais complexo, o ideal provavelmente seria evoluir a complexidade da arquitetura de forma condizente com o problema, mas mantive a simplicidade para evita mexer demais na estrutura do c\u00f3digo utilizado na etapa anterior.</p> <p>Depois, com a ajuda do chatGPT, adicionei uma visualiza\u00e7\u00e3o animada do processo de treinamento, mostrando como as fronteiras de decis\u00e3o evolui ao longo das \u00e9pocas.</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.patches import Patch\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n\nn = 1500\n\nX0, _ = make_classification(\n    n_samples=n // 3,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=2,\n    n_classes=3,\n    random_state=42,\n)\ny0 = np.zeros(X0.shape[0], dtype=int)\n\nX1, _ = make_classification(\n    n_samples=n // 3,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=3,\n    n_classes=3,\n    random_state=43,\n)\ny1 = np.ones(X1.shape[0], dtype=int)\n\nX2, _ = make_classification(\n    n_samples=n - (n // 3) * 2,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=4,\n    n_classes=3,\n    random_state=44,\n)\ny2 = np.full(X2.shape[0], 2)\n\nX = np.vstack([X0, X1, X2])\ny = np.hstack([y0, y1, y2])\n\n# one-hot encoding\ny_onehot = np.zeros((y.size, y.max() + 1))\ny_onehot[np.arange(y.size), y] = 1\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_onehot, test_size=0.2, random_state=42, stratify=y\n)\n\n\nrng = np.random.default_rng(42)\nW1 = rng.normal(0, 1, (12, 4)) / np.sqrt(4)\nb1 = np.zeros(12)\nW2 = rng.normal(0, 1, (3, 12)) / np.sqrt(12)\nb2 = np.zeros(3)\n\n\ndef tanh(x):\n    return np.tanh(x)\n\n\neta = 0.05\nepochs = 500\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n\n\ndef forward(X_in):\n    z1 = X_in @ W1.T + b1\n    h1 = tanh(z1)\n    z2 = h1 @ W2.T + b2\n    return tanh(z2), h1\n\n\ndef update(frame):\n    global W1, b1, W2, b2\n\n    # treino por alguns steps\n    for _ in range(5):\n        y_pred, h1 = forward(X_train)\n        loss = np.mean((y_train - y_pred) ** 2)\n\n        grad_z2 = 2 * (y_pred - y_train) * (1 - y_pred**2) / len(X_train)\n        grad_W2 = grad_z2.T @ h1\n        grad_b2 = grad_z2.sum(axis=0)\n\n        grad_h1 = grad_z2 @ W2\n        grad_z1 = grad_h1 * (1 - h1**2)\n        grad_W1 = grad_z1.T @ X_train\n        grad_b1 = grad_z1.sum(axis=0)\n\n        W2 -= eta * grad_W2\n        b2 -= eta * grad_b2\n        W1 -= eta * grad_W1\n        b1 -= eta * grad_b1\n\n    # fronteira (2 primeiras features, extras zeradas)\n    grid = np.c_[\n        xx.ravel(), yy.ravel(), np.zeros_like(xx.ravel()), np.zeros_like(xx.ravel())\n    ]\n    y_grid, _ = forward(grid)\n    y_grid_class = np.argmax(y_grid, axis=1)\n\n    # predi\u00e7\u00f5es no treino\n    y_pred_train, _ = forward(X_train)\n    y_pred_class = np.argmax(y_pred_train, axis=1)\n    y_true_class = np.argmax(y_train, axis=1)\n\n    acc = (y_pred_class == y_true_class).mean() * 100\n    errors = y_pred_class != y_true_class\n    correct = ~errors\n\n    ax.clear()\n    ax.contourf(xx, yy, y_grid_class.reshape(xx.shape), alpha=0.3, cmap=plt.cm.Set1)\n\n    # pontos corretos\n    ax.scatter(\n        X_train[correct, 0],\n        X_train[correct, 1],\n        c=y_true_class[correct],\n        cmap=plt.cm.Set1,\n        edgecolors=\"k\",\n        s=30,\n        marker=\"o\",\n        label=\"Corretos\",\n    )\n\n    # pontos errados\n    ax.scatter(\n        X_train[errors, 0],\n        X_train[errors, 1],\n        c=y_true_class[errors],\n        cmap=plt.cm.Set1,\n        edgecolors=\"k\",\n        s=60,\n        marker=\"x\",\n        label=\"Errados\",\n    )\n\n    # t\u00edtulo din\u00e2mico\n    ax.set_title(f\"\u00c9poca {frame*5} | Loss={loss:.4f} | Acc={acc:.2f}%\")\n\n    # legenda de classes + corretos/errados\n    class_labels = [\n        Patch(color=plt.cm.Set1(i / 3), label=f\"Classe {i}\") for i in range(3)\n    ]\n    ax.legend(\n        handles=class_labels\n        + [\n            plt.Line2D(\n                [0],\n                [0],\n                marker=\"o\",\n                color=\"w\",\n                markerfacecolor=\"gray\",\n                markeredgecolor=\"k\",\n                label=\"Corretos\",\n                markersize=8,\n            ),\n            plt.Line2D([0], [0], marker=\"x\", color=\"k\", label=\"Errados\", markersize=8),\n        ],\n        loc=\"upper right\",\n    )\n    return ax\n\n\nani = FuncAnimation(fig, update, frames=epochs // 5, interval=200, repeat=False)\n\nani.save(\"mlp_multiclass_legendas.gif\", writer=\"pillow\")\nplt.show()</pre> Output Clear <pre><code></code></pre> <p></p> <p>O gif resultante mostra claramente como as fronteiras de decis\u00e3o se ajustam ao longo do treinamento. \u00c9 poss\u00edvel observ\u00e1-la abaixo:</p> <p></p>"},{"location":"mlp/#exercicio-4-mlp-profundo-duas-camadas-ocultas","title":"Exerc\u00edcio\u00a04 \u2014 MLP profundo (duas camadas ocultas)","text":"<p>A \u00faltima experi\u00eancia consistiu em reutilizar o conjunto de dados do exerc\u00edcio\u00a03, mas treinar um MLP mais profundo com duas camadas ocultas (optei por 32 neur\u00f4nios na primeira, 16 na segunda). A l\u00f3gica do treino permanece a mesma; a principal diferen\u00e7a \u00e9 que propagamos os gradientes por mais uma camada oculta. O c\u00f3digo completo est\u00e1 abaixo:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nn = 1500\nX0, _ = make_classification(\n    n_samples=n // 3,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=2,\n    n_classes=3,\n    random_state=42,\n)\ny0 = np.zeros(X0.shape[0], dtype=int)\n\nX1, _ = make_classification(\n    n_samples=n // 3,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=3,\n    n_classes=3,\n    random_state=43,\n)\ny1 = np.ones(X1.shape[0], dtype=int)\n\nX2, _ = make_classification(\n    n_samples=n - 2 * (n // 3),\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=4,\n    n_classes=3,\n    random_state=44,\n)\ny2 = np.full(X2.shape[0], 2)\n\nX = np.vstack([X0, X1, X2])\ny = np.hstack([y0, y1, y2])\n\ny_onehot = np.zeros((y.size, y.max() + 1))\ny_onehot[np.arange(y.size), y] = 1\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_onehot, test_size=0.2, random_state=42, stratify=y\n)\n\nrng = np.random.default_rng(42)\n\nW1 = rng.normal(0, 1, (32, 4)) / np.sqrt(4)  # input \u2192 hidden1\nb1 = np.zeros(32)\n\nW2 = rng.normal(0, 1, (16, 32)) / np.sqrt(32)  # hidden1 \u2192 hidden2\nb2 = np.zeros(16)\n\nW3 = rng.normal(0, 1, (3, 16)) / np.sqrt(16)  # hidden2 \u2192 output\nb3 = np.zeros(3)\n\n\ndef tanh(x):\n    return np.tanh(x)\n\n\neta = 0.05\nepochs = 500\ny_train = y_train.astype(float)\n\nfor ep in range(1, epochs + 1):\n    # forward\n\n    z1 = X_train @ W1.T + b1\n    h1 = tanh(z1)\n    z2 = h1 @ W2.T + b2\n    h2 = tanh(z2)\n    z3 = h2 @ W3.T + b3\n    y_pred = tanh(z3)\n\n    # loss (MSE)\n    loss = np.mean((y_train - y_pred) ** 2)\n\n    # backward\n    grad_z3 = 2 * (y_pred - y_train) * (1 - y_pred**2) / len(X_train)\n    grad_W3 = grad_z3.T @ h2\n    grad_b3 = grad_z3.sum(axis=0)\n\n    grad_h2 = grad_z3 @ W3\n    grad_z2 = grad_h2 * (1 - h2**2)\n    grad_W2 = grad_z2.T @ h1\n    grad_b2 = grad_z2.sum(axis=0)\n\n    grad_h1 = grad_z2 @ W2\n    grad_z1 = grad_h1 * (1 - h1**2)\n    grad_W1 = grad_z1.T @ X_train\n    grad_b1 = grad_z1.sum(axis=0)\n\n    W3 -= eta * grad_W3\n    b3 -= eta * grad_b3\n    W2 -= eta * grad_W2\n    b2 -= eta * grad_b2\n    W1 -= eta * grad_W1\n    b1 -= eta * grad_b1\n\n    if ep % 50 == 0 or ep == 1:\n        print(f\"\u00e9poca {ep} | loss {loss:.4f}\")\n\nz1 = X_test @ W1.T + b1\nh1 = tanh(z1)\nz2 = h1 @ W2.T + b2\nh2 = tanh(z2)\nz3 = h2 @ W3.T + b3\ny_pred_test = tanh(z3)\n\nypred = np.argmax(y_pred_test, axis=1)\nytrue = np.argmax(y_test, axis=1)\n\nacc = (ypred == ytrue).mean()\nprint(f\"\\nacur\u00e1cia teste: {(acc*100):.4f}%\")</pre> Output Clear <pre></pre> <p></p> <p>A acur\u00e1cia no conjunto de teste foi de aproximadamente 49%, o mesmo que foi visto no exerc\u00edcio passado, mas ainda longe do ideal. Com a inten\u00e7\u00e3o de melhorar a performance, testei arquiteturas ainda mais profunda, aumentando o n\u00famero de neur\u00f4nios, mudando as fun\u00e7\u00f5es da sa\u00edda e do loss, al\u00e9m de outros testes (como normalizar os dados, por exemplo), que n\u00e3o tiveram sucesso. Com isso, na vers\u00e3o em que tive mais sucesso, optei por uma arquitetura de 64 neur\u00f4nios na primeira camada oculta e 32 na segunda, com a fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code> na camada de entrada e a fun\u00e7\u00e3o <code>softmax</code> na camada de sa\u00edda. A fun\u00e7\u00e3o de perda foi alterada para a entropia cruzada (cross-entropy loss). O c\u00f3digo completo est\u00e1 abaixo:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nn = 1500\nX0, _ = make_classification(\n    n_samples=n // 3,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=2,\n    n_classes=3,\n    random_state=42,\n)\ny0 = np.zeros(X0.shape[0], dtype=int)\n\nX1, _ = make_classification(\n    n_samples=n // 3,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=3,\n    n_classes=3,\n    random_state=43,\n)\ny1 = np.ones(X1.shape[0], dtype=int)\n\nX2, _ = make_classification(\n    n_samples=n - 2 * (n // 3),\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_clusters_per_class=4,\n    n_classes=3,\n    random_state=44,\n)\ny2 = np.full(X2.shape[0], 2)\n\nX = np.vstack([X0, X1, X2])\ny = np.hstack([y0, y1, y2])\n\ny_onehot = np.zeros((y.size, y.max() + 1))\ny_onehot[np.arange(y.size), y] = 1\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_onehot, test_size=0.2, random_state=42, stratify=y\n)\n\nrng = np.random.default_rng(42)\n\nW1 = rng.normal(0, 1, (64, 4)) / np.sqrt(4)\nb1 = np.zeros(64)\n\nW2 = rng.normal(0, 1, (32, 64)) / np.sqrt(64)\nb2 = np.zeros(32)\n\nW3 = rng.normal(0, 1, (3, 32)) / np.sqrt(32)\nb3 = np.zeros(3)\n\n\ndef tanh(x):\n    return np.tanh(x)\n\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n\n\ndef cross_entropy(y_true, y_pred):\n    eps = 1e-9\n    return -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))\n\n\neta = 0.05\nepochs = 1000\ny_train = y_train.astype(float)\n\nfor ep in range(1, epochs + 1):\n    # forward\n    z1 = X_train @ W1.T + b1\n    h1 = tanh(z1)\n\n    z2 = h1 @ W2.T + b2\n    h2 = tanh(z2)\n\n    z3 = h2 @ W3.T + b3\n    y_pred = softmax(z3)\n\n    # loss\n    loss = cross_entropy(y_train, y_pred)\n\n    # backward\n    grad_z3 = (y_pred - y_train) / len(X_train)\n    grad_W3 = grad_z3.T @ h2\n    grad_b3 = grad_z3.sum(axis=0)\n\n    grad_h2 = grad_z3 @ W3\n    grad_z2 = grad_h2 * (1 - h2**2)\n    grad_W2 = grad_z2.T @ h1\n    grad_b2 = grad_z2.sum(axis=0)\n\n    grad_h1 = grad_z2 @ W2\n    grad_z1 = grad_h1 * (1 - h1**2)\n    grad_W1 = grad_z1.T @ X_train\n    grad_b1 = grad_z1.sum(axis=0)\n\n    # updates\n    W3 -= eta * grad_W3\n    b3 -= eta * grad_b3\n    W2 -= eta * grad_W2\n    b2 -= eta * grad_b2\n    W1 -= eta * grad_W1\n    b1 -= eta * grad_b1\n\n    if ep % 100 == 0 or ep == 1:\n        print(f\"\u00e9poca {ep} | loss {loss:.4f}\")\n\nz1 = X_test @ W1.T + b1\nh1 = tanh(z1)\n\nz2 = h1 @ W2.T + b2\nh2 = tanh(z2)\n\nz3 = h2 @ W3.T + b3\ny_pred_test = softmax(z3)\n\nypred = np.argmax(y_pred_test, axis=1)\nytrue = np.argmax(y_test, axis=1)\n\nacc = (ypred == ytrue).mean()\nprint(f\"\\nacur\u00e1cia teste: {(acc*100):.4f}%\")</pre> Output Clear <pre><code></code></pre> <p></p> <p>Com essa configura\u00e7\u00e3o, a acur\u00e1cia no conjunto de teste melhorou para aproximadamente 56%, indicando que a arquitetura mais profunda e as mudan\u00e7as na fun\u00e7\u00e3o de ativa\u00e7\u00e3o e perda ajudaram a capturar melhor a complexidade do problema. Apesar de tudo, a acur\u00e1cia ainda est\u00e1 longe do ideal, sugerindo que o modelo pode se beneficiar de mais ajustes.</p> <p>A explica\u00e7\u00e3o pela qual optei por essas altera\u00e7\u00f5es foram as seguintes:</p> <ul> <li> <p>Softmax: usei essa fun\u00e7\u00e3o de ativa\u00e7\u00e3o na sa\u00edda porque ela transforma os valores finais da rede em probabilidades normalizadas, permitindo interpretar facilmente qual classe tem maior chance de ser correta.</p> </li> <li> <p>Cross-Entropy: escolhi essa fun\u00e7\u00e3o para o loss pois ela mede a diferen\u00e7a entre as probabilidades previstas e as reais, penalizando mais fortemente previs\u00f5es incorretas e sendo mais adequada que o MSE em classifica\u00e7\u00e3o multi-classe.</p> </li> <li> <p>Aumento da arquitetura (64 \u2192 32): deu mais capacidade de aprendizado \u00e0 rede, permitindo representar padr\u00f5es mais complexos do conjunto de dados.</p> </li> <li> <p>Mais \u00e9pocas (500 \u2192 1000): ofereceu mais tempo de treino, reduzindo o underfitting e possibilitando que a rede convergisse melhor.</p> </li> </ul>"},{"location":"mlp/#referencias","title":"Refer\u00eancias","text":"<ul> <li>Notas da disciplina \u2013 Para realizar as etapas foram baseadas no material do curso.</li> <li>scikit\u2011learn \u2013 Fun\u00e7\u00e3o <code>make_classification</code> para gerar dados sint\u00e9ticos com diferentes clusters.</li> <li>ChatGPT \u2013 Assistente usado para apoio no arranjo do relat\u00f3rio e debug de erros de programa\u00e7\u00e3o. Al\u00e9m disso, ajudou na cria\u00e7\u00e3o das visualiza\u00e7\u00f5es animadas e na refatora\u00e7\u00e3o de algumas partes do c\u00f3digo. Me ajudou tamb\u00e9m com sugest\u00f5es de melhorias na arquitetura do MLP final para que eu pudesse alcan\u00e7ar uma acur\u00e1cia melhor.</li> </ul>"},{"location":"perceptron_report_template/","title":"Implementation of a Perceptron","text":""},{"location":"perceptron_report_template/#activity-understanding-perceptrons-and-their-limitations","title":"Activity: Understanding Perceptrons and Their Limitations","text":"Edi\u00e7\u00e3o <p>2025.2</p>"},{"location":"perceptron_report_template/#exercise-1-linearly-separable-data","title":"Exercise 1 \u2014 Linearly Separable Data","text":""},{"location":"perceptron_report_template/#data-generation","title":"Data Generation","text":"<p>Generated 2D dataset with 1000 samples per class using Gaussian distributions:</p> <ul> <li>Class 0: Mean = [1.5, 1.5], Covariance = [[0.5, 0], [0, 0.5]]</li> <li>Class 1: Mean = [5, 5], Covariance = [[0.5, 0], [0, 0.5]]</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nmean_A = np.array([1.5, 1.5])\n\nSigma_A = np.array(\n    [\n        [0.5, 0],\n        [0, 0.5],\n    ]\n)\n\nmean_B = np.array([5, 5])\n\nSigma_B = np.array(\n    [\n        [0.5, 0],\n        [0, 0.5],\n    ]\n)\n\nptsA = np.random.multivariate_normal(mean_A, Sigma_A, 1000)\nptsB = np.random.multivariate_normal(mean_B, Sigma_B, 1000)\n\nplt.scatter(ptsA[:, 0], ptsA[:, 1], alpha=0.6, label=\"Class A\", color=\"blue\")\nplt.scatter(ptsB[:, 0], ptsB[:, 1], alpha=0.6, label=\"Class B\", color=\"red\")\nplt.legend()\nplt.title(\"Generated Data \u2014 Exercise 1\")\nplt.show()\nplt.close()\n</code></pre> <p>Visualization: </p>"},{"location":"perceptron_report_template/#perceptron-implementation","title":"Perceptron Implementation","text":"<p>Implemented perceptron from scratch using the data generated above:</p> <ul> <li>Initialized weights <code>w</code> and bias <code>b</code>.</li> <li>Update rule: \\(w = w + \u03b7 * (y - \\hat{y}) * x\\), \\(b = b + \u03b7 * (y - \\hat{y})\\).</li> <li>Learning rate \\(\u03b7 = 0.01\\).</li> <li>Trained until convergence or 100 epochs.</li> </ul> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\nnp.random.seed(42)\n\nmean_A = [1.5, 1.5]\nSigma_A = [[0.5, 0], [0, 0.5]]\nmean_B = [5, 5]\nSigma_B = [[0.5, 0], [0, 0.5]]\n\nptsA = np.random.multivariate_normal(mean_A, Sigma_A, 1000)\nptsB = np.random.multivariate_normal(mean_B, Sigma_B, 1000)\n\ndef activation_function(z):\n    return 1 if z &gt;= 0 else 0\n\nYa = np.zeros(len(ptsA), dtype=int)\nYb = np.ones(len(ptsB), dtype=int)\n\nX = np.vstack([ptsA, ptsB])\nY = np.hstack([Ya, Yb])\n\nw = np.array([1, 0])\nb = 0\neta = 0.1\nmax_epochs = 100\niter = 1\n\nfor epoch in range(max_epochs):\n    errors = 0\n    for i, x in enumerate(X):\n        y_pred = activation_function(float(w @ x + b))\n        erro = Y[i] - y_pred\n        if erro != 0:\n            w = w + eta * erro * x\n            b = b + eta * erro\n            errors += 1\n    if errors == 0:\n        break\n    iter += 1\n\nprint(\"Treinamento conclu\u00eddo.\")\nprint(\"Resultados finais:\")\nprint(f\"\u00c9pocas percorridas: {iter} | w final = {w} | b final = {b:.4f}\")</pre> Output Clear <pre><code></code></pre> <p></p> <p>From the training, recorded weights, bias, and accuracy per epoch, I was able to visualize the decision boundary and the convergence process in an animation. The animation can be seen below:</p>"},{"location":"perceptron_report_template/#_1","title":"Implementation of a Perceptron","text":"<p>The perceptron accuracy per epoch was also tracked and plotted. The accuracy reached 100% quickly due to the linear separability of the data. Below is the accuracy plot:</p>"},{"location":"perceptron_report_template/#_2","title":"Implementation of a Perceptron","text":""},{"location":"perceptron_report_template/#results","title":"Results","text":"<ul> <li>Epochs Passed: 11</li> <li>Final Weights: w = [0.27536592 0.16377714]</li> <li>Final Bias: b = -1.5</li> <li>Accuracy: 100%</li> </ul> <p>Analysis: The data is linearly separable, with two well-defined clusters and little overlap. Because a single straight line can clearly divide the classes, the perceptron converges quickly, requiring only a few updates to adjust its weights and reach perfect classification.</p>"},{"location":"perceptron_report_template/#exercise-2-overlapping-classes","title":"Exercise 2 \u2014 Overlapping Classes","text":""},{"location":"perceptron_report_template/#data-generation_1","title":"Data Generation","text":"<p>Generated 2D dataset with partial overlap:</p> <ul> <li>Class 0: Mean = [3, 3], Covariance = [[1.5, 0], [0, 1.5]]</li> <li>Class 1: Mean = [4, 4], Covariance = [[1.5, 0], [0, 1.5]]</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nmean_A = np.array([3, 3])\n\nSigma_A = np.array(\n    [\n        [1.5, 0],\n        [0, 1.5],\n    ]\n)\n\nmean_B = np.array([4, 4])\n\nSigma_B = np.array(\n    [\n        [1.5, 0],\n        [0, 1.5],\n    ]\n)\n\nptsA = np.random.multivariate_normal(mean_A, Sigma_A, 1000)\nptsB = np.random.multivariate_normal(mean_B, Sigma_B, 1000)\n\nplt.scatter(ptsA[:, 0], ptsA[:, 1], alpha=0.6, label=\"Class A\", color=\"blue\")\nplt.scatter(ptsB[:, 0], ptsB[:, 1], alpha=0.6, label=\"Class B\", color=\"red\")\nplt.legend()\nplt.title(\"Generated Data \u2014 Exercise 2\")\nplt.show()\nplt.close()\n</code></pre> <p>Visualization: </p>"},{"location":"perceptron_report_template/#perceptron-implementation_1","title":"Perceptron Implementation","text":"<p>To this second test, I followed the same training process.</p> <ul> <li>Maximum 100 epochs.</li> <li>Observed possible oscillation due to overlap.</li> <li>Recorded best accuracy over multiple runs.</li> </ul> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\nnp.random.seed(42)\n\nmean_A = np.array([3, 3])\n\nSigma_A = np.array(\n    [\n        [1.5, 0],\n        [0, 1.5],\n    ]\n)\n\nmean_B = np.array([4, 4])\n\nSigma_B = np.array(\n    [\n        [1.5, 0],\n        [0, 1.5],\n    ]\n)\n\nptsA = np.random.multivariate_normal(mean_A, Sigma_A, 1000)\nptsB = np.random.multivariate_normal(mean_B, Sigma_B, 1000)\n\nptsA = np.random.multivariate_normal(mean_A, Sigma_A, 1000)\nptsB = np.random.multivariate_normal(mean_B, Sigma_B, 1000)\n\ndef activation_function(x):\n    return 1 if x &gt;= 0 else 0\n\nYa = np.zeros(len(ptsA), dtype=int)\nYb = np.ones(len(ptsB), dtype=int)\n\nX = np.vstack([ptsA, ptsB])\nY = np.hstack([Ya, Yb])\n\nw = np.array([1, 0])\nb = 0\neta = 0.1\nmax_epochs = 100\nhist = [(w.copy(), b)]\n\nfor epoch in range(max_epochs):\n    errors = 0\n    for i, x in enumerate(X):\n        y_pred = activation_function(float(w @ x + b))\n        erro = Y[i] - y_pred\n        if erro != 0:\n            w = w + eta * erro * x\n            b = b + eta * erro\n            errors += 1\n    hist.append((w.copy(), b))\n\n    if errors == 0:\n        break\n\nprint(\"Treinamento conclu\u00eddo.\")\nprint(\"Resultados finais:\")\nprint(f\"\u00c9pocas percorridas: {len(hist)-1} | w final = {w} | b final = {b:.4f}\")</pre> Output Clear <pre></pre> <p></p> <p>Similar to Exercise 1, I visualized the decision boundary and convergence process in an animation. The animation can be seen below:</p> <p></p> <p>The perceptron accuracy per epoch was also tracked and plotted. The accuracy never quite reached a stable accuracy due to the overlap in the data. Below is the accuracy plot:</p> <p></p>"},{"location":"perceptron_report_template/#results_1","title":"Results","text":"<ul> <li>Epochs Passed: 100</li> <li>Final Weights: [0.36998284 0.25593958]</li> <li>Final Bias: -0.2000</li> <li>Mean Accuracy: 50.21%</li> </ul> <p>As a convergence was not achieved, I ran the training 5 times and recorded the mean accuracy obtained.</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef activation_function(x):\n    return 1 if x &gt;= 0 else 0\n\neta = 0.1\nmax_epochs = 100\n\nfinal_accuracies = []\n\nfor run in range(5):\n    mean_A = np.array([3, 3])\n    Sigma_A = np.array([[1.5, 0], [0, 1.5]])\n    mean_B = np.array([4, 4])\n    Sigma_B = np.array([[1.5, 0], [0, 1.5]])\n\n    ptsA = np.random.multivariate_normal(mean_A, Sigma_A, 1000)\n    ptsB = np.random.multivariate_normal(mean_B, Sigma_B, 1000)\n\n    Ya = np.zeros(len(ptsA), dtype=int)\n    Yb = np.ones(len(ptsB), dtype=int)\n\n    X = np.vstack([ptsA, ptsB])\n    Y = np.hstack([Ya, Yb])\n\n    w = np.array([1.0, 0.0])\n    b = 0.0\n    hist = [(w.copy(), b)]\n\n    for epoch in range(max_epochs):\n        errors = 0\n        for i, x in enumerate(X):\n            y_pred = activation_function(float(w @ x + b))\n            erro = Y[i] - y_pred\n            if erro != 0:\n                w = w + eta * erro * x\n                b = b + eta * erro\n                errors += 1\n        hist.append((w.copy(), b))\n\n        if errors == 0:\n            break\n\n    y_pred_final = (X @ w + b &gt;= 0).astype(int)\n    acc = (y_pred_final == Y).mean()\n    final_accuracies.append(acc)\n\n    print(f\"Run {run+1}: epochs={len(hist)-1}, accuracy={acc:.3f}\")\n\nprint(\"\\nAccuracies finais das 5 execu\u00e7\u00f5es:\", final_accuracies)\nprint(\"M\u00e9dia:\", np.mean(final_accuracies))</pre> Output Clear <pre></pre> <p></p> <p>Analysis: The heavy overlap between the two classes prevents the perceptron from finding a clear linear separator. As a result, the training accuracy fluctuates around 50%, as shown in the plot and by those 5 executions above. The continuous misclassifications in the overlapping region force repeated weight updates, which cause oscillation without convergence. This highlights the limitation of perceptrons: they cannot achieve meaningful accuracy when data is not linearly separable.</p>"},{"location":"perceptron_report_template/#references","title":"References","text":"<ul> <li>Numpy Documentation - multivariate_normal</li> <li>ANN-DL Course - Perceptron</li> <li>Report (Text correction and adaptation to english) and perceptron animation support generated with assistance from ChatGPT.</li> </ul>"},{"location":"projetos/classification_project/","title":"Project 1 - Classification","text":""},{"location":"projetos/classification_project/#classificacao-com-mlp","title":"Classifica\u00e7\u00e3o com MLP","text":""},{"location":"projetos/classification_project/#grupo","title":"Grupo","text":"<ol> <li>Eduardo Selber Castanho</li> <li>Henrique Fazzio Badin</li> <li>Lucas Fernando de Souza Lima</li> </ol> <p>O reposit\u00f3rio em que fizemos o projeto \u00e9 o seginte: Reposit\u00f3rio do projeto. Estou fazendo a jun\u00e7\u00e3o desse relat\u00f3rio com o meu reposit\u00f3rio pessoal da mat\u00e9ria no GitHub.</p> <p>Arquivo Jupyter implementando o MLP aqui.</p>"},{"location":"projetos/classification_project/#1-dataset-overview","title":"1. Dataset Overview","text":"<p>Dataset: Predict Students' Dropout and Academic Success Fonte: UCI Machine Learning Repository Dom\u00ednio: Educa\u00e7\u00e3o / Ci\u00eancias Sociais Tipo de dados: Tabular (num\u00e9ricos, categ\u00f3ricos e inteiros) Tarefa: Classifica\u00e7\u00e3o multiclasse (3 classes: dropout, enrolled, graduate) Amostras: 4.424 Features: 36 Descri\u00e7\u00e3o: Dados de estudantes de gradua\u00e7\u00e3o em diversas \u00e1reas, contendo informa\u00e7\u00f5es demogr\u00e1ficas, socioecon\u00f4micas e de desempenho acad\u00eamico nos dois primeiros semestres. O objetivo \u00e9 prever a situa\u00e7\u00e3o final do aluno \u2014 evas\u00e3o, matr\u00edcula ativa ou gradua\u00e7\u00e3o.</p> <p>Motiva\u00e7\u00e3o: O Predict Students' Dropout and Academic Success foi escolhido por ser tamb\u00e9m utilizado em uma competi\u00e7\u00e3o no Kaggle, o que refor\u00e7a sua relev\u00e2ncia e permite compara\u00e7\u00f5es de desempenho entre diferentes abordagens de classifica\u00e7\u00e3o.</p> <p>Al\u00e9m disso, trata-se de um tema socialmente importante, pois os resultados obtidos podem ser aplicados em escolas, universidades e institui\u00e7\u00f5es educacionais para identificar precocemente alunos com risco de evas\u00e3o e orientar estrat\u00e9gias de apoio acad\u00eamico.</p> <p>O dataset \u00e9 robusto (mais de 4.000 inst\u00e2ncias e 36 vari\u00e1veis) e, ao mesmo tempo, bem estruturado, com dados limpos e sem valores ausentes ou duplicados, al\u00e9m de descri\u00e7\u00f5es claras das features, o que facilita o desenvolvimento e an\u00e1lise de modelos de aprendizado de m\u00e1quina.</p>"},{"location":"projetos/classification_project/#2-dataset-explanation","title":"2. Dataset Explanation","text":"<p>O conjunto de dados Predict Students\u2019 Dropout and Academic Success cont\u00e9m informa\u00e7\u00f5es acad\u00eamicas, demogr\u00e1ficas e socioecon\u00f4micas de estudantes de cursos de gradua\u00e7\u00e3o, coletadas no momento da matr\u00edcula e ao final dos dois primeiros semestres. O objetivo \u00e9 prever o status final do aluno \u2014 dropout, enrolled ou graduate \u2014 configurando um problema de classifica\u00e7\u00e3o multiclasse.</p> <p>O dataset possui 36 vari\u00e1veis e 4.424 inst\u00e2ncias, todas sem valores nulos ou duplicados. As features incluem dados pessoais, hist\u00f3rico acad\u00eamico, desempenho em disciplinas e indicadores macroecon\u00f4micos.</p>"},{"location":"projetos/classification_project/#21-feature-description","title":"2.1 Feature Description","text":"Feature Tipo Descri\u00e7\u00e3o Valores / Intervalos Marital Status Integer Estado civil 1\u2013single, 2\u2013married, 3\u2013widower, 4\u2013divorced, 5\u2013facto union, 6\u2013legally separated Application mode Integer Tipo de candidatura ao curso 1\u2013general, 5\u2013special contingent, 17\u20132nd phase, etc. Application order Integer Ordem de prefer\u00eancia da candidatura 0\u20139 Course Integer Curso matriculado V\u00e1rios c\u00f3digos (ex.: 9147\u2013Management, 9500\u2013Nursing) Daytime/evening attendance Integer Turno 1\u2013daytime, 0\u2013evening Previous qualification Integer N\u00edvel de escolaridade anterior 1\u2013secondary, 2\u2013higher ed., etc. Previous qualification (grade) Continuous Nota da qualifica\u00e7\u00e3o anterior 0\u2013200 Nationality Integer Nacionalidade Ex.: 1\u2013Portuguese, 41\u2013Brazilian, 21\u2013Angolan Mother\u2019s qualification Integer Escolaridade da m\u00e3e 1\u2013secondary, 5\u2013doctorate, etc. Father\u2019s qualification Integer Escolaridade do pai 1\u2013secondary, 5\u2013doctorate, etc. Mother\u2019s occupation Integer Ocupa\u00e7\u00e3o da m\u00e3e 0\u2013student, 1\u2013manager, 5\u2013service worker, etc. Father\u2019s occupation Integer Ocupa\u00e7\u00e3o do pai 0\u2013student, 1\u2013manager, 8\u2013technician, etc. Admission grade Continuous Nota de admiss\u00e3o no curso 0\u2013200 Displaced Integer Mora fora da cidade de origem 1\u2013yes, 0\u2013no Educational special needs Integer Necessidades educacionais especiais 1\u2013yes, 0\u2013no Debtor Integer Est\u00e1 em d\u00e9bito financeiro 1\u2013yes, 0\u2013no Tuition fees up to date Integer Mensalidades em dia 1\u2013yes, 0\u2013no Gender Integer Sexo do estudante 1\u2013male, 0\u2013female Scholarship holder Integer Possui bolsa de estudos 1\u2013yes, 0\u2013no Age at enrollment Integer Idade no momento da matr\u00edcula anos International Integer Estudante internacional 1\u2013yes, 0\u2013no Curricular units 1st sem (credited) Integer Disciplinas creditadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (enrolled) Integer Disciplinas matriculadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (evaluations) Integer Avalia\u00e7\u00f5es realizadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (approved) Integer Disciplinas aprovadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (grade) Integer M\u00e9dia de notas (1\u00ba semestre) 0\u201320 Curricular units 1st sem (without evaluations) Integer Disciplinas sem avalia\u00e7\u00e3o (1\u00ba semestre) 0\u2013n Curricular units 2nd sem (credited) Integer Disciplinas creditadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (enrolled) Integer Disciplinas matriculadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (evaluations) Integer Avalia\u00e7\u00f5es realizadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (approved) Integer Disciplinas aprovadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (grade) Integer M\u00e9dia de notas (2\u00ba semestre) 0\u201320 Curricular units 2nd sem (without evaluations) Integer Disciplinas sem avalia\u00e7\u00e3o (2\u00ba semestre) 0\u2013n Unemployment rate Continuous Taxa de desemprego (%) valor cont\u00ednuo Inflation rate Continuous Taxa de infla\u00e7\u00e3o (%) valor cont\u00ednuo GDP Continuous Produto Interno Bruto valor cont\u00ednuo Target Categorical Situa\u00e7\u00e3o final do aluno Dropout, Enrolled, Graduate"},{"location":"projetos/classification_project/#22-target-variable","title":"2.2 Target Variable","text":"<p>A vari\u00e1vel Target representa a situa\u00e7\u00e3o final do estudante ao t\u00e9rmino do curso:</p> <ul> <li>Dropout (0): o aluno abandonou o curso</li> <li>Enrolled (1): o aluno ainda est\u00e1 matriculado</li> <li>Graduate (2): o aluno concluiu o curso com sucesso</li> </ul>"},{"location":"projetos/classification_project/#23-data-issues","title":"2.3 Data Issues","text":"<p>O dataset j\u00e1 foi pr\u00e9-processado e limpo pelos autores originais, n\u00e3o apresentando:</p> <ul> <li>Valores ausentes (<code>NaN</code>);</li> <li>Duplicatas;</li> <li>Outliers sem explica\u00e7\u00e3o evidente.</li> </ul>"},{"location":"projetos/classification_project/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization","text":"<p>A etapa de limpeza e normaliza\u00e7\u00e3o foi realizada para garantir a qualidade dos dados e preparar o conjunto para o treinamento da rede MLP. O processo incluiu inspe\u00e7\u00e3o inicial, remo\u00e7\u00e3o de outliers, codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas, normaliza\u00e7\u00e3o num\u00e9rica e an\u00e1lise explorat\u00f3ria com PCA (Principal Component Analysis).</p>"},{"location":"projetos/classification_project/#31-initial-inspection","title":"3.1 Initial Inspection","text":"<p>Ap\u00f3s o carregamento do dataset (<code>train.csv</code>), foi feita uma an\u00e1lise explorat\u00f3ria inicial com <code>pandas</code> e <code>matplotlib</code> para verificar estrutura, tipos de dados e poss\u00edveis problemas de consist\u00eancia.</p> <pre><code>df.info()\ndf.isnull().sum()\ndf.describe().transpose()\ndf.hist(bins=30, figsize=(20, 15))\n</code></pre> <ul> <li>Nenhum valor nulo foi encontrado.</li> <li>N\u00e3o h\u00e1 colunas duplicadas.</li> <li>As distribui\u00e7\u00f5es mostram amplitudes diferentes entre vari\u00e1veis, o que justifica a posterior normaliza\u00e7\u00e3o.</li> </ul> <p>Figura 1 \u2014 Distribui\u00e7\u00e3o inicial das features</p> <p></p>"},{"location":"projetos/classification_project/#32-outlier-detection-and-removal","title":"3.2 Outlier Detection and Removal","text":"<p>Foram definidos limites manuais de plausibilidade (bounds) para cada vari\u00e1vel quantitativa, baseados em conhecimento de dom\u00ednio e na distribui\u00e7\u00e3o dos dados. A fun\u00e7\u00e3o <code>remove_outliers_by_bounds()</code> filtrou linhas fora desses intervalos.</p> <pre><code>df_sem_outliers = remove_outliers_by_bounds(df)\n</code></pre>"},{"location":"projetos/classification_project/#principais-resultados","title":"Principais resultados:","text":"Vari\u00e1vel Intervalo Mantido Linhas Removidas Previous qualification (grade) [80, 180] 13 Admission grade [90, 180] 16 Age at enrollment [16, 60] 33 Curricular units 1st sem (approved) [0, 15] 108 Curricular units 1st sem (without evaluations) [0, 4] 127 Curricular units 2nd sem (without evaluations) [0, 1] 1.011 Inflation rate [-1.5, 3.5] 7.327 GDP [-4.5, 3.5] 5.082 <p>Resumo final: 76.518 \u2192 62.502 linhas ap\u00f3s limpeza.</p> <p>Figura 2 \u2014 Distribui\u00e7\u00e3o ap\u00f3s remo\u00e7\u00e3o de outliers</p> <p></p> <p>A filtragem removeu registros com valores extremos (principalmente em indicadores macroecon\u00f4micos e notas m\u00e9dias), reduzindo ru\u00eddos sem comprometer o volume de dados.</p>"},{"location":"projetos/classification_project/#33-feature-encoding","title":"3.3 Feature Encoding","text":"<p>Para transformar vari\u00e1veis categ\u00f3ricas em num\u00e9ricas, foi aplicada codifica\u00e7\u00e3o one-hot com <code>pandas.get_dummies()</code>, preservando todas as categorias:</p> <pre><code>df_encoded = pd.get_dummies(df, columns=[\n    \"Nacionality\", \"Marital status\", \"Application mode\", \"Course\",\n    \"Previous qualification\", \"Mother's qualification\", \"Father's qualification\",\n    \"Mother's occupation\", \"Father's occupation\"\n])\n</code></pre> <ul> <li>As novas colunas (<code>uint8</code>) representaram corretamente cada categoria.</li> <li> <p>O <code>Target</code> foi mapeado para valores num\u00e9ricos:</p> </li> <li> <p><code>Graduate = 1.0</code></p> </li> <li><code>Dropout = 0.5</code></li> <li><code>Enrolled = 0.0</code></li> </ul>"},{"location":"projetos/classification_project/#34-normalization","title":"3.4 Normalization","text":"<p>Como as features possuem escalas diferentes (por exemplo, notas entre 0\u2013200 e idades entre 16\u201360), aplicou-se a normaliza\u00e7\u00e3o Min\u2013Max, escalando todos os valores para o intervalo [0, 1]:</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_scaled[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n</code></pre> <p>Essa transforma\u00e7\u00e3o assegura que todas as vari\u00e1veis tenham o mesmo peso relativo no c\u00e1lculo dos gradientes durante o treinamento do MLP.</p>"},{"location":"projetos/classification_project/#35-dimensionality-reduction-pca","title":"3.5 Dimensionality Reduction (PCA)","text":"<p>Devido ao grande n\u00famero de vari\u00e1veis presentes no dataset \u2014 ap\u00f3s o one-hot encoding havia centenas de features num\u00e9ricas \u2014 aplicamos uma An\u00e1lise de Componentes Principais (PCA) com o objetivo de reduzir a dimensionalidade dos dados e facilitar a visualiza\u00e7\u00e3o da separabilidade entre as classes.</p> <p>A redu\u00e7\u00e3o de dimensionalidade \u00e9 \u00fatil neste caso porque:</p> <ul> <li>Diminui a complexidade computacional para an\u00e1lises explorat\u00f3rias.</li> <li>Permite observar padr\u00f5es e agrupamentos de classes em um espa\u00e7o bidimensional.</li> <li>Ajuda a verificar se h\u00e1 algum grau de separa\u00e7\u00e3o natural entre as classes antes do treinamento do MLP.</li> </ul> <p>O PCA foi aplicado sobre as colunas num\u00e9ricas normalizadas, extraindo os dois primeiros componentes principais:</p> <pre><code>from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca_result = pca.fit_transform(df_scaled[numeric_cols])\n</code></pre> <p>Os resultados mostraram que:</p> <ul> <li>PC1: explica aproximadamente 34% da vari\u00e2ncia total</li> <li>PC2: explica aproximadamente 21% da vari\u00e2ncia</li> <li>Total: cerca de 55% da vari\u00e2ncia foi capturada pelos dois primeiros componentes</li> </ul> <p>Isso indica que, embora a maior parte da informa\u00e7\u00e3o ainda esteja distribu\u00edda entre diversas dimens\u00f5es, \u00e9 poss\u00edvel visualizar parte da estrutura dos dados em duas dimens\u00f5es.</p> <p>A figura abaixo mostra a proje\u00e7\u00e3o dos dados no plano formado pelos dois primeiros componentes principais, com as classes representadas por cores diferentes.</p> <p>Figura 3 \u2014 PCA (PC1 vs PC2) colorido por classe</p> <p></p>"},{"location":"projetos/classification_project/#36-summary","title":"3.6 Summary","text":"<ul> <li>Nenhum dado ausente ou duplicado encontrado.</li> <li>Outliers removidos com base em limites definidos manualmente.</li> <li>Vari\u00e1veis categ\u00f3ricas convertidas por one-hot encoding.</li> <li>Normaliza\u00e7\u00e3o Min\u2013Max aplicada a todas as vari\u00e1veis num\u00e9ricas.</li> </ul>"},{"location":"projetos/classification_project/#4-mlp-implementation","title":"4. MLP Implementation","text":"<p>Para a implementa\u00e7\u00e3o do MLP consideramos decis\u00f5es de design sobre arquitetura, fun\u00e7\u00f5es de ativa\u00e7\u00e3o, fun\u00e7\u00e3o de perda, inicializa\u00e7\u00e3o de pesos e hiperpar\u00e2metros. A implementa\u00e7\u00e3o foi feita em NumPy para refor\u00e7ar a compreens\u00e3o das opera\u00e7\u00f5es fundamentais (forward, loss, backprop e atualiza\u00e7\u00e3o de pesos). Em seguida detalhamos cada componente.</p>"},{"location":"projetos/classification_project/#41-network-architecture","title":"4.1 Network Architecture","text":"<p>Optou-se por um MLP com duas camadas ocultas:</p> <ul> <li>Input: dimens\u00e3o <code>D</code> (n\u00famero de features ap\u00f3s one-hot + normaliza\u00e7\u00e3o).</li> <li>Hidden layer 1: <code>H1 = 64</code> neur\u00f4nios, ativa\u00e7\u00e3o <code>tanh</code>.</li> <li>Hidden layer 2: <code>H2 = 32</code> neur\u00f4nios, ativa\u00e7\u00e3o <code>tanh</code>.</li> <li>Output: <code>K</code> neur\u00f4nios (n\u00famero de classes), ativa\u00e7\u00e3o <code>softmax</code>.</li> </ul> <p>Justificativa curta:</p> <ul> <li>Duas camadas ocultas capturam n\u00e3o linearidades mais complexas que uma \u00fanica camada sem tornar a rede excessivamente profunda.</li> <li>H1/H2 escolhidos por experimenta\u00e7\u00e3o emp\u00edrica: aumentos significativos na largura n\u00e3o trouxeram ganhos relevantes, apenas custo computacional.</li> </ul>"},{"location":"projetos/classification_project/#42-activation-functions","title":"4.2 Activation Functions","text":"<ul> <li> <p>Camadas ocultas: <code>tanh</code></p> </li> <li> <p>Vantagem: sa\u00edda centrada em zero, derivada simples (<code>1 - tanh^2</code>) e bom comportamento com inicializa\u00e7\u00f5es escalonadas.</p> </li> <li> <p>Observa\u00e7\u00e3o: <code>ReLU</code> pode acelerar converg\u00eancia, mas neste trabalho mantivemos <code>tanh</code> por estabilidade e por corresponder ao material do curso.</p> </li> <li> <p>Camada de sa\u00edda: <code>softmax</code></p> </li> <li> <p>Produz probabilidades normalizadas por amostra, adequada para cross-entropy multiclasses.</p> </li> </ul>"},{"location":"projetos/classification_project/#43-loss-function","title":"4.3 Loss Function","text":"<ul> <li>Cross-entropy (categorical) combinada com regulariza\u00e7\u00e3o L2 aplicada a todos os pesos:</li> </ul> \\[ \\text{loss} = -\\frac{1}{B}\\sum_{i=1}^{B}\\sum_{k=1}^{K} y_{ik}\\log(\\hat{y}_{ik} + \\varepsilon) + \\lambda \\sum_{\\ell} \\|W^{(\\ell)}\\|^2 \\] <ul> <li><code>B</code> \u00e9 o tamanho do mini-batch.</li> <li> <p>A combina\u00e7\u00e3o cross-entropy + softmax simplifica a derivada do output para <code>(yhat - y_onehot)/B</code>, o que facilita o c\u00e1lculo de gradientes.</p> </li> <li> <p>Motiva\u00e7\u00e3o: cross-entropy penaliza previs\u00f5es confiantes e erradas de forma mais forte que MSE, sendo padr\u00e3o em classifica\u00e7\u00e3o probabil\u00edstica.</p> </li> </ul>"},{"location":"projetos/classification_project/#44-hyperparameters","title":"4.4 Hyperparameters","text":"<p>Hiperpar\u00e2metros selecionados (valores usados nos experimentos):</p> <ul> <li> <p>Learning Rate (lr): <code>0.005</code></p> </li> <li> <p>Testes com 0.001, 0.005 e 0.01 indicaram que 0.005 entregou bom equil\u00edbrio entre velocidade e estabilidade com <code>tanh</code>.</p> </li> <li> <p>Epochs (m\u00e1x): <code>500</code></p> </li> <li> <p>Limite superior; o treinamento \u00e9 interrompido via early stopping quando a valida\u00e7\u00e3o n\u00e3o melhora.</p> </li> <li> <p>Batch size: <code>64</code></p> </li> <li> <p>Atualiza\u00e7\u00e3o mais frequente que <code>128</code>, com bom trade-off entre estabilidade do gradiente e varia\u00e7\u00e3o estoc\u00e1stica \u00fatil.</p> </li> <li> <p>Regularization (L2): <code>lambda_l2 = 1e-4</code></p> </li> <li> <p>Penaliza\u00e7\u00e3o leve que reduz overfitting sem prejudicar aprendizagem.</p> </li> <li> <p>Patience (early stopping): <code>15</code> \u00e9pocas sem melhora em <code>val_loss</code>.</p> </li> <li> <p>Inicializa\u00e7\u00e3o de pesos: normal padr\u00e3o com escala <code>1/sqrt(fan_in)</code>:</p> </li> <li> <p>ex.: <code>W = rng.normal(0,1,(out,in)) / sqrt(in)</code> \u2014 reduz satura\u00e7\u00e3o inicial e ajuda na estabilidade do treino.</p> </li> <li> <p>Seed / reproducibility: <code>np.random.default_rng(42)</code> e <code>random_state=42</code> nos splits.</p> </li> </ul> <p>Ajustes e recomenda\u00e7\u00f5es:</p> <ul> <li>Para acelerar converg\u00eancia pode-se testar <code>ReLU + Adam</code> (optimizers modernos) e <code>batch normalization</code>.</li> <li>Para lidar com desbalanceamento usar <code>class_weight</code> no loss ou t\u00e9cnicas de oversampling (SMOTE).</li> </ul>"},{"location":"projetos/classification_project/#45-implementation-details","title":"4.5 Implementation Details","text":"<p>A implementa\u00e7\u00e3o do MLP foi feita inteiramente em NumPy, permitindo controle direto sobre cada etapa \u2014 forward pass, c\u00e1lculo da fun\u00e7\u00e3o de perda, retropropaga\u00e7\u00e3o e atualiza\u00e7\u00e3o dos par\u00e2metros. Essa abordagem possibilita compreender o fluxo interno de dados e gradientes, sem a abstra\u00e7\u00e3o de frameworks como PyTorch ou TensorFlow.</p> <p>Para refer\u00eancia, as etapas de treinamento seguiram o fluxo do material de Numerical Simulation do curso, dividido em:</p> <ol> <li>Forward Pass: os dados s\u00e3o propagados camada a camada;</li> <li>Loss Calculation: c\u00e1lculo da cross-entropy e penaliza\u00e7\u00e3o L2;</li> <li>Backward Pass: propaga\u00e7\u00e3o reversa dos gradientes para ajustar pesos e vieses.</li> </ol> <pre><code># Forward Pass (propaga\u00e7\u00e3o)\nz1 = xb @ W1.T + b1; h1 = tanh(z1)\nz2 = h1 @ W2.T + b2; h2 = tanh(z2)\nz3 = h2 @ W3.T + b3; yhat = softmax(z3)\n\n# C\u00e1lculo da Loss (cross-entropy + L2)\nloss_ce = -np.mean(np.sum(yb * np.log(yhat + 1e-9), axis=1))\nl2_term = lambda_l2 * (np.sum(W1*W1) + np.sum(W2*W2) + np.sum(W3*W3))\nloss = loss_ce + l2_term\n\n# Backpropagation\nd3 = (yhat - yb) / B\ngW3 = d3.T @ h2 + 2*lambda_l2*W3\ngb3 = d3.sum(axis=0)\n\ndh2 = d3 @ W3\ndz2 = dh2 * (1 - h2**2)\ngW2 = dz2.T @ h1 + 2*lambda_l2*W2\ngb2 = dz2.sum(axis=0)\n\ndh1 = dz2 @ W2\ndz1 = dh1 * (1 - h1**2)\ngW1 = dz1.T @ xb + 2*lambda_l2*W1\ngb1 = dz1.sum(axis=0)\n\n# Atualiza\u00e7\u00e3o dos par\u00e2metros\nW3 -= lr * gW3; b3 -= lr * gb3\nW2 -= lr * gW2; b2 -= lr * gb2\nW1 -= lr * gW1; b1 -= lr * gb1\n</code></pre> <p>Durante o treinamento, foram adotadas tr\u00eas pr\u00e1ticas fundamentais:</p> <ul> <li>Mini-Batch Training: processa subconjuntos de amostras, aumentando a efici\u00eancia e introduzindo ru\u00eddo estoc\u00e1stico que ajuda na generaliza\u00e7\u00e3o.</li> <li>Regulariza\u00e7\u00e3o L2: evita overfitting penalizando pesos muito grandes.</li> <li>Early Stopping: interrompe o treino se a perda de valida\u00e7\u00e3o (<code>val_loss</code>) n\u00e3o melhorar ap\u00f3s 15 \u00e9pocas consecutivas, preservando o modelo \u00f3timo.</li> </ul>"},{"location":"projetos/classification_project/#monitoramento-de-metricas","title":"Monitoramento de m\u00e9tricas","text":"<p>Durante as \u00e9pocas, foram registrados vetores de perda e acur\u00e1cia para treino e valida\u00e7\u00e3o:</p> <ul> <li><code>train_losses</code>, <code>val_losses</code></li> <li><code>train_accs</code>, <code>val_accs</code></li> </ul> <p>Exemplo dos logs registrados:</p> <pre><code>\u00c9poca  10 | TrainLoss 0.5499 | ValLoss 0.5394 | TrainAcc 0.793 | ValAcc 0.791\n\u00c9poca 200 | TrainLoss 0.4619 | ValLoss 0.4555 | TrainAcc 0.826 | ValAcc 0.823\nEarly stopping (\u00e9poca 219) melhor val_loss=0.4528\n</code></pre> <p>Esses logs mostram uma converg\u00eancia suave, com perda diminuindo at\u00e9 a \u00e9poca ~200, quando a valida\u00e7\u00e3o estabiliza, demonstrando equil\u00edbrio entre aprendizado e generaliza\u00e7\u00e3o.</p>"},{"location":"projetos/classification_project/#geracao-das-curvas-de-aprendizado","title":"Gera\u00e7\u00e3o das curvas de aprendizado","text":"<p>As curvas de loss e acur\u00e1cia foram plotadas para inspe\u00e7\u00e3o visual da converg\u00eancia:</p> <pre><code>plt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Val Loss\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Loss\"); plt.title(\"Curva de Loss\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(train_accs, label=\"Train Acc\")\nplt.plot(val_accs, label=\"Val Acc\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\"); plt.title(\"Curva de Acur\u00e1cia\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Esses gr\u00e1ficos permitem verificar se h\u00e1 overfitting (diverg\u00eancia entre treino e valida\u00e7\u00e3o) ou underfitting (valores de perda altos e est\u00e1veis).</p>"},{"location":"projetos/classification_project/#avaliacao-e-checkpoint","title":"Avalia\u00e7\u00e3o e checkpoint","text":"<p>Ao final do treino:</p> <ul> <li>O modelo \u00e9 restaurado para os pesos do melhor checkpoint (menor <code>val_loss</code>);</li> <li>Avaliado no conjunto de teste com m\u00e9tricas: acur\u00e1cia, precis\u00e3o, recall e F1-score;</li> <li>Uma matriz de confus\u00e3o \u00e9 gerada para an\u00e1lise dos erros;</li> <li>O decision boundary \u00e9 visualizado no espa\u00e7o 2D via PCA.</li> </ul>"},{"location":"projetos/classification_project/#5-model-training","title":"5. Model Training","text":""},{"location":"projetos/classification_project/#51-training-setup","title":"5.1 Training Setup","text":"<ul> <li>Dados j\u00e1 codificados e escalados (MinMax).</li> <li>Split feito com <code>stratify</code> para preservar propor\u00e7\u00f5es de classe (70% train / 15% val / 15% test).</li> <li>Shuffle por \u00e9poca com <code>rng.permutation</code>.</li> </ul>"},{"location":"projetos/classification_project/#52-training-loop-resumo","title":"5.2 Training Loop (resumo)","text":"<ul> <li> <p>Para cada \u00e9poca:</p> </li> <li> <p>embaralha treino;</p> </li> <li>itera por mini-batches;</li> <li>forward \u2192 calcula <code>loss</code> (+ L2) \u2192 backward \u2192 atualiza par\u00e2metros;</li> <li> <p>ao fim da \u00e9poca calcula <code>train_loss</code>, <code>val_loss</code>, <code>train_acc</code>, <code>val_acc</code>.</p> </li> <li> <p>Early stopping: interrompe se <code>val_loss</code> n\u00e3o melhorar por <code>patience</code> \u00e9pocas.</p> </li> </ul> <p>Trechos chave (exemplo do forward, loss e update):</p> <pre><code># forward (mini-batch)\nz1 = xb @ W1.T + b1; h1 = tanh(z1)\nz2 = h1 @ W2.T + b2; h2 = tanh(z2)\nz3 = h2 @ W3.T + b3; yhat = softmax(z3)\n\n# loss CE + L2\nloss_ce = -np.mean(np.sum(yb * np.log(yhat + 1e-9), axis=1))\nl2_term = lambda_l2 * (np.sum(W1*W1) + ...)\nloss = loss_ce + l2_term\n\n# backward (resumo)\nd3 = (yhat - yb) / B\ngW3 = d3.T @ h2 + 2*lambda_l2*W3\n... # calcula gW2,gW1 e gb*\n\n# update\nW3 -= lr * gW3\n...\n</code></pre>"},{"location":"projetos/classification_project/#53-training-observations-logs","title":"5.3 Training Observations (logs)","text":"<p>Trechos de log produzidos durante treinamento:</p> <pre><code>\u00c9poca   1 | TrainLoss 0.9766 | ValLoss 0.8791 | TrainAcc 0.654 | ValAcc 0.653\n\u00c9poca  10 | TrainLoss 0.5499 | ValLoss 0.5394 | TrainAcc 0.793 | ValAcc 0.791\n...\n\u00c9poca 210 | TrainLoss 0.4615 | ValLoss 0.4534 | TrainAcc 0.825 | ValAcc 0.823\nEarly stopping (\u00e9poca 219) melhor val_loss=0.4528\n</code></pre> <ul> <li>Observa-se r\u00e1pida converg\u00eancia nas primeiras 50 \u00e9pocas; depois estabiliza\u00e7\u00e3o com pequenas melhorias.</li> <li>Early stopping ao redor de ~219 \u00e9pocas.</li> </ul>"},{"location":"projetos/classification_project/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy","text":""},{"location":"projetos/classification_project/#61-data-split","title":"6.1 Data Split","text":"<ul> <li><code>train / val / test = 70% / 15% / 15%</code> com stratify para preservar distribui\u00e7\u00e3o de classes.</li> </ul>"},{"location":"projetos/classification_project/#62-validation-strategy","title":"6.2 Validation Strategy","text":"<ul> <li>Valida\u00e7\u00e3o simples (conjunto de valida\u00e7\u00e3o dedicado) usada para sele\u00e7\u00e3o de hiperpar\u00e2metros e early stopping.</li> </ul>"},{"location":"projetos/classification_project/#63-reproducibility","title":"6.3 Reproducibility","text":"<ul> <li>Seed fixada via <code>np.random.default_rng(42)</code> e <code>random_state=42</code> nos <code>train_test_split</code>.</li> </ul>"},{"location":"projetos/classification_project/#64-overfitting-prevention","title":"6.4 Overfitting Prevention","text":"<ul> <li>Regulariza\u00e7\u00e3o L2 em todos os pesos.</li> <li>Early stopping baseado em <code>val_loss</code>.</li> <li>Batch training (mini-batch) fornece ru\u00eddo \u00fatil ao otimizar.</li> </ul>"},{"location":"projetos/classification_project/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization","text":""},{"location":"projetos/classification_project/#71-loss-and-accuracy-curves","title":"7.1 Loss and Accuracy Curves","text":"<p>Figura: Curva de Loss e Acur\u00e1cia (treino vs valida\u00e7\u00e3o).</p> <p></p>"},{"location":"projetos/classification_project/#72-analysis-of-learning-behavior","title":"7.2 Analysis of Learning Behavior","text":"<ul> <li>Treino e valida\u00e7\u00e3o converge pr\u00f3ximos e sem diverg\u00eancia acentuada \u2192 overfitting limitado gra\u00e7as a L2 + early stopping.</li> <li>Acur\u00e1cia de valida\u00e7\u00e3o estabilizou ~0.823, indicando bom ajuste do modelo.</li> </ul>"},{"location":"projetos/classification_project/#8-evaluation-metrics","title":"8. Evaluation Metrics","text":""},{"location":"projetos/classification_project/#81-test-metrics-resultado-final","title":"8.1 Test Metrics (resultado final)","text":"<p>Resultados calculados no conjunto de teste:</p> <pre><code>M\u00c9TRICAS TESTE\nAcur\u00e1cia : 0.8195\nPrecis\u00e3o : 0.7897\nRecall   : 0.7690\nF1-score : 0.7771\n</code></pre>"},{"location":"projetos/classification_project/#82-baseline-majority-class","title":"8.2 Baseline (majority class)","text":"<p>Classe majorit\u00e1ria no teste: <code>2</code> Baseline (predizer sempre a classe majorit\u00e1ria):</p> <pre><code>M\u00c9TRICAS BASELINE (Majority Class)\nAcur\u00e1cia : 0.4708\nPrecis\u00e3o : 0.1569\nRecall   : 0.3333\nF1-score : 0.2134\n</code></pre> <p>Compara\u00e7\u00e3o: o MLP supera claramente o baseline em todas as m\u00e9tricas (ex.: acur\u00e1cia 0.8195 vs 0.4708).</p>"},{"location":"projetos/classification_project/#83-confusion-matrix","title":"8.3 Confusion Matrix","text":"<p>Figura: Matriz de Confus\u00e3o (Teste):</p> <p></p> <p>Classe \"Graduate (1.0)\" O modelo obteve o melhor desempenho nesta classe, com 4088 acertos e poucos erros de confus\u00e3o. Representa tamb\u00e9m a classe majorit\u00e1ria, o que explica a facilidade de identifica\u00e7\u00e3o. Poucos graduados foram confundidos como \u201cDropout\u201d (242) ou \u201cEnrolled\u201d (84).</p> <p>Classe \"Enrolled (0.5)\" A segunda melhor classe em termos de acerto (2568 corretos). Alguns alunos ainda matriculados foram incorretamente classificados como \u201cDropout\u201d (341) \u2014 possivelmente devido a perfis de desempenho inicial similares. Menor confus\u00e3o com \u201cGraduate\u201d (205) indica que o modelo diferencia razoavelmente alunos ativos dos formados.</p> <p>Classe \"Dropout (0.0)\" A classe mais problem\u00e1tica: apenas 1028 acertos contra 820 erros (211+609). Quase 600 alunos que abandonaram o curso foram classificados como \u201cGraduate\u201d \u2014 mostrando que o modelo tem dificuldade em detectar evas\u00e3o, o que \u00e9 cr\u00edtico neste tipo de aplica\u00e7\u00e3o. Essa confus\u00e3o indica que o modelo tende a superestimar o sucesso acad\u00eamico, possivelmente por conta do desbalanceamento do dataset.</p>"},{"location":"projetos/classification_project/#84-decision-boundary-pca-2d","title":"8.4 Decision Boundary (PCA 2D)","text":"<p>Figura: Decis\u00e3o no espa\u00e7o PCA (PC1 \u00d7 PC2) com pontos corretos/errados do conjunto de teste.</p> <p></p>"},{"location":"projetos/classification_project/#9-conclusion","title":"9. Conclusion","text":""},{"location":"projetos/classification_project/#91-key-findings","title":"9.1 Key Findings","text":"<ul> <li>MLP implementado do zero (NumPy) atingiu Acur\u00e1cia = 0.8195 e F1 \u2248 0.7771 no conjunto de teste, superando largamente o baseline majorit\u00e1rio.</li> <li>PCA mostrou separa\u00e7\u00e3o parcial entre classes, justificando uso de modelo n\u00e3o-linear (MLP).</li> </ul>"},{"location":"projetos/classification_project/#92-limitations","title":"9.2 Limitations","text":"<ul> <li>Desbalanceamento de classes requer an\u00e1lise adicional (class weighting, oversampling/undersampling).</li> <li>Remo\u00e7\u00e3o de outliers foi feita por limites manuais \u2014 abordagem autom\u00e1tica (IQR, isolation forest) poderia ser comparada.</li> <li>Implementa\u00e7\u00e3o atual usa <code>tanh</code>; testar <code>ReLU</code> + batchnorm pode acelerar/aperfei\u00e7oar treino.</li> </ul>"},{"location":"projetos/classification_project/#10-referencias","title":"10. Refer\u00eancias","text":"<ol> <li> <p>Material de apoio \u2014 Artificial Neural Networks and Deep Learning https://insper.github.io/ann-dl/</p> </li> <li> <p>Kaggle \u2014 Academic Success Classifier Competition https://www.kaggle.com/competitions/academic-success-classifier/data</p> </li> <li> <p>UCI Machine Learning Repository \u2014 Predict Students\u2019 Dropout and Academic Success https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success</p> </li> </ol>"},{"location":"projetos/classification_project/#appendix-codigo-de-treino-e-avaliacao","title":"Appendix \u2014 C\u00f3digo de Treino e Avalia\u00e7\u00e3o","text":"<p>Arquivo Jupyter implementando o MLP aqui.</p>"},{"location":"projetos/regression_project/","title":"ANN Regression","text":"In\u00a0[5]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns  import tensorflow as tf from tensorflow.keras.utils import plot_model  from sklearn.preprocessing import MinMaxScaler from sklearn.model_selection import train_test_split, cross_val_score from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename)) In\u00a0[56]: Copied! <pre>df = pd.read_csv(\"data/Clean_Dataset.csv\")\ndf = df.drop(['Unnamed: 0'], axis = 1)\nprint(\"\\n================== HEAD DO DATASET ==================\\n\")\ndisplay(df.head())\nprint(\"\\n=====================================================\\n\")\nprint(\"\\n================== SHAPE DO DATASET ==================\\n\")\ndisplay(df.shape)\nprint(\"\\n=====================================================\\n\")\ndisplay(df.dtypes)\nprint(\"\\n================== VISUALIZANDO VALORES NULOS ==================\\n\")\nnull_summary = df.isnull().sum().to_frame().rename(columns={0: 'Null Count'})\nnull_summary['Percentage'] = (null_summary['Null Count'] / len(df)) * 100\ndisplay(null_summary)\nprint(\"\\n=====================================================\\n\")\n</pre> df = pd.read_csv(\"data/Clean_Dataset.csv\") df = df.drop(['Unnamed: 0'], axis = 1) print(\"\\n================== HEAD DO DATASET ==================\\n\") display(df.head()) print(\"\\n=====================================================\\n\") print(\"\\n================== SHAPE DO DATASET ==================\\n\") display(df.shape) print(\"\\n=====================================================\\n\") display(df.dtypes) print(\"\\n================== VISUALIZANDO VALORES NULOS ==================\\n\") null_summary = df.isnull().sum().to_frame().rename(columns={0: 'Null Count'}) null_summary['Percentage'] = (null_summary['Null Count'] / len(df)) * 100 display(null_summary) print(\"\\n=====================================================\\n\") <pre>================== HEAD DO DATASET ==================\n\n</pre> airline flight source_city departure_time stops arrival_time destination_city class duration days_left price 0 SpiceJet SG-8709 Delhi Evening zero Night Mumbai Economy 2.17 1 5953 1 SpiceJet SG-8157 Delhi Early_Morning zero Morning Mumbai Economy 2.33 1 5953 2 AirAsia I5-764 Delhi Early_Morning zero Early_Morning Mumbai Economy 2.17 1 5956 3 Vistara UK-995 Delhi Morning zero Afternoon Mumbai Economy 2.25 1 5955 4 Vistara UK-963 Delhi Morning zero Morning Mumbai Economy 2.33 1 5955 <pre>=====================================================\n\n\n================== SHAPE DO DATASET ==================\n\n</pre> <pre>(300153, 11)</pre> <pre>=====================================================\n\n</pre> <pre>airline              object\nflight               object\nsource_city          object\ndeparture_time       object\nstops                object\narrival_time         object\ndestination_city     object\nclass                object\nduration            float64\ndays_left             int64\nprice                 int64\ndtype: object</pre> <pre>================== VISUALIZANDO VALORES NULOS ==================\n\n</pre> Null Count Percentage airline 0 0.0 flight 0 0.0 source_city 0 0.0 departure_time 0 0.0 stops 0 0.0 arrival_time 0 0.0 destination_city 0 0.0 class 0 0.0 duration 0 0.0 days_left 0 0.0 price 0 0.0 <pre>=====================================================\n\n</pre> <p>Na primeira breve an\u00e1lise, o dataset n\u00e3o possui valores nulos, h\u00e1 v\u00e1rias vari\u00e1veis categ\u00f3ricas que precisam ser tratadas e o target \u00e9 num\u00e9rico cont\u00ednuo.</p> In\u00a0[19]: Copied! <pre># Todas poss\u00edveis vari\u00e1veis categ\u00f3ricas\ncat_features = [column for column in df.columns if df[column].dtype in [\"object\", \"category\", \"bool\"]]\n\n# Num\u00e9ricas mas na verdade s\u00e3o categ\u00f3ricas\nnum_but_cat_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (df[column].nunique() &lt; 9)]\n\n# Vari\u00e1veis categ\u00f3ricas\ncategoric_features = cat_features + num_but_cat_features\ncategoric_features = [column for column in categoric_features]\n\ndf[categoric_features]\n</pre> # Todas poss\u00edveis vari\u00e1veis categ\u00f3ricas cat_features = [column for column in df.columns if df[column].dtype in [\"object\", \"category\", \"bool\"]]  # Num\u00e9ricas mas na verdade s\u00e3o categ\u00f3ricas num_but_cat_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (df[column].nunique() &lt; 9)]  # Vari\u00e1veis categ\u00f3ricas categoric_features = cat_features + num_but_cat_features categoric_features = [column for column in categoric_features]  df[categoric_features] Out[19]: airline flight source_city departure_time stops arrival_time destination_city class 0 SpiceJet SG-8709 Delhi Evening zero Night Mumbai Economy 1 SpiceJet SG-8157 Delhi Early_Morning zero Morning Mumbai Economy 2 AirAsia I5-764 Delhi Early_Morning zero Early_Morning Mumbai Economy 3 Vistara UK-995 Delhi Morning zero Afternoon Mumbai Economy 4 Vistara UK-963 Delhi Morning zero Morning Mumbai Economy ... ... ... ... ... ... ... ... ... 300148 Vistara UK-822 Chennai Morning one Evening Hyderabad Business 300149 Vistara UK-826 Chennai Afternoon one Night Hyderabad Business 300150 Vistara UK-832 Chennai Early_Morning one Night Hyderabad Business 300151 Vistara UK-828 Chennai Early_Morning one Evening Hyderabad Business 300152 Vistara UK-822 Chennai Morning one Evening Hyderabad Business <p>300153 rows \u00d7 8 columns</p> In\u00a0[\u00a0]: Copied! <pre>sns.set_style(\"whitegrid\")\nsns.set_context(\"talk\") \npalette_colors = \"Set2\" \n\nplt.figure(figsize=(20,32))\n\nplt.subplot(4, 2, 1)\nsns.countplot(x=df[\"airline\"], data=df, legend=False)\nplt.title(\"Frequency of Airline\")\nsns.despine() \n\nplt.subplot(4, 2, 2)\nsns.countplot(x=df[\"source_city\"], data=df, palette=palette_colors, \n              hue=df[\"source_city\"], legend=False) \nplt.title(\"Frequency of Source City\")\nsns.despine() \n\nplt.subplot(4, 2, 3)\nsns.countplot(x=df[\"departure_time\"], data=df, palette=palette_colors, \n              hue=df[\"departure_time\"], legend=False)\nplt.title(\"Frequency of Departure Time\")\nsns.despine() \n\nplt.subplot(4, 2, 4)\nsns.countplot(x=df[\"stops\"], data=df, palette=\"deep\", \n              hue=df[\"stops\"], legend=False) \nplt.title(\"Frequency of Stops\")\nsns.despine() \n\nplt.subplot(4, 2, 5)\nsns.countplot(x=df[\"arrival_time\"], data=df, palette=palette_colors, \n              hue=df[\"arrival_time\"], legend=False)\nplt.title(\"Frequency of Arrival Time\")\nsns.despine() \n\nplt.subplot(4, 2, 6)\nsns.countplot(x=df[\"destination_city\"], data=df, palette=palette_colors, \n              hue=df[\"destination_city\"], legend=False)\nplt.title(\"Frequency of Destination City\")\nsns.despine() \n\nplt.subplot(4, 2, 7)\nsns.countplot(x=df[\"class\"], data=df, palette=\"pastel\", \n              hue=df[\"class\"], legend=False) \nplt.title(\"Class Frequency\")\nsns.despine() \n\nplt.tight_layout() \nplt.show()\n</pre> sns.set_style(\"whitegrid\") sns.set_context(\"talk\")  palette_colors = \"Set2\"   plt.figure(figsize=(20,32))  plt.subplot(4, 2, 1) sns.countplot(x=df[\"airline\"], data=df, legend=False) plt.title(\"Frequency of Airline\") sns.despine()   plt.subplot(4, 2, 2) sns.countplot(x=df[\"source_city\"], data=df, palette=palette_colors,                hue=df[\"source_city\"], legend=False)  plt.title(\"Frequency of Source City\") sns.despine()   plt.subplot(4, 2, 3) sns.countplot(x=df[\"departure_time\"], data=df, palette=palette_colors,                hue=df[\"departure_time\"], legend=False) plt.title(\"Frequency of Departure Time\") sns.despine()   plt.subplot(4, 2, 4) sns.countplot(x=df[\"stops\"], data=df, palette=\"deep\",                hue=df[\"stops\"], legend=False)  plt.title(\"Frequency of Stops\") sns.despine()   plt.subplot(4, 2, 5) sns.countplot(x=df[\"arrival_time\"], data=df, palette=palette_colors,                hue=df[\"arrival_time\"], legend=False) plt.title(\"Frequency of Arrival Time\") sns.despine()   plt.subplot(4, 2, 6) sns.countplot(x=df[\"destination_city\"], data=df, palette=palette_colors,                hue=df[\"destination_city\"], legend=False) plt.title(\"Frequency of Destination City\") sns.despine()   plt.subplot(4, 2, 7) sns.countplot(x=df[\"class\"], data=df, palette=\"pastel\",                hue=df[\"class\"], legend=False)  plt.title(\"Class Frequency\") sns.despine()   plt.tight_layout()  plt.show() In\u00a0[20]: Copied! <pre>numeric_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (column not in categoric_features)]\n\ndf[numeric_features]\n</pre> numeric_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (column not in categoric_features)]  df[numeric_features] Out[20]: duration days_left price 0 2.17 1 5953 1 2.33 1 5953 2 2.17 1 5956 3 2.25 1 5955 4 2.33 1 5955 ... ... ... ... 300148 10.08 49 69265 300149 10.42 49 77105 300150 13.83 49 79099 300151 10.00 49 81585 300152 10.08 49 81585 <p>300153 rows \u00d7 3 columns</p> In\u00a0[25]: Copied! <pre>plt.figure(figsize=(20,6))\n\nfor i, col in enumerate([\"duration\", \"days_left\", \"price\"], 1):\n    plt.subplot(1, 3, i)\n    sns.histplot(df[col], kde=True, color=\"skyblue\", bins=30)\n    plt.title(f\"Distribui\u00e7\u00e3o de {col.capitalize()}\", fontsize=14)\n    plt.xlabel(col.capitalize())\n    plt.ylabel(\"Frequ\u00eancia\")\n\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(20,6))  for i, col in enumerate([\"duration\", \"days_left\", \"price\"], 1):     plt.subplot(1, 3, i)     sns.histplot(df[col], kde=True, color=\"skyblue\", bins=30)     plt.title(f\"Distribui\u00e7\u00e3o de {col.capitalize()}\", fontsize=14)     plt.xlabel(col.capitalize())     plt.ylabel(\"Frequ\u00eancia\")  plt.tight_layout() plt.show()  In\u00a0[30]: Copied! <pre>corr = df[\n    [c for c in numeric_features if not c.startswith('Unnamed')]\n].corr()\n\nplt.figure(figsize=(7, 6))\n\ncmap = sns.color_palette(\"vlag\", as_cmap=True) \n\nax = sns.heatmap(\n    corr,\n    cmap=cmap,\n    vmin=-1, vmax=1, center=0,       \n    annot=True, fmt=\".2f\",            \n    annot_kws={\"size\":8},\n    square=True,\n    linewidths=0.4, linecolor=\"white\",\n    cbar_kws={\"shrink\":0.8, \"label\":\"Pearson r\"},\n)\n\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.title(\"Matriz de Correla\u00e7\u00e3o (Pearson)\")\nplt.tight_layout()\nplt.show()\n</pre> corr = df[     [c for c in numeric_features if not c.startswith('Unnamed')] ].corr()  plt.figure(figsize=(7, 6))  cmap = sns.color_palette(\"vlag\", as_cmap=True)   ax = sns.heatmap(     corr,     cmap=cmap,     vmin=-1, vmax=1, center=0,            annot=True, fmt=\".2f\",                 annot_kws={\"size\":8},     square=True,     linewidths=0.4, linecolor=\"white\",     cbar_kws={\"shrink\":0.8, \"label\":\"Pearson r\"}, )  plt.xticks(rotation=45, ha=\"right\") plt.yticks(rotation=0) plt.title(\"Matriz de Correla\u00e7\u00e3o (Pearson)\") plt.tight_layout() plt.show() <p>Uma vez que \"stop\" e \"class\" s\u00e3o vari\u00e1veis orin\u00e1rias, o encoding tamb\u00e9m deve ser de acordo.</p> In\u00a0[31]: Copied! <pre>df[\"stops\"] = df[\"stops\"].replace({'zero': 0,\n                                   'one': 1,\n                                   'two_or_more': 2})\n\ndf[\"class\"] = df[\"class\"].replace({'Economy': 0,\n                                   'Business': 1})\n</pre> df[\"stops\"] = df[\"stops\"].replace({'zero': 0,                                    'one': 1,                                    'two_or_more': 2})  df[\"class\"] = df[\"class\"].replace({'Economy': 0,                                    'Business': 1}) <pre>/tmp/ipykernel_87733/3553766138.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"stops\"] = df[\"stops\"].replace({'zero': 0,\n/tmp/ipykernel_87733/3553766138.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"class\"] = df[\"class\"].replace({'Economy': 0,\n</pre> In\u00a0[\u00a0]: Copied! <pre># One Hot Encoding\ndummies_variables = [\"airline\", \"source_city\", \"departure_time\", \"arrival_time\", \"destination_city\"]\ndummies = pd.get_dummies(df[dummies_variables], drop_first= True)\n\ndf = pd.concat([df, dummies], axis=1)\ndf = df.drop(dummies_variables + ['flight'], axis=1) # Excluindo colunas antigas antes do one hot.\n\ndf.head()\n</pre> # One Hot Encoding dummies_variables = [\"airline\", \"source_city\", \"departure_time\", \"arrival_time\", \"destination_city\"] dummies = pd.get_dummies(df[dummies_variables], drop_first= True)  df = pd.concat([df, dummies], axis=1) df = df.drop(dummies_variables + ['flight'], axis=1) # Excluindo colunas antigas antes do one hot.  df.head() Out[\u00a0]: stops class duration days_left price airline_Air_India airline_GO_FIRST airline_Indigo airline_SpiceJet airline_Vistara ... arrival_time_Early_Morning arrival_time_Evening arrival_time_Late_Night arrival_time_Morning arrival_time_Night destination_city_Chennai destination_city_Delhi destination_city_Hyderabad destination_city_Kolkata destination_city_Mumbai 0 0 0 2.17 1 5953 False False False True False ... False False False False True False False False False True 1 0 0 2.33 1 5953 False False False True False ... False False False True False False False False False True 2 0 0 2.17 1 5956 False False False False False ... True False False False False False False False False True 3 0 0 2.25 1 5955 False False False False True ... False False False False False False False False False True 4 0 0 2.33 1 5955 False False False False True ... False False False True False False False False False True <p>5 rows \u00d7 30 columns</p> In\u00a0[33]: Copied! <pre>df.shape\n</pre> df.shape Out[33]: <pre>(300153, 30)</pre> <p>Ap\u00f3s o one hot, 18 novas featutes surgiram.</p> In\u00a0[34]: Copied! <pre>corr = df.corr(numeric_only=True)\nplt.figure(figsize=(35, 25))\nax = sns.heatmap(\n    corr,                      \n    annot=True, fmt=\".2f\",\n    vmin=-1.0, vmax=1.0, center=0,\n    cmap=\"vlag\",             \n    square=True,\n    linewidths=0.4, linecolor=\"white\",\n    cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"},\n)\n\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.title(\"Matriz de Correla\u00e7\u00e3o\")\nplt.tight_layout()\nplt.show()\n</pre> corr = df.corr(numeric_only=True) plt.figure(figsize=(35, 25)) ax = sns.heatmap(     corr,                           annot=True, fmt=\".2f\",     vmin=-1.0, vmax=1.0, center=0,     cmap=\"vlag\",                  square=True,     linewidths=0.4, linecolor=\"white\",     cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"}, )  plt.xticks(rotation=45, ha=\"right\") plt.yticks(rotation=0) plt.title(\"Matriz de Correla\u00e7\u00e3o\") plt.tight_layout() plt.show() In\u00a0[37]: Copied! <pre>import numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nSEED = 42\n\ny = df['price']\nX = df.drop('price', axis=1)\n</pre> import numpy as np, pandas as pd from sklearn.model_selection import train_test_split SEED = 42  y = df['price'] X = df.drop('price', axis=1) In\u00a0[38]: Copied! <pre>X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.30, random_state=SEED)\nX_val,   X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=SEED)\n\nprint(f\"Train ({(len(X_train)/len(X)):.2f}): {len(X_train)} | Val ({len(X_val)/len(X):.2f}): {len(X_val)} | Test ({len(X_test)/len(X):.2f}): {len(X_test)}\")\n</pre> X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.30, random_state=SEED) X_val,   X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=SEED)  print(f\"Train ({(len(X_train)/len(X)):.2f}): {len(X_train)} | Val ({len(X_val)/len(X):.2f}): {len(X_val)} | Test ({len(X_test)/len(X):.2f}): {len(X_test)}\") <pre>Train (0.70): 210107 | Val (0.15): 45023 | Test (0.15): 45023\n</pre> In\u00a0[39]: Copied! <pre>X_train\n</pre> X_train Out[39]: stops class duration days_left airline_Air_India airline_GO_FIRST airline_Indigo airline_SpiceJet airline_Vistara source_city_Chennai ... arrival_time_Early_Morning arrival_time_Evening arrival_time_Late_Night arrival_time_Morning arrival_time_Night destination_city_Chennai destination_city_Delhi destination_city_Hyderabad destination_city_Kolkata destination_city_Mumbai 2406 1 0 13.42 14 False False False False True False ... False False False True False False False False False True 275865 1 1 9.58 23 False False False False True False ... False True False False False False True False False False 297156 1 1 11.17 29 False False False False True True ... False True False False False False False False True False 12826 1 0 5.08 16 False False False False True False ... False False False False True False False False False False 93166 1 0 12.58 45 False True False False False False ... False False False False True False True False False False ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 119879 1 0 20.50 2 False False False False True False ... False False False False False False True False False False 259178 1 1 25.42 7 False False False False True False ... False False False False True True False False False False 131932 1 0 13.67 29 True False False False False False ... False False False True False False False False False True 146867 1 0 8.33 39 False True False False False False ... False False False False True False False True False False 121958 1 0 20.17 17 True False False False False False ... False False False True False False True False False False <p>210107 rows \u00d7 29 columns</p> In\u00a0[40]: Copied! <pre># Scaling AP\u00d3S o split (sem tocar na vari\u00e1vel-alvo)\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnum_vars = ['duration', 'days_left']  # \u2b05\ufe0f n\u00e3o inclui 'price'\n\n# Ajusta no treino e aplica em val/test (sem vazamento)\nX_train.loc[:, num_vars] = scaler.fit_transform(X_train[num_vars])\nX_val.loc[:,   num_vars] = scaler.transform(X_val[num_vars])\nX_test.loc[:,  num_vars] = scaler.transform(X_test[num_vars])\n</pre> # Scaling AP\u00d3S o split (sem tocar na vari\u00e1vel-alvo) from sklearn.preprocessing import MinMaxScaler  scaler = MinMaxScaler() num_vars = ['duration', 'days_left']  # \u2b05\ufe0f n\u00e3o inclui 'price'  # Ajusta no treino e aplica em val/test (sem vazamento) X_train.loc[:, num_vars] = scaler.fit_transform(X_train[num_vars]) X_val.loc[:,   num_vars] = scaler.transform(X_val[num_vars]) X_test.loc[:,  num_vars] = scaler.transform(X_test[num_vars]) <pre>/tmp/ipykernel_87733/2227656102.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.27083333 0.45833333 0.58333333 ... 0.58333333 0.79166667 0.33333333]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  X_train.loc[:, num_vars] = scaler.fit_transform(X_train[num_vars])\n/tmp/ipykernel_87733/2227656102.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.20833333 0.875      0.27083333 ... 1.         1.         0.47916667]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  X_val.loc[:,   num_vars] = scaler.transform(X_val[num_vars])\n/tmp/ipykernel_87733/2227656102.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.0625     0.5625     0.77083333 ... 0.25       0.10416667 0.64583333]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  X_test.loc[:,  num_vars] = scaler.transform(X_test[num_vars])\n</pre> <p>A rede neural artificial (ANN) implementada realiza uma tarefa de regress\u00e3o supervisionada, com o objetivo de prever um valor cont\u00ednuo (no caso, o pre\u00e7o da passagem a\u00e9rea) a partir de 30 vari\u00e1veis de entrada previamente processadas.</p> <p>O modelo foi constru\u00eddo do zero, utilizando apenas NumPy, sem o uso de frameworks de alto n\u00edvel como TensorFlow ou PyTorch, o que permite controle total sobre o fluxo de c\u00e1lculo e o aprendizado.</p> <p>A arquitetura definida segue o padr\u00e3o feedforward totalmente conectado (Multilayer Perceptron \u2013 MLP), composta por:</p> <ul> <li>Camada de entrada: 30 neur\u00f4nios (uma para cada feature num\u00e9rica e categ\u00f3rica codificada).</li> <li>1\u00aa camada oculta: 32 neur\u00f4nios com fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU (Rectified Linear Unit).</li> <li>2\u00aa camada oculta: 16 neur\u00f4nios, tamb\u00e9m com ativa\u00e7\u00e3o ReLU.</li> <li>Camada de sa\u00edda: 1 neur\u00f4nio com sa\u00edda linear, adequada para tarefas de regress\u00e3o (onde o valor previsto \u00e9 cont\u00ednuo).</li> </ul> In\u00a0[54]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom math import sqrt\n\nX_train_base = X_train   \nX_val_base = X_val   \nX_test_base = X_test\n\nX_train = np.asarray(X_train_base, dtype=float)\nX_val   = np.asarray(X_val_base,   dtype=float)\nX_test  = np.asarray(X_test_base,  dtype=float)\n\ny_train = np.asarray(y_train, dtype=float).ravel()\ny_val   = np.asarray(y_val,   dtype=float).ravel()\ny_test  = np.asarray(y_test,  dtype=float).ravel()\n\ny_train_col = y_train.reshape(-1, 1)\ny_val_col   = y_val.reshape(-1, 1)\n\nN, D = X_train.shape\n\n# 1) M\u00e9tricas de regress\u00e3o\ndef metrics(y_true, y_pred):\n    y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()\n    mae  = np.mean(np.abs(y_true - y_pred))\n    rmse = sqrt(np.mean((y_true - y_pred)**2))\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n    r2 = 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan\n    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n\n# 2) Arquitetura: ReLU 128\u219264\u21921 (linear)\nrng = np.random.default_rng(42)\nH1, H2 = 32, 16\n\nW1 = rng.standard_normal((H1, D)) * np.sqrt(2.0 / D)\nb1 = np.zeros((1, H1))\n\nW2 = rng.standard_normal((H2, H1)) * np.sqrt(2.0 / H1) \nb2 = np.zeros((1, H2))\n\nW3 = rng.standard_normal((1, H2)) * np.sqrt(2.0 / H2)\nb3 = np.zeros((1, 1))\n\ndef relu(x):    \n    return np.maximum(0, x)\n\ndef relu_d(x):  \n    return (x &gt; 0).astype(x.dtype)\n\ndef forward(X):\n    Z1 = X @ W1.T + b1; A1 = relu(Z1)\n    Z2 = A1 @ W2.T + b2; A2 = relu(Z2)\n    Y  = A2 @ W3.T + b3          # sa\u00edda linear (regress\u00e3o)\n    return Z1, A1, Z2, A2, Y\n\n# 3) Treino: Adam + MSE\nlr = 1e-3\nepochs = 100\nbatch  = 128\n\nbeta1, beta2, eps = 0.9, 0.999, 1e-8\n\n# Adam buffers\nmW1 = np.zeros_like(W1) \nvW1 = np.zeros_like(W1)\n\nmW2 = np.zeros_like(W2)\nvW2 = np.zeros_like(W2)\n\nmW3 = np.zeros_like(W3)\nvW3 = np.zeros_like(W3)\n\nmb1 = np.zeros_like(b1)\nvb1 = np.zeros_like(b1)\n\nmb2 = np.zeros_like(b2) \nvb2 = np.zeros_like(b2)\n\nmb3 = np.zeros_like(b3)\nvb3 = np.zeros_like(b3)\n\nt = 0\n\ndef adam_update(param, grad, m, v):\n    m = beta1*m + (1-beta1)*grad\n    v = beta2*v + (1-beta2)*(grad*grad)\n    m_hat = m / (1 - beta1**t)\n    v_hat = v / (1 - beta2**t)\n    param -= lr * m_hat / (np.sqrt(v_hat) + eps)\n    return param, m, v\n\ntrain_losses, val_losses = [], []\n\nfor ep in range(1, epochs+1):\n    idx = rng.permutation(N)\n    epoch_loss = 0.0\n\n    for i in range(0, N, batch):\n        bidx = idx[i:i+batch]\n        xb = X_train[bidx]\n        yb = y_train_col[bidx]\n        B  = xb.shape[0]\n\n        #  forward \n        Z1, A1, Z2, A2, Yp = forward(xb)\n\n        #  loss (MSE)\n        mse = np.mean((Yp - yb)**2)\n        epoch_loss += mse * B\n\n        #  backprop \n        d3  = (2.0/B) * (Yp - yb)     # (B,1)\n        gW3 = d3.T @ A2               # (1,H2)\n        gb3 = d3.sum(axis=0, keepdims=True)\n\n        dA2 = d3 @ W3                 # (B,H2)\n        dZ2 = dA2 * relu_d(Z2)\n        gW2 = dZ2.T @ A1              # (H2,H1)\n        gb2 = dZ2.sum(axis=0, keepdims=True)\n\n        dA1 = dZ2 @ W2                # (B,H1)\n        dZ1 = dA1 * relu_d(Z1)\n        gW1 = dZ1.T @ xb              # (H1,D)\n        gb1 = dZ1.sum(axis=0, keepdims=True)\n\n        #  Adam step (incrementa t uma vez por batch) \n        t += 1\n        W3, mW3, vW3 = adam_update(W3, gW3, mW3, vW3)\n        b3, mb3, vb3 = adam_update(b3, gb3, mb3, vb3)\n        W2, mW2, vW2 = adam_update(W2, gW2, mW2, vW2)\n        b2, mb2, vb2 = adam_update(b2, gb2, mb2, vb2)\n        W1, mW1, vW1 = adam_update(W1, gW1, mW1, vW1)\n        b1, mb1, vb1 = adam_update(b1, gb1, mb1, vb1)\n\n    epoch_loss /= N\n    train_losses.append(epoch_loss)\n\n    # valida\u00e7\u00e3o\n    _, _, _, _, Yv = forward(X_val)\n    val_mse = np.mean((Yv - y_val_col)**2)\n    val_losses.append(val_mse)\n\n    print(f\"\u00c9poca {ep:02d} | Train MSE {epoch_loss:.4f} | Val MSE {val_mse:.4f}\")\n\n# 4) Avalia\u00e7\u00e3o e gr\u00e1ficos\ndef predict(X):\n    _, _, _, _, Y = forward(X)\n    return Y.ravel()\n\npred_val  = predict(X_val)\npred_test = predict(X_test)\n\nprint(\"Val :\", metrics(y_val,  pred_val))\nprint(\"Test:\", metrics(y_test, pred_test))\n\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(train_losses, label=\"Train\")\nplt.plot(val_losses,   label=\"Val\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\"); plt.title(\"Curva de Loss\"); plt.legend()\n\nplt.subplot(1,2,2)\nplt.scatter(y_val, pred_val, s=8, alpha=0.4)\nlims = [min(y_val.min(), pred_val.min()), max(y_val.max(), pred_val.max())]\nplt.plot(lims, lims, \"--\")\nplt.xlabel(\"Verdadeiro\"); plt.ylabel(\"Previsto\"); plt.title(\"Val: y_true vs y_pred\")\nplt.tight_layout(); plt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from math import sqrt  X_train_base = X_train    X_val_base = X_val    X_test_base = X_test  X_train = np.asarray(X_train_base, dtype=float) X_val   = np.asarray(X_val_base,   dtype=float) X_test  = np.asarray(X_test_base,  dtype=float)  y_train = np.asarray(y_train, dtype=float).ravel() y_val   = np.asarray(y_val,   dtype=float).ravel() y_test  = np.asarray(y_test,  dtype=float).ravel()  y_train_col = y_train.reshape(-1, 1) y_val_col   = y_val.reshape(-1, 1)  N, D = X_train.shape  # 1) M\u00e9tricas de regress\u00e3o def metrics(y_true, y_pred):     y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()     mae  = np.mean(np.abs(y_true - y_pred))     rmse = sqrt(np.mean((y_true - y_pred)**2))     ss_res = np.sum((y_true - y_pred)**2)     ss_tot = np.sum((y_true - np.mean(y_true))**2)     r2 = 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan     return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}  # 2) Arquitetura: ReLU 128\u219264\u21921 (linear) rng = np.random.default_rng(42) H1, H2 = 32, 16  W1 = rng.standard_normal((H1, D)) * np.sqrt(2.0 / D) b1 = np.zeros((1, H1))  W2 = rng.standard_normal((H2, H1)) * np.sqrt(2.0 / H1)  b2 = np.zeros((1, H2))  W3 = rng.standard_normal((1, H2)) * np.sqrt(2.0 / H2) b3 = np.zeros((1, 1))  def relu(x):         return np.maximum(0, x)  def relu_d(x):       return (x &gt; 0).astype(x.dtype)  def forward(X):     Z1 = X @ W1.T + b1; A1 = relu(Z1)     Z2 = A1 @ W2.T + b2; A2 = relu(Z2)     Y  = A2 @ W3.T + b3          # sa\u00edda linear (regress\u00e3o)     return Z1, A1, Z2, A2, Y  # 3) Treino: Adam + MSE lr = 1e-3 epochs = 100 batch  = 128  beta1, beta2, eps = 0.9, 0.999, 1e-8  # Adam buffers mW1 = np.zeros_like(W1)  vW1 = np.zeros_like(W1)  mW2 = np.zeros_like(W2) vW2 = np.zeros_like(W2)  mW3 = np.zeros_like(W3) vW3 = np.zeros_like(W3)  mb1 = np.zeros_like(b1) vb1 = np.zeros_like(b1)  mb2 = np.zeros_like(b2)  vb2 = np.zeros_like(b2)  mb3 = np.zeros_like(b3) vb3 = np.zeros_like(b3)  t = 0  def adam_update(param, grad, m, v):     m = beta1*m + (1-beta1)*grad     v = beta2*v + (1-beta2)*(grad*grad)     m_hat = m / (1 - beta1**t)     v_hat = v / (1 - beta2**t)     param -= lr * m_hat / (np.sqrt(v_hat) + eps)     return param, m, v  train_losses, val_losses = [], []  for ep in range(1, epochs+1):     idx = rng.permutation(N)     epoch_loss = 0.0      for i in range(0, N, batch):         bidx = idx[i:i+batch]         xb = X_train[bidx]         yb = y_train_col[bidx]         B  = xb.shape[0]          #  forward          Z1, A1, Z2, A2, Yp = forward(xb)          #  loss (MSE)         mse = np.mean((Yp - yb)**2)         epoch_loss += mse * B          #  backprop          d3  = (2.0/B) * (Yp - yb)     # (B,1)         gW3 = d3.T @ A2               # (1,H2)         gb3 = d3.sum(axis=0, keepdims=True)          dA2 = d3 @ W3                 # (B,H2)         dZ2 = dA2 * relu_d(Z2)         gW2 = dZ2.T @ A1              # (H2,H1)         gb2 = dZ2.sum(axis=0, keepdims=True)          dA1 = dZ2 @ W2                # (B,H1)         dZ1 = dA1 * relu_d(Z1)         gW1 = dZ1.T @ xb              # (H1,D)         gb1 = dZ1.sum(axis=0, keepdims=True)          #  Adam step (incrementa t uma vez por batch)          t += 1         W3, mW3, vW3 = adam_update(W3, gW3, mW3, vW3)         b3, mb3, vb3 = adam_update(b3, gb3, mb3, vb3)         W2, mW2, vW2 = adam_update(W2, gW2, mW2, vW2)         b2, mb2, vb2 = adam_update(b2, gb2, mb2, vb2)         W1, mW1, vW1 = adam_update(W1, gW1, mW1, vW1)         b1, mb1, vb1 = adam_update(b1, gb1, mb1, vb1)      epoch_loss /= N     train_losses.append(epoch_loss)      # valida\u00e7\u00e3o     _, _, _, _, Yv = forward(X_val)     val_mse = np.mean((Yv - y_val_col)**2)     val_losses.append(val_mse)      print(f\"\u00c9poca {ep:02d} | Train MSE {epoch_loss:.4f} | Val MSE {val_mse:.4f}\")  # 4) Avalia\u00e7\u00e3o e gr\u00e1ficos def predict(X):     _, _, _, _, Y = forward(X)     return Y.ravel()  pred_val  = predict(X_val) pred_test = predict(X_test)  print(\"Val :\", metrics(y_val,  pred_val)) print(\"Test:\", metrics(y_test, pred_test))  plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.plot(train_losses, label=\"Train\") plt.plot(val_losses,   label=\"Val\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\"); plt.title(\"Curva de Loss\"); plt.legend()  plt.subplot(1,2,2) plt.scatter(y_val, pred_val, s=8, alpha=0.4) lims = [min(y_val.min(), pred_val.min()), max(y_val.max(), pred_val.max())] plt.plot(lims, lims, \"--\") plt.xlabel(\"Verdadeiro\"); plt.ylabel(\"Previsto\"); plt.title(\"Val: y_true vs y_pred\") plt.tight_layout(); plt.show() <pre>\u00c9poca 01 | Train MSE 678509857.4816 | Val MSE 398164542.4253\n\u00c9poca 02 | Train MSE 326570275.8006 | Val MSE 248977535.9151\n\u00c9poca 03 | Train MSE 167966263.4366 | Val MSE 95650901.6718\n\u00c9poca 04 | Train MSE 62077396.4720 | Val MSE 47291686.2173\n\u00c9poca 05 | Train MSE 42988600.4069 | Val MSE 42002038.3514\n\u00c9poca 06 | Train MSE 39708288.1543 | Val MSE 39712292.8742\n\u00c9poca 07 | Train MSE 37961861.7792 | Val MSE 38414171.9839\n\u00c9poca 08 | Train MSE 36873997.5994 | Val MSE 37462698.1271\n\u00c9poca 09 | Train MSE 36046750.1699 | Val MSE 36682944.0764\n\u00c9poca 10 | Train MSE 35317419.2662 | Val MSE 35993500.8448\n\u00c9poca 11 | Train MSE 34632413.1965 | Val MSE 35377141.7630\n\u00c9poca 12 | Train MSE 34014803.2772 | Val MSE 34735166.3910\n\u00c9poca 13 | Train MSE 33459533.5891 | Val MSE 34206057.7083\n\u00c9poca 14 | Train MSE 32954468.3794 | Val MSE 33734255.0602\n\u00c9poca 15 | Train MSE 32493218.0879 | Val MSE 33285006.0355\n\u00c9poca 16 | Train MSE 32073442.7710 | Val MSE 32837303.2367\n\u00c9poca 17 | Train MSE 31692858.1649 | Val MSE 32475898.6631\n\u00c9poca 18 | Train MSE 31347365.0347 | Val MSE 32104119.8431\n\u00c9poca 19 | Train MSE 31028605.0552 | Val MSE 31797864.3642\n\u00c9poca 20 | Train MSE 30761997.4167 | Val MSE 31528169.5052\n\u00c9poca 21 | Train MSE 30514767.7398 | Val MSE 31282686.8446\n\u00c9poca 22 | Train MSE 30295995.6337 | Val MSE 31077958.8826\n\u00c9poca 23 | Train MSE 30112897.3051 | Val MSE 30914770.8588\n\u00c9poca 24 | Train MSE 29949203.5508 | Val MSE 30743507.9177\n\u00c9poca 25 | Train MSE 29802686.8104 | Val MSE 30548239.1552\n\u00c9poca 26 | Train MSE 29661708.6134 | Val MSE 30393361.4386\n\u00c9poca 27 | Train MSE 29535435.6979 | Val MSE 30309425.7199\n\u00c9poca 28 | Train MSE 29417528.0130 | Val MSE 30166478.4950\n\u00c9poca 29 | Train MSE 29299797.7066 | Val MSE 30040308.5257\n\u00c9poca 30 | Train MSE 29200205.5227 | Val MSE 29921136.0605\n\u00c9poca 31 | Train MSE 29111662.9087 | Val MSE 29811439.1397\n\u00c9poca 32 | Train MSE 29003781.6697 | Val MSE 29766148.0476\n\u00c9poca 33 | Train MSE 28918996.7807 | Val MSE 29635477.4848\n\u00c9poca 34 | Train MSE 28827376.0748 | Val MSE 29520364.5602\n\u00c9poca 35 | Train MSE 28726392.2459 | Val MSE 29415309.2885\n\u00c9poca 36 | Train MSE 28626951.2461 | Val MSE 29322534.5992\n\u00c9poca 37 | Train MSE 28526149.2728 | Val MSE 29232215.9211\n\u00c9poca 38 | Train MSE 28406294.0774 | Val MSE 29120707.5696\n\u00c9poca 39 | Train MSE 28267459.0380 | Val MSE 28930305.1212\n\u00c9poca 40 | Train MSE 28096645.9363 | Val MSE 28755814.9254\n\u00c9poca 41 | Train MSE 27872789.8970 | Val MSE 28505155.5965\n\u00c9poca 42 | Train MSE 27645605.1708 | Val MSE 28317125.3604\n\u00c9poca 43 | Train MSE 27435152.8005 | Val MSE 28058055.6164\n\u00c9poca 44 | Train MSE 27210915.4433 | Val MSE 27826197.8575\n\u00c9poca 45 | Train MSE 26984133.8583 | Val MSE 27610014.3938\n\u00c9poca 46 | Train MSE 26746877.4561 | Val MSE 27391632.2307\n\u00c9poca 47 | Train MSE 26505597.2339 | Val MSE 27138844.3174\n\u00c9poca 48 | Train MSE 26260932.3331 | Val MSE 26874781.5938\n\u00c9poca 49 | Train MSE 25980388.0590 | Val MSE 26596627.9164\n\u00c9poca 50 | Train MSE 25732835.4208 | Val MSE 26380494.2580\n\u00c9poca 51 | Train MSE 25490131.6671 | Val MSE 26261886.9336\n\u00c9poca 52 | Train MSE 25283257.2011 | Val MSE 25974609.5864\n\u00c9poca 53 | Train MSE 25083967.0691 | Val MSE 25781837.3601\n\u00c9poca 54 | Train MSE 24887912.7925 | Val MSE 25893364.7678\n\u00c9poca 55 | Train MSE 24725293.9297 | Val MSE 25417175.5198\n\u00c9poca 56 | Train MSE 24559105.0121 | Val MSE 25283222.6262\n\u00c9poca 57 | Train MSE 24410687.5923 | Val MSE 25132808.3956\n\u00c9poca 58 | Train MSE 24256907.3456 | Val MSE 25098362.9468\n\u00c9poca 59 | Train MSE 24104936.0335 | Val MSE 24815227.1668\n\u00c9poca 60 | Train MSE 23962774.6408 | Val MSE 24700838.6018\n\u00c9poca 61 | Train MSE 23818127.3866 | Val MSE 24525977.2965\n\u00c9poca 62 | Train MSE 23688804.0406 | Val MSE 24414001.4563\n\u00c9poca 63 | Train MSE 23559978.4693 | Val MSE 24265398.2340\n\u00c9poca 64 | Train MSE 23443781.1897 | Val MSE 24157470.8105\n\u00c9poca 65 | Train MSE 23330519.8728 | Val MSE 24084077.5306\n\u00c9poca 66 | Train MSE 23224405.8256 | Val MSE 23958684.3995\n\u00c9poca 67 | Train MSE 23117822.4916 | Val MSE 23851343.9213\n\u00c9poca 68 | Train MSE 23032522.0461 | Val MSE 23773041.7690\n\u00c9poca 69 | Train MSE 22945403.2833 | Val MSE 23673327.5016\n\u00c9poca 70 | Train MSE 22862552.0517 | Val MSE 23580895.8501\n\u00c9poca 71 | Train MSE 22788407.7972 | Val MSE 23508502.2706\n\u00c9poca 72 | Train MSE 22716801.7800 | Val MSE 23448826.7116\n\u00c9poca 73 | Train MSE 22651504.6391 | Val MSE 23374096.7541\n\u00c9poca 74 | Train MSE 22543559.8271 | Val MSE 23260535.9626\n\u00c9poca 75 | Train MSE 22429042.1169 | Val MSE 23150273.1690\n\u00c9poca 76 | Train MSE 22327322.6257 | Val MSE 23034294.4389\n\u00c9poca 77 | Train MSE 22228108.5185 | Val MSE 22934799.4622\n\u00c9poca 78 | Train MSE 22140555.7481 | Val MSE 22883420.4062\n\u00c9poca 79 | Train MSE 22062846.1529 | Val MSE 22800066.4794\n\u00c9poca 80 | Train MSE 21978606.1706 | Val MSE 22687620.6776\n\u00c9poca 81 | Train MSE 21894684.1021 | Val MSE 22642557.3078\n\u00c9poca 82 | Train MSE 21820379.9311 | Val MSE 22552175.5755\n\u00c9poca 83 | Train MSE 21756795.1734 | Val MSE 22576422.2681\n\u00c9poca 84 | Train MSE 21688711.3699 | Val MSE 22432106.0638\n\u00c9poca 85 | Train MSE 21633667.4533 | Val MSE 22344762.2909\n\u00c9poca 86 | Train MSE 21584896.0175 | Val MSE 22404937.2086\n\u00c9poca 87 | Train MSE 21529884.7170 | Val MSE 22264439.5486\n\u00c9poca 88 | Train MSE 21469470.9937 | Val MSE 22222362.1503\n\u00c9poca 89 | Train MSE 21410579.3042 | Val MSE 22165393.1789\n\u00c9poca 90 | Train MSE 21354969.2609 | Val MSE 22095142.5159\n\u00c9poca 91 | Train MSE 21301860.7765 | Val MSE 22090995.9187\n\u00c9poca 92 | Train MSE 21259211.6110 | Val MSE 21968464.5830\n\u00c9poca 93 | Train MSE 21201740.6738 | Val MSE 21941721.5422\n\u00c9poca 94 | Train MSE 21162246.5955 | Val MSE 21897818.8100\n\u00c9poca 95 | Train MSE 21123956.3953 | Val MSE 21840419.5727\n\u00c9poca 96 | Train MSE 21081397.3961 | Val MSE 21812064.0865\n\u00c9poca 97 | Train MSE 21042992.2923 | Val MSE 21763704.7335\n\u00c9poca 98 | Train MSE 21012378.9608 | Val MSE 21809897.7625\n\u00c9poca 99 | Train MSE 20981444.4583 | Val MSE 21741157.0445\n\u00c9poca 100 | Train MSE 20943550.3786 | Val MSE 21706152.6864\nVal : {'MAE': np.float64(2719.284140828667), 'RMSE': 4658.986229469688, 'R2': np.float64(0.9581852820771005)}\nTest: {'MAE': np.float64(2702.177655808821), 'RMSE': 4554.172782841557, 'R2': np.float64(0.959349325120191)}\n</pre> In\u00a0[55]: Copied! <pre>from math import sqrt\nimport matplotlib.pyplot as plt\nplt.rcParams.update({\n    \"font.size\": 10,\n    \"axes.titlesize\": 12,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n    \"figure.titlesize\": 13\n})\n\n# Fun\u00e7\u00f5es auxiliares de m\u00e9tricas\ndef _to1d(a):\n    a = np.asarray(a, dtype=float)\n    return a.ravel() if a.ndim &gt; 1 else a\n\ndef mape(y_true, y_pred, eps=1e-8):\n    y_true = np.asarray(y_true, dtype=float)\n    denom = np.maximum(np.abs(y_true), eps)\n    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0\n\ndef smape(y_true, y_pred, eps=1e-8):\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    denom = np.maximum((np.abs(y_true) + np.abs(y_pred)) / 2, eps)\n    return np.mean(np.abs(y_true - y_pred) / denom) * 100.0\n\ndef r2_score_np(y_true, y_pred):\n    y_true = np.asarray(y_true, dtype=float)\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n    return 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan\n\n# Dados e previs\u00f5es\nX_val   = np.asarray(X_val, dtype=float)\nX_test  = np.asarray(X_test, dtype=float)\ny_train = _to1d(y_train)\ny_val   = _to1d(y_val)\ny_test  = _to1d(y_test)\n\ny_pred_val  = _to1d(predict(X_val))\ny_pred_test = _to1d(predict(X_test))\n\nbaseline_val  = np.full_like(y_val,  fill_value=float(np.mean(y_train)), dtype=float)\nbaseline_test = np.full_like(y_test, fill_value=float(np.mean(y_train)), dtype=float)\n\n# Fun\u00e7\u00e3o de m\u00e9tricas gerais\ndef print_metrics(title, y_true, y_pred, y_base):\n    mae  = np.mean(np.abs(y_true - y_pred))\n    mse  = np.mean((y_true - y_pred)**2)\n    rmse = sqrt(mse)\n    r2   = r2_score_np(y_true, y_pred)\n    mp   = mape(y_true, y_pred)\n    smp  = smape(y_true, y_pred)\n\n    mae_b  = np.mean(np.abs(y_true - y_base))\n    mse_b  = np.mean((y_true - y_base)**2)\n    rmse_b = sqrt(mse_b)\n    r2_b   = r2_score_np(y_true, y_base)\n    mp_b   = mape(y_true, y_base)\n    smp_b  = smape(y_true, y_base)\n\n    print(f\"\\n=== {title} ===\")\n    print(\"Metric           Model         Baseline(mean y_train)\")\n    print(f\"MAE      :   {mae:10.4f}   {mae_b:10.4f}\")\n    print(f\"MSE      :   {mse:10.4f}   {mse_b:10.4f}\")\n    print(f\"RMSE     :   {rmse:10.4f}   {rmse_b:10.4f}\")\n    print(f\"R2       :   {r2:10.4f}   {r2_b:10.4f}\")\n    print(f\"MAPE(%)  :   {mp:10.4f}   {mp_b:10.4f}\")\n    print(f\"sMAPE(%) :   {smp:10.4f}   {smp_b:10.4f}\")\n\nprint_metrics(\"Validation\", y_val,  y_pred_val,  baseline_val)\nprint_metrics(\"Test\",       y_test, y_pred_test, baseline_test)\n\n# ------------------------------------------------------------\ntrain_acc = 1 - np.array(train_losses) / np.max(train_losses)\nval_acc   = 1 - np.array(val_losses) / np.max(val_losses)\n\nss_tot_val = np.sum((y_val - np.mean(y_val))**2)\nr2_by_epoch = [1 - (mse * len(y_val)) / ss_tot_val for mse in val_losses]\n\nfinal_mse_test = np.mean((y_pred_test - y_test)**2)\ntest_acc = 1 - final_mse_test / np.max(train_losses)\n\nplt.figure(figsize=(15,4))\n\nplt.subplot(1,3,1)\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Val Loss\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\")\nplt.title(\"Curva de Loss por \u00c9poca\")\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.plot(train_acc, label=\"Train Accuracy (1 - Loss Normalizada)\")\nplt.plot(val_acc, label=\"Val Accuracy (1 - Loss Normalizada)\")\nplt.hlines(test_acc, 0, len(train_acc)-1, colors='orange', linestyles='--', label=f\"Test Accuracy ({test_acc:.3f})\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"Acur\u00e1cia (Normalizada)\")\nplt.title(\"Acur\u00e1cia por \u00c9poca (Train / Val / Test)\")\nplt.legend()\n\nplt.subplot(1,3,3)\nplt.plot(r2_by_epoch, color='green', label=\"R\u00b2 (Val)\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"R\u00b2\")\nplt.title(\"R\u00b2 por \u00c9poca (Valida\u00e7\u00e3o)\")\nplt.ylim(0, 1.05)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# 2) Gr\u00e1ficos de res\u00edduos e previs\u00e3o x real\nres_val = y_val - y_pred_val\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.scatter(y_pred_val, res_val, alpha=0.6)\nplt.axhline(0, linestyle=\"--\")\nplt.xlabel(\"Predicted (val)\"); plt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot (Val)\")\n\nplt.subplot(1,2,2)\nplt.scatter(y_val, y_pred_val, alpha=0.6)\nmn = float(min(np.min(y_val), np.min(y_pred_val)))\nmx = float(max(np.max(y_val), np.max(y_pred_val)))\nplt.plot([mn, mx], [mn, mx], linestyle=\"--\")\nplt.xlabel(\"Actual (y_val)\"); plt.ylabel(\"Predicted (val)\")\nplt.title(\"Actual vs Predicted (Val)\")\nplt.tight_layout()\nplt.show()\n\nres_test = y_test - y_pred_test\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.scatter(y_pred_test, res_test, alpha=0.6)\nplt.axhline(0, linestyle=\"--\")\nplt.xlabel(\"Predicted (test)\"); plt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot (Test)\")\n\nplt.subplot(1,2,2)\nplt.scatter(y_test, y_pred_test, alpha=0.6)\nmn = float(min(np.min(y_test), np.min(y_pred_test)))\nmx = float(max(np.max(y_test), np.max(y_pred_test)))\nplt.plot([mn, mx], [mn, mx], linestyle=\"--\")\nplt.xlabel(\"Actual (y_test)\"); plt.ylabel(\"Predicted (test)\")\nplt.title(\"Actual vs Predicted (Test)\")\nplt.tight_layout()\nplt.show()\n</pre> from math import sqrt import matplotlib.pyplot as plt plt.rcParams.update({     \"font.size\": 10,     \"axes.titlesize\": 12,     \"axes.labelsize\": 10,     \"xtick.labelsize\": 9,     \"ytick.labelsize\": 9,     \"legend.fontsize\": 9,     \"figure.titlesize\": 13 })  # Fun\u00e7\u00f5es auxiliares de m\u00e9tricas def _to1d(a):     a = np.asarray(a, dtype=float)     return a.ravel() if a.ndim &gt; 1 else a  def mape(y_true, y_pred, eps=1e-8):     y_true = np.asarray(y_true, dtype=float)     denom = np.maximum(np.abs(y_true), eps)     return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0  def smape(y_true, y_pred, eps=1e-8):     y_true = np.asarray(y_true, dtype=float)     y_pred = np.asarray(y_pred, dtype=float)     denom = np.maximum((np.abs(y_true) + np.abs(y_pred)) / 2, eps)     return np.mean(np.abs(y_true - y_pred) / denom) * 100.0  def r2_score_np(y_true, y_pred):     y_true = np.asarray(y_true, dtype=float)     ss_res = np.sum((y_true - y_pred)**2)     ss_tot = np.sum((y_true - np.mean(y_true))**2)     return 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan  # Dados e previs\u00f5es X_val   = np.asarray(X_val, dtype=float) X_test  = np.asarray(X_test, dtype=float) y_train = _to1d(y_train) y_val   = _to1d(y_val) y_test  = _to1d(y_test)  y_pred_val  = _to1d(predict(X_val)) y_pred_test = _to1d(predict(X_test))  baseline_val  = np.full_like(y_val,  fill_value=float(np.mean(y_train)), dtype=float) baseline_test = np.full_like(y_test, fill_value=float(np.mean(y_train)), dtype=float)  # Fun\u00e7\u00e3o de m\u00e9tricas gerais def print_metrics(title, y_true, y_pred, y_base):     mae  = np.mean(np.abs(y_true - y_pred))     mse  = np.mean((y_true - y_pred)**2)     rmse = sqrt(mse)     r2   = r2_score_np(y_true, y_pred)     mp   = mape(y_true, y_pred)     smp  = smape(y_true, y_pred)      mae_b  = np.mean(np.abs(y_true - y_base))     mse_b  = np.mean((y_true - y_base)**2)     rmse_b = sqrt(mse_b)     r2_b   = r2_score_np(y_true, y_base)     mp_b   = mape(y_true, y_base)     smp_b  = smape(y_true, y_base)      print(f\"\\n=== {title} ===\")     print(\"Metric           Model         Baseline(mean y_train)\")     print(f\"MAE      :   {mae:10.4f}   {mae_b:10.4f}\")     print(f\"MSE      :   {mse:10.4f}   {mse_b:10.4f}\")     print(f\"RMSE     :   {rmse:10.4f}   {rmse_b:10.4f}\")     print(f\"R2       :   {r2:10.4f}   {r2_b:10.4f}\")     print(f\"MAPE(%)  :   {mp:10.4f}   {mp_b:10.4f}\")     print(f\"sMAPE(%) :   {smp:10.4f}   {smp_b:10.4f}\")  print_metrics(\"Validation\", y_val,  y_pred_val,  baseline_val) print_metrics(\"Test\",       y_test, y_pred_test, baseline_test)  # ------------------------------------------------------------ train_acc = 1 - np.array(train_losses) / np.max(train_losses) val_acc   = 1 - np.array(val_losses) / np.max(val_losses)  ss_tot_val = np.sum((y_val - np.mean(y_val))**2) r2_by_epoch = [1 - (mse * len(y_val)) / ss_tot_val for mse in val_losses]  final_mse_test = np.mean((y_pred_test - y_test)**2) test_acc = 1 - final_mse_test / np.max(train_losses)  plt.figure(figsize=(15,4))  plt.subplot(1,3,1) plt.plot(train_losses, label=\"Train Loss\") plt.plot(val_losses, label=\"Val Loss\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\") plt.title(\"Curva de Loss por \u00c9poca\") plt.legend()  plt.subplot(1,3,2) plt.plot(train_acc, label=\"Train Accuracy (1 - Loss Normalizada)\") plt.plot(val_acc, label=\"Val Accuracy (1 - Loss Normalizada)\") plt.hlines(test_acc, 0, len(train_acc)-1, colors='orange', linestyles='--', label=f\"Test Accuracy ({test_acc:.3f})\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"Acur\u00e1cia (Normalizada)\") plt.title(\"Acur\u00e1cia por \u00c9poca (Train / Val / Test)\") plt.legend()  plt.subplot(1,3,3) plt.plot(r2_by_epoch, color='green', label=\"R\u00b2 (Val)\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"R\u00b2\") plt.title(\"R\u00b2 por \u00c9poca (Valida\u00e7\u00e3o)\") plt.ylim(0, 1.05) plt.legend()  plt.tight_layout() plt.show()  # 2) Gr\u00e1ficos de res\u00edduos e previs\u00e3o x real res_val = y_val - y_pred_val plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.scatter(y_pred_val, res_val, alpha=0.6) plt.axhline(0, linestyle=\"--\") plt.xlabel(\"Predicted (val)\"); plt.ylabel(\"Residuals\") plt.title(\"Residual Plot (Val)\")  plt.subplot(1,2,2) plt.scatter(y_val, y_pred_val, alpha=0.6) mn = float(min(np.min(y_val), np.min(y_pred_val))) mx = float(max(np.max(y_val), np.max(y_pred_val))) plt.plot([mn, mx], [mn, mx], linestyle=\"--\") plt.xlabel(\"Actual (y_val)\"); plt.ylabel(\"Predicted (val)\") plt.title(\"Actual vs Predicted (Val)\") plt.tight_layout() plt.show()  res_test = y_test - y_pred_test plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.scatter(y_pred_test, res_test, alpha=0.6) plt.axhline(0, linestyle=\"--\") plt.xlabel(\"Predicted (test)\"); plt.ylabel(\"Residuals\") plt.title(\"Residual Plot (Test)\")  plt.subplot(1,2,2) plt.scatter(y_test, y_pred_test, alpha=0.6) mn = float(min(np.min(y_test), np.min(y_pred_test))) mx = float(max(np.max(y_test), np.max(y_pred_test))) plt.plot([mn, mx], [mn, mx], linestyle=\"--\") plt.xlabel(\"Actual (y_test)\"); plt.ylabel(\"Predicted (test)\") plt.title(\"Actual vs Predicted (Test)\") plt.tight_layout() plt.show()  <pre>=== Validation ===\nMetric           Model         Baseline(mean y_train)\nMAE      :    2719.2841   19825.9131\nMSE      :   21706152.6864   519108303.5968\nRMSE     :    4658.9862   22783.9484\nR2       :       0.9582      -0.0000\nMAPE(%)  :      20.3035     238.0368\nsMAPE(%) :      18.8756     101.0205\n\n=== Test ===\nMetric           Model         Baseline(mean y_train)\nMAE      :    2702.1777   19702.5628\nMSE      :   20740489.7360   510225579.3527\nRMSE     :    4554.1728   22588.1734\nR2       :       0.9593      -0.0000\nMAPE(%)  :      20.5237     239.3193\nsMAPE(%) :      19.0806     100.9381\n</pre>"},{"location":"projetos/regression_project/#ann-regression","title":"ANN Regression\u00b6","text":"<p>Autores: Lucas Lima, Henrique Badin e Eduardo Selber.</p> <p>Dataset Escolhido: Fligt Price Prediction.</p> <p>Notebook De Refer\u00eancia: Flight Price Prediction | EDA | Regression &amp; ANN</p>"},{"location":"projetos/regression_project/#descricao-do-conjunto-de-dados","title":"Descri\u00e7\u00e3o do Conjunto de Dados\u00b6","text":"<p>O conjunto de dados cont\u00e9m informa\u00e7\u00f5es sobre passagens a\u00e9reas obtidas do site EaseMyTrip, com o objetivo de analisar fatores que influenciam o pre\u00e7o das passagens e treinar um modelo de regress\u00e3o linear para previs\u00e3o. Foram coletados 300.261 registros entre 11 de fevereiro e 31 de mar\u00e7o de 2022, abrangendo voos entre as seis principais cidades da \u00cdndia.</p>"},{"location":"projetos/regression_project/#estrutura-do-dataset","title":"Estrutura do Dataset\u00b6","text":"<p>O conjunto possui 11 vari\u00e1veis, sendo 10 preditoras e 1 alvo (Price):</p> Vari\u00e1vel Tipo Descri\u00e7\u00e3o Airline Categ\u00f3rica Companhia a\u00e9rea (6 valores \u00fanicos) Flight Categ\u00f3rica C\u00f3digo do voo Source City Categ\u00f3rica Cidade de origem Departure Time Categ\u00f3rica Faixa de hor\u00e1rio de partida Stops Categ\u00f3rica N\u00famero de escalas Arrival Time Categ\u00f3rica Faixa de hor\u00e1rio de chegada Destination City Categ\u00f3rica Cidade de destino Class Categ\u00f3rica Classe da passagem (Economy ou Business) Duration Num\u00e9rica cont\u00ednua Dura\u00e7\u00e3o do voo em horas Days Left Num\u00e9rica discreta Dias entre a compra e a data do voo Price Num\u00e9rica cont\u00ednua Vari\u00e1vel alvo \u2013 pre\u00e7o da passagem"},{"location":"projetos/regression_project/#tipos-de-variaveis","title":"Tipos de Vari\u00e1veis\u00b6","text":"<ul> <li>Categ\u00f3ricas nominais: Airline, Flight, Source City, Destination City, Class</li> <li>Categ\u00f3ricas ordinais: Departure Time, Arrival Time, Stops</li> <li>Num\u00e9ricas: Duration, Days Left, Price</li> </ul>"},{"location":"projetos/regression_project/#dataset-observation","title":"Dataset Observation\u00b6","text":""},{"location":"projetos/regression_project/#analise-das-variaveis-categoricas","title":"An\u00e1lise das vari\u00e1veis categ\u00f3ricas\u00b6","text":""},{"location":"projetos/regression_project/#analise-das-variaveis-numericas","title":"An\u00e1lise das vari\u00e1veis num\u00e9ricas\u00b6","text":""},{"location":"projetos/regression_project/#analise-de-correlacao-da-variavel-target","title":"An\u00e1lise de correla\u00e7\u00e3o da vari\u00e1vel target\u00b6","text":""},{"location":"projetos/regression_project/#pre-processamento","title":"Pr\u00e9 Processamento\u00b6","text":""},{"location":"projetos/regression_project/#separacao-do-conjunto-de-testes-e-treino","title":"Separa\u00e7\u00e3o do conjunto de testes e treino\u00b6","text":""},{"location":"projetos/regression_project/#normalizacao-das-variaveis","title":"Normaliza\u00e7\u00e3o das vari\u00e1veis\u00b6","text":""},{"location":"projetos/regression_project/#rede-neural-artificial","title":"Rede Neural Artificial\u00b6","text":""},{"location":"projetos/regression_project/#processo-de-treinamento","title":"Processo de Treinamento\u00b6","text":"<p>O treinamento ocorre via gradiente descendente estoc\u00e1stico com o otimizador Adam, uma varia\u00e7\u00e3o adaptativa que combina momentum e RMSProp para ajustar dinamicamente a taxa de aprendizado.</p> <p>Os principais par\u00e2metros adotados foram:</p> <ul> <li>Taxa de aprendizado (<code>lr</code>): 0.001</li> <li>Tamanho do batch (<code>batch size</code>): 128 amostras por atualiza\u00e7\u00e3o de gradiente</li> <li>N\u00famero de \u00e9pocas: 100</li> <li>Fun\u00e7\u00e3o de custo: Erro Quadr\u00e1tico M\u00e9dio (MSE), apropriada para regress\u00e3o.</li> </ul> <p>Durante o treinamento:</p> <ol> <li>O conjunto de treino \u00e9 embaralhado e dividido em batches.</li> <li>Para cada batch, \u00e9 feita a propaga\u00e7\u00e3o direta (forward pass) para gerar previs\u00f5es.</li> <li>O erro \u00e9 calculado via MSE.</li> <li>O erro \u00e9 propagado para tr\u00e1s (backpropagation) para calcular os gradientes das camadas.</li> <li>Os pesos e vieses s\u00e3o atualizados conforme as regras do Adam.</li> </ol> <p>Ao final de cada \u00e9poca, o c\u00f3digo tamb\u00e9m calcula a loss de valida\u00e7\u00e3o, permitindo monitorar a generaliza\u00e7\u00e3o do modelo ao longo do tempo.</p>"},{"location":"projetos/regression_project/#avaliacao-e-resultados","title":"Avalia\u00e7\u00e3o e Resultados\u00b6","text":"<p>Ap\u00f3s o treinamento, o modelo foi avaliado nos conjuntos de valida\u00e7\u00e3o e teste, com as m\u00e9tricas:</p> <ul> <li>MAE (Mean Absolute Error)</li> <li>RMSE (Root Mean Squared Error)</li> <li>R\u00b2 (Coeficiente de Determina\u00e7\u00e3o)</li> </ul> <p>Os resultados obtidos mostram um modelo com erro m\u00e9dio relativamente baixo e alto coeficiente de determina\u00e7\u00e3o (R\u00b2 \u2248 0.96), indicando que a rede aprendeu bem as rela\u00e7\u00f5es entre as vari\u00e1veis de entrada e o pre\u00e7o.</p> <p>Al\u00e9m disso, foram gerados dois gr\u00e1ficos principais:</p> <ul> <li>Curva de loss (MSE) para treino e valida\u00e7\u00e3o, evidenciando a converg\u00eancia do modelo.</li> <li>Dispers\u00e3o entre valores reais e previstos (y_true vs y_pred), mostrando boa correla\u00e7\u00e3o e baixa dispers\u00e3o residual.</li> </ul>"},{"location":"projetos/regression_project/#visualizcao-dos-resultados","title":"Visualiz\u00e7\u00e3o dos resultados\u00b6","text":""},{"location":"projetos/regression_project/#conclusoes","title":"Conclus\u00f5es\u00b6","text":"<p>Foram analisadas as curvas de treinamento, as m\u00e9tricas de regress\u00e3o e os gr\u00e1ficos de desempenho do modelo de rede neural aplicado \u00e0 predi\u00e7\u00e3o do pre\u00e7o de passagens a\u00e9reas.</p> <p>Durante o treinamento, as curvas de loss (MSE) de treino e valida\u00e7\u00e3o apresentaram queda acentuada nas primeiras \u00e9pocas e estabiliza\u00e7\u00e3o progressiva a partir da 80\u00aa \u00e9poca, indicando converg\u00eancia e aus\u00eancia de overfitting. As curvas de treino e valida\u00e7\u00e3o mant\u00eam-se pr\u00f3ximas, o que evidencia boa generaliza\u00e7\u00e3o do modelo. O gr\u00e1fico de R\u00b2 por \u00e9poca confirma essa estabilidade, atingindo valores pr\u00f3ximos de 0,96, o que mostra que o modelo explica cerca de 96% da variabilidade dos pre\u00e7os.</p> <p>Os gr\u00e1ficos de res\u00edduos (Residual Plots) e Actual vs Predicted indicam uma forte correla\u00e7\u00e3o entre valores reais e previstos, com os pontos bem distribu\u00eddos ao longo da linha de refer\u00eancia. H\u00e1, contudo, ligeiro aumento da dispers\u00e3o para pre\u00e7os mais altos, o que sugere heterocedasticidade \u2014 isto \u00e9, erros ligeiramente maiores em passagens mais caras, um comportamento esperado em dados econ\u00f4micos.</p> <p>As m\u00e9tricas de regress\u00e3o confirmam a boa performance do modelo em rela\u00e7\u00e3o ao baseline (previs\u00e3o pela m\u00e9dia do treino):</p> M\u00e9trica Valida\u00e7\u00e3o (Modelo / Baseline) Teste (Modelo / Baseline) MAE 2.719 / 19.826 2.702 / 19.703 MSE 21.706.152 / 519.108.303 20.740.489 / 510.225.579 RMSE 4.659 / 22.783 4.554 / 22.588 R\u00b2 0.958 0.959 MAPE (%) 20.30 / 238.03 20.52 / 239.31 sMAPE (%) 18.88 / 101.02 19.08 / 100.94 <p>Os resultados mostram que a rede neural supera amplamente o baseline, apresentando erros absolutos e percentuais significativamente menores e um R\u00b2 elevado, o que evidencia um modelo robusto e bem ajustado.</p> <p>Em s\u00edntese:</p> <ul> <li>O modelo apresenta excelente capacidade preditiva e estabilidade, com erro m\u00e9dio (MAE) em torno de 2.700 unidades monet\u00e1rias e erro percentual pr\u00f3ximo de 20%.</li> <li>As curvas e o R\u00b2 mostram converg\u00eancia est\u00e1vel e boa generaliza\u00e7\u00e3o.</li> <li>Pequenos desvios para valores altos de pre\u00e7o indicam espa\u00e7o para aprimoramentos, como aplicar transforma\u00e7\u00f5es no alvo (<code>log(price)</code>) ou empregar fun\u00e7\u00f5es de perda mais robustas (MAE ou Huber).</li> <li>O desempenho geral demonstra que a arquitetura da rede foi adequada para capturar padr\u00f5es complexos no comportamento dos pre\u00e7os de passagens a\u00e9reas.</li> </ul> <p>OBS: Fizemos uma m\u00e9trica de acur\u00e1cia baseada no MSE, esta m\u00e9trica foi criada simplesmente pois na p\u00e1gina da disciplina, que descreve o projeto, pede um gr\u00e1fico de acur\u00e1cia por \u00e9poca.</p>"},{"location":"thisdocumentation/main/","title":"This documentation","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}